{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ed4bb4-18b8-4b7c-b8b8-4d267b5e5940",
   "metadata": {},
   "source": [
    "Notebook by **Jakob R. Jürgens** - Final project for the courses **OSE - Data Science** in the summer semester 2021 and **OSE - Scientific Computing** in the winter semester 2021/2022 - Find me at [jakobjuergens.com](https://jakobjuergens.com) <br>\n",
    "# Outlier Detection in Sensor Data using Functional Depth Measures\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499e3e4-1138-4526-b80b-c43b9ab3ebbf",
   "metadata": {},
   "source": [
    "The best way to access this project is to clone this repository and execute the jupyter notebook and the shiny app locally. Alternatively, on the main site of this repository, there are nbviewer and binder badges set up to directly look at them in the browser.\n",
    "\n",
    "The following packages and their dependencies are necessary to execute the notebook and the shiny app. If you are executing the code locally, make sure that these packages are provided and that an R Kernel (like irkernel) is activated in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91968e22-6c4e-425c-8c63-d35c9792ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(library(devtools))\n",
    "suppressMessages(library(MASS))\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(shiny))\n",
    "suppressMessages(library(shinydashboard))\n",
    "suppressMessages(library(largeList))\n",
    "suppressMessages(library(parallel))\n",
    "suppressMessages(library(Rcpp))\n",
    "suppressMessages(library(repr))\n",
    "\n",
    "options(repr.plot.width=30, repr.plot.height=8)\n",
    "\n",
    "install.packages('Code/OutDetectR_1.0.tar.gz', repos = NULL, type = 'source')\n",
    "library(OutDetectR)\n",
    "\n",
    "source(\"Code/auxiliary/observation_vis.R\")\n",
    "source(\"Code/auxiliary/distribution_vis.R\")\n",
    "source(\"Code/auxiliary/updating_vis.R\")\n",
    "source(\"Code/auxiliary/generate_set_1.R\")\n",
    "source(\"Code/auxiliary/generate_set_2.R\")\n",
    "source(\"Code/auxiliary/generate_set_3.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11165dab-5b70-4287-a181-2acdda5fe636",
   "metadata": {},
   "source": [
    "If you are in posession of the original Endanzug data set, convert it to a .RDS object using the provided function **data_prep()** from the package written for this project. Set the path to the RDS-object here. As it is property of Daimler AG, it is not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b349707-0ed0-4a32-9834-1f54ed16b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data <- TRUE\n",
    "if(real_data){\n",
    "    endanzug_path <- '~/F/data_local/Projekt_AMEIUS_Daten/schra.RDS'\n",
    "    endanzug_data <- readRDS(file = endanzug_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692058a-65c4-483b-8b9a-ac8f1dc005c9",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "---\n",
    "1. [Introduction](#introduction)\n",
    "2. [Observation Structure](#observation)\n",
    "3. [The Algorithm](#algorithm)\n",
    "4. [h-modal depth](#h-modal)\n",
    "5. [Difficulties due to the Data](#difficulties)\n",
    "6. [Sampling Approach](#sampling)\n",
    "7. [Finding Comparable Sets of Observations](#comparables)\n",
    "8. [Description of Full Procedure for Existing Data sets](#procedure)\n",
    "9. [Updating](#updating)\n",
    "10. [Implementation](#implementation)\n",
    "    1. [Grid Approximation](#impl_grid) \n",
    "    2. [Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) for Observations on a Shared Grid](#impl_algorithm)\n",
    "    3. [Sampling Approach](#impl_sampling)\n",
    "    4. [Dynamic Splitting](#impl_splitting)\n",
    "    5. [Full Procedure](#impl_full)\n",
    "    6. [Updating](#impl_upd)\n",
    "11. [How to use the package?](#how_to)\n",
    "12. [Simulated Data](#simulated)\n",
    "13. [Endanzug-Data](#application)\n",
    "14. [Shiny App](#shiny)\n",
    "15. [Outlook](#outlook)\n",
    "16. [Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a2cc7-bc7f-4b0b-884c-e60b46529dc2",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "---\n",
    "This project is part of a cooperation with **Daimler AG** and deals with outlier detection in sensor data from production processes. <br>\n",
    "It serves as my final project for the courses **OSE - Data Science** (summer semester 2021) and **OSE - Scientific Computing for Economists** (winter semester 2021/2022).\n",
    "\n",
    "The main data set that is used in this project deals with the relation of angle and torque during the process of tightening a bolt in a screwing connection. The corresponding data set will be called \"Endanzugsproblem\" in the following notebook and contains ~350000 observations of what can be imagined as a function that maps angles to torque. The following schematic will give an idea of what the data set represents and what the problem is:\n",
    "\n",
    "<img src=\"material/SchraubdatenPrinzipskizze.png\">\n",
    "\n",
    "To clarify some things about this simplified schematic:\n",
    "* The so called Endanzug is only part of the tightening process, but the parts of the observation happening before it are not subject of this analysis\n",
    "* The focus of this project lies on curves that are \"In Ordnung\", so observations that do not immediately disqualify themselves in some way by for example not reaching the fixed window of acceptable final values\n",
    "* The observations typically have a high frequency of measuring torque, but the measuring points are not equidistant\n",
    "* The angles where torque is measured are not shared between observations, but the measuring interval of angles might overlap between observations\n",
    "* The Endanzug does not start at the same angle for every observation and also does not necessarily start at the same torque\n",
    "* Outliers can be very general, so methods based on detecting only specific types of outliers may not be able to effectively filter out other suspicious observations. So the optimum would be to have some kind of Omnibus test for outliers\n",
    "\n",
    "Especially due to the high frequency of measurement and the non-identical points where torque is measured, the idea of interpreting each observation as a function and therefore approaching the problem from a standpoint of functional data analysis comes to mind. One method that is used in functional data analysis to identify outliers is based on what is called a \"functional depth measure\". Gijbels and Nagy (2017) introduces the idea of depth as follows and then elaborate on the theoretical properties a depth function for functional data should possess:\n",
    "\n",
    "> For univariate data the sample median is well known\n",
    "to be appropriately describing the centre of a data\n",
    "cloud. An extension of this concept for multivariate\n",
    "data (say p-dimensional) is the notion of a point (in\n",
    "$\\mathbb{R}^p$) for which a statistical depth function is maximized. \n",
    "\n",
    "The idea is to define an analogous concept to centrality measures (such as the distance from some central tendency like the median) in a scalar setting for functional data and then use those to determine which functions in a set are typical for the whole population. Due to the more applied nature of this project, I will not go into detail on the theoretical properties of the methods used, but focus on giving intuition why the chosen methods make sense in this context.\n",
    "\n",
    "The main inspiration for my approach to the problem of detecting outliers in a data set such as the one described above is the paper \"**Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels**\" by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I am going to first describe their algorithm, then present my extensions and my implementation and finally apply it to three simulated data sets mimicking the \"Endanzugsproblem\" as the original data is property of Daimler AG, which I cannot make public."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b574b3d-f896-4b6b-94bc-4d674ac8f1d5",
   "metadata": {},
   "source": [
    "## Observation Structure <a name=\"observation\"></a>\n",
    "---\n",
    "For the sake of clarity, I will show the typical structure of one observation and define a couple of objects I will refer to later. To give context for the later choices of data generating processes, it is useful to know that the physical process of tightening a bolt is typically associated with a linear relationship between angle and torque (at least in the relevant parts of the tightening process) - this approximation is good for the parts of the tightening process that are part of the \"Endanzug\". Therefore, simulations in the later parts of this notebook typically assume an approximately linear process as the data generating process for non-outliers.\n",
    "\n",
    "One observation in a data set might look as follows.\n",
    "\n",
    "<img src=\"material/observation.png\" width=\"1000\" align=\"center\">\n",
    "\n",
    "| Observation \t| 1    \t| 2    \t| 3    \t| 4    \t| 5    \t| 6    \t| 7    \t| 8    \t| 9    \t| 10   \t| 11   \t| 12   \t| 13   \t| 14   \t| 15   \t| 16   \t| 17   \t| 18   \t| 19   \t| 20   \t|\n",
    "|:-----------:\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|\n",
    "|    Angle    \t| 2.01 \t| 2.21 \t| 2.91 \t| 3.00 \t| 3.07 \t| 3.95 \t| 4.33 \t| 4.35 \t| 4.41 \t| 4.74 \t| 4.77 \t| 5.06 \t| 6.33 \t| 6.37 \t| 6.41 \t| 6.57 \t| 7.25 \t| 7.32 \t| 7.71 \t| 7.94 \t|\n",
    "|    Torque   \t| 2.02 \t| 2.61 \t| 3.05 \t| 3.16 \t| 2.99 \t| 4.19 \t| 4.24 \t| 4.37 \t| 4.73 \t| 4.89 \t| 5.03 \t| 5.45 \t| 6.32 \t| 6.18 \t| 6.22 \t| 7.06 \t| 7.30 \t| 7.59 \t| 7.99 \t| 8.06 \t|\n",
    "\n",
    "* The red diamonds represent measurements of torque that were taken at a recorded angle.\n",
    "* The blue lines are an example of **linear interpolation** between the points that were actually measured. This will become important later on.\n",
    "* The **measuring interval** marked in green is the convex hull of angles where measurements were taken for this observation.\n",
    "\n",
    "The data set contains many of these objects, that do not necessarily share these characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1d4ed-40ca-461f-8031-5e6fa2a02e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated using function from auxiliary/observation_vis.R\n",
    "# source(\"auxiliary/observation_vis.R\")\n",
    "# obs_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bdb4b-cd4a-43ec-99d6-3d072fbd5d1f",
   "metadata": {},
   "source": [
    "## The Algorithm <a name=\"algorithm\"></a>\n",
    "---\n",
    "The idea of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) is an iterative process that classifies observations as outliers if their functional depth lies below a threshold C, which is determined using a bootstrapping procedure in each iteration. <br> \n",
    "The algorithm can be decomposed into two parts:\n",
    "\n",
    "1. **The iterative process**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i),\\: \\dots \\: ,D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Let $x_{i_1},\\: \\dots,\\: x_{i_k}$ be the k curves such that $D_n(x_{i_k}) \\leq C$, for a given cutoff $C$. Then, assume that $x_{i_1},\\: \\dots,\\: x_{i_k}$ are outliers and delete them from the sample.\n",
    "    3. Then, come back to step 1 with the new dataset after deleting the outliers found in step 2. Repeat this until no more outliers are found. \n",
    "    \n",
    "The underlying idea is that observations that are more central in the sense of the chosen depth will have higher depth assigned to them. So choosing the least deep observations from a data set is a reasonable way to search for atypical observations. The question of where to draw the border for observations to be classified as abnormal is done using a bootstrapping procedure described next. <br>\n",
    "\n",
    "2. **Determining C**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i),\\: \\dots ,\\:D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Obtain B standard bootstrap samples of size n from the dataset of curves obtained after deleting the $\\alpha \\%$ less deepest curves. The bootstrap samples are denoted by $x_i^b$ for $i = 1,\\: \\dots,\\: n$ and $b = 1,\\: \\dots,\\: B$.\n",
    "    3. Obtain smoothed bootstrap samples $y_i^b = x_i^b + z_i^b$, where $z_i^b$ is such that $(z_i^b(t_1), \\dots, z_i^b(t_m))$ is normally distributed with mean 0 and covariance matrix $\\gamma\\Sigma_x$ where $\\Sigma_x$ is the covariance matrix of $x(t_1),\\: \\dots,\\: x(t_m)$ and $\\gamma$ is a smoothing parameter. Let $y_i^b$, $i = 1,\\: \\dots,\\:n$ and $b = 1,\\:\\dots,\\: B$ be these samples. *\n",
    "    4. For each bootstrap set $b = 1,\\:\\dots,\\:B$, obtain $C^b$ as the empirical 1% percentile of the distribution of the depths $D(y_i^b)$, $i = 1,\\: \\dots,\\: n$.\n",
    "    5. Take $C$ as the median of the values of $C^b$, $b = 1,\\: \\dots,\\: B$. \n",
    "<br>\n",
    "\n",
    "This is done, as a theoretical derivation of the distribution of depth values for data generated by a specific data generating process is often infeasible. Instead, one drops a fraction of observations $\\alpha$ and uses a smoothed bootstrapping algorithm to approximate the corresponding threshold value to remove approximately that fraction of observations in the procedure listed under 1.\n",
    "\n",
    "*At this point in the algorithm the assumption that the functional observations are observed at a set of discrete points $t_1,\\:\\dots,\\:t_m$ has already been made. <br>\n",
    "\n",
    "Some important points:\n",
    "* I decided to deviate from the original procedure proposed by the authors by allowing the user to decide whether to reestimate $C$ in each iteration of the process. The authors present good arguments, why keeping $C$ fixed is the better approach and my testing confirms those. But to keep the possibilities for experimentation open, I opted to implement it in this way nevertheless. In later stages of this project I will go into detail on why keeping $C$ constant also has some problems and try to introduce corrections to improve the method developped below.\n",
    "* The authors recommend a choice of $\\gamma = 0.05$ as a smoothing parameter for the bootstrap which I adopted in my applications.\n",
    "* For the choice of $\\alpha$ preexisting information on the data should be used if available. A good way to choose this is the expected fraction of outliers in the data. However, my testing showed that in some cases the choice of this parameter had to be lower than the actual fraction of observations generated by abnormal processes when using a sampling procedure to get better results.\n",
    "\n",
    "The authors propose three functional depth measures and benchmark them in a simulation setting. Because of their results and the computational cost which are comparatively small, I chose to use **h-modal depth** for my implementation, which I will introduce in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63443be-611f-4d03-b664-93d445943d38",
   "metadata": {},
   "source": [
    "## h-modal depth <a name=\"h-modal\"></a>\n",
    "---\n",
    "\n",
    "Introduced by Cuevas, Febrero-Bande, and Fraiman (2006) h-modal depth is one of three depth measures covered in Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I will follow the summary in the latter paper for my overview.  <br> The idea behind this depth is that a curve is central in a set of curves if it is closely surrounded by other curves. <br>\n",
    "\n",
    "In mathematical terms, the h-modal depth of a curve $x_i$ in relation to a set of curves $x_1, \\dots, x_n$ is defined as follows: <br>\n",
    "\\begin{equation}\n",
    "    MD_m(x_i,h) = \\sum_{k = 1}^{n} K(\\frac{||x_i - x_k||}{h})\n",
    "\\end{equation}\n",
    "\n",
    "where $K: \\mathbb{R^{+}} \\rightarrow \\mathbb{R^{+}}$ is a kernel function and $h$ is a bandwidth. <br>\n",
    "The authors recommend using the truncated Gaussian kernel, which is defined as follows:\n",
    "\\begin{equation}\n",
    "    K(t) = \\frac{2}{\\sqrt{2\\pi}} \\exp(-\\frac{t^2}{2}) \\quad \\text{for} \\quad t>0 \\quad \\text{and} \\quad 0 \\quad \\text{otherwise}\n",
    "\\end{equation}\n",
    "\n",
    "and to choose $h$ as the 15th percentile of the empirical distribution of $\\{||x_i - x_k|| \\, | \\, i,k = 1,\\:\\dots,\\:n\\}$\n",
    "\n",
    "I chose to implement the $L^2$ norm - one of the norms recommended by the authors - as it performed better than the $L^{\\infty}$ norm (which was also recommended) in my preliminary tests. In a functional setting $L^2$ is defined by:\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\int_a^b (x_i(t) - x_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "where a and b are the boundaries of the measurement interval. This can be replaced by its empirical version\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 2}^m \\Delta_j(x_i(t_j) - x_k(t_j))^2} = \\sqrt{\\sum_{j = 2}^m (t_j - t_{j-1})(x_i(t_j) - x_k(t_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "in case of a discrete set of $m$ observation points shared between observations.\n",
    "\n",
    "The choice of the functional norm could be adjusted to deal with data resembling different functional forms. This could be part of a possible extension, where different norms are implemented and compared. This might become part of future iteratons of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17ebc8-b676-4fef-b044-afd3a3a3e6c3",
   "metadata": {},
   "source": [
    "## Difficulties due to the Data <a name=\"difficulties\"></a>\n",
    "---\n",
    "### The Endanzug does not start at the same angle for every observation.\n",
    "In this setting the fact that the first measurement is taken at different angles is not of a problem, since the property of interest is the shape of the curve after the Endanzug has started. In real world terms, the beginning of the \"Endanzug\" is determined by the first angle where a specific torque is exceeded and the change in torque during the \"Endanzug\" is of interest and not the position of the \"Endanzug\" in the whole tightening process. Therefore, all observations can be modified by subtracting the first angle of the \"Endanzug\" from all angles, effectively **zeroing** the observations. \n",
    "\n",
    "At this time, I am going to focus on problems where zeroing is admissible and elaborate on how to possibly extend the method to scenarios where it is not. Those generalizations will be part of further work on this project.\n",
    "\n",
    "<img src=\"material/zero.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "### After zeroing, the measurement intervals might still not be identical due to differing lengths.\n",
    "I decided to try to remedy this remaining problem in the zeroed data set by stretching the measuring intervals to a shared interval while leaving the observed torques identical. Excessive stretching however is problematic, as it can lead to similar observations ending up very different. Putting a conservative limit on stretching should however conserve the quality of relationships between observations. This should lead to limited influence on the calculated depths of the functional observations. To do so, I define a parameter $\\lambda \\geq 1$ called **acceptable stretching** and make observations comparable by stretching their measuring intervals by a factor $\\psi_i \\in [1/\\lambda, \\lambda]$ before approximating them using linear interpolation. In an optimal setting stretching would not be necessary and employing it leads to trade offs, so this parameter should be chosen conservatively.\n",
    "\n",
    "If zeroing as described in 1. is not appropriate, a combination of **acceptable stretching** and **acceptable shifting** could be implemented to increase the size of sets of pairwise comparable functions. This will be part of future extensions of this project.\n",
    "\n",
    "<img src=\"material/stretch.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "As you can already see in this animation, the acceptable stretching introduces inaccuracies even in an approximately linear setting. Outlier classifications could be quite sensitive to this parameter.\n",
    "\n",
    "### The angles where torque is measured are not shared between observations.\n",
    "Assuming that the measuring intervals are identical, I decided to use **linear interpolation** to approximate the observations. This is done to treat them as if they had been observed at a **shared grid of angles** to make them compatible with the simplification of the **h-modal norm** described above. This is only an approximation, but choosing an appropriately fine grid to approximate the observations should limit the influence of this procedure on the calculated functional depths. <br>\n",
    "Espacially in case of the dataset under consideration in this project, performing this approximation by linear interpolation should not result in large distortions due to the linearity of the described physical process. In other settings this approximation could lead to bad performance. One example that came to my mind is if most observations are zero, the frequency of taking measurements is comparatively low and the relevant parts of the observations are instantaneous deviations from zero (or spikes that have infinitesimally small duration). In cases like the one described, it would probably be a better idea to choose a different method or at least choose a different functional norm more appropriate for the dgp due to the necessity of very fine grids to achieve a appropriate approximation of the data.<br> Benchmarking of the method for different data generating processes will be part of future revisions to explore the applicability of the developed method.<br>\n",
    "\n",
    "For sufficiently smooth processes with a high measuring frequency this approximation should however not result in huge distortions.\n",
    "\n",
    "<img src=\"material/grid_approx.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "Another possibility to approach this third problem would be to use a different version of the norm for discretized points I described above. <br>\n",
    "Instead of calculating \n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 2}^m \\Delta_j(x_i(t_j) - x_k(t_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "for a set of points on a grid approximated by linear interpolation, one could instead define functions $\\tilde{x}_i$ which are just the piece wise linear functions defined by connecting the observed points of $x_i$. A norm based on this could be constructed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{\\tilde{2}} = \\sqrt{\\int_a^b (\\tilde{x}_i(t) - \\tilde{x}_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "For very fine grid approximations these criteria should result in similar depths, as under some mild conditions, the first approach will converge to the second for increasingly fine grids. In a sense the first approach is similar to a Riemann sum which for increasingly fine grids converges to the corresponding Riemann integral under the necessary assumptions.\n",
    "\n",
    "### Runtime Complexity\n",
    "The runtime complexity of this algorithm is at least $O(n^2)$ and I concluded that using my implementation it is infeasible to use it on a very large data set such as the \"Endanzugsproblem\" (assuming that all observations are comparable at once). Even when splitting up the observations as proposed above into comparable subsets, some of them will be too large to directly approach with this method. <br>\n",
    "To solve this problem, I instead opted to use a **sampling approach** which I explain in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8799d-49f3-40d2-9ff4-5d86db584d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The gifs have been rendered using function from auxiliary/observation_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/observation_vis.R\")\n",
    "# stretching_vis()\n",
    "# zeroing_vis()\n",
    "# lin_approx_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98161569-efdf-4525-ba07-b39c722aab8c",
   "metadata": {},
   "source": [
    "## Sampling Approach <a name=\"sampling\"></a>\n",
    "---\n",
    "Since it is infeasible to use this method on very large data sets at once, I chose to pursue a sampling-based approach instead. Intuitively this is reasonable as in many cases observations that are atypical in the full data set will be classified as atypical in its subsamples more often than \"typical\" observations. If this principle does not apply to the data set, a sampling-based approach is difficult to justify. In a case like that it is more reasonable to choose differnet methods to identify abnormal observations. <br>\n",
    "\n",
    "As the assumption seems reasonable in case of the \"Endanzugsproblem\", instead of performing the algorithm described above on the whole data set (or its comparable subsets), I devise a procedure as described in the following: <br>\n",
    "\n",
    "Let $\\{x_1, \\dots, x_L\\}$ be a set of observations that are comparable using the algorithm but too large to perform this procedure in reality.\n",
    "1. Define the following objects:\n",
    "    * Let *num\\_samples* $ = (a_1,\\:\\dots,\\: a_L) \\in \\mathbb{N}_0^L$ where $a_i$ is the number of samples $x_i$ was part of $\\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$. <br>(Initialize all entries as 0)\n",
    "    * Let *num\\_outliers* $ = (b_1,\\:\\dots,\\: b_L) \\in \\mathbb{N}_0^L$ where $b_i$ is the number of samples $x_i$ was classified as an outlier in $\\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$. <br>(Initialize all as entries 0)\n",
    "    * Let *frac\\_outliers* $ = (c_1,\\:\\dots,\\: c_L) \\in \\mathbb{R}_{\\geq 0}^L$ where $c_i = \\begin{cases}1 & a_i = 0 \\\\ \\frac{b_i}{a_i} & a_i > 0\\end{cases} \\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$  <br>(Initialize all entries as 1)\n",
    "2. Draw a sample of size K from $\\{x_1,\\: \\dots,\\: x_L\\}$ without replacement\n",
    "3. Perform the outlier detection procedure on this sample and update the vectors according to your results.\n",
    "4. Go back to two and iterate this process until some condition is fulfilled.\n",
    "\n",
    "Typical conditions could be:\n",
    "* A specified number of iterations was reached\n",
    "* Every observation was part of more than a specified number of samples\n",
    "* The vector of certainties did not change enough according to some criterion over a specified number of iterations\n",
    "\n",
    "In the end, the entries of *frac\\_outliers* can be used as a metric for the outlyingness of an observation. If a binary decision rule is needed, every observation with an entry over some specified threshold could be classified as an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fadb66-508b-4504-93d2-5b2ddc39365c",
   "metadata": {},
   "source": [
    "## Finding Comparable Sets of Observations <a name=\"comparables\"></a>\n",
    "---\n",
    "Assume for the sake of simplicity that **zeroing** is reasonable so that the minima of the measurement intervals of the observations are all zero. Therefore, the measurement intervals only differ in their end points or lengths which is identical in this case. <br>\n",
    "As an example to visualize possible ways to find comparable subsets assume that the empirical distribution of endpoints looks as follows:\n",
    "\n",
    "<img src=\"material/dist.png\" width=\"1000\" align=\"center\">\n",
    "\n",
    "In the following I describe three methods to find comparable subsets to perform the sampling procedure on and explain why I chose the method that I ultimately implemented.\n",
    "\n",
    "### Static Splitting\n",
    "\n",
    "For **Static Splitting** the idea is to partition the whole data set into pairwise disjunct subsets of pairwise comparable observations. This partition depends on the acceptable stretching parameter and is not necessarily unique for a choice of the acceptable stretching parameter. Therefore, a choice procedure would have to be introduced if this approach were to be taken. <br>\n",
    "Some possible partitions of the set above (not necessarily consistent with the same acceptable stretching parameter) could look like this: \n",
    "\n",
    "<img src=\"material/static_splits.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "However, this idea has some problems:\n",
    "* The choice of subsets could introduce a new source of distortions in addition to the acceptable stretching parameter.\n",
    "* Adding new observations could change the chosen subsets, making an updating procedure difficult to realize.\n",
    "* In each individual subset, the observations that are changed due to stretching are identical over samples. This could lead to distortions since for some observations not the original but only the stretched observations are taken into account in the classification.\n",
    "\n",
    "### Dynamic Splitting\n",
    "\n",
    "In **Dynamic Splitting** the allocation of comparable subsets takes place dynamically for each realization of the observation interval. The procedure is as follows: <br>\\\n",
    "For each realization of the endpoint, determine the subset of comparable observations and perform the sampling approach on this subset but keep the parameter for acceptable stretching constant for all end points.\n",
    "\n",
    "<img src=\"material/dyn_splits.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "This approach has some advantages over the first one:\n",
    "* The choice of subsets becomes only a question of the acceptable stretching parameter and not of the choice of algorithm that chooses the partition of the data set.\n",
    "* Adding new observations is unproblematic, as new observations do not influence the allocation of comparable subsets. Therefore additional samples containing the new observation can be drawn without creating problems in the comparability to previous sampling-based results. A procedure like this is described in the next section.\n",
    "* Each observation can enter the classification procedure undistorted in at least the comparable subset corresponding to its own endpoint. Additionally, it can enter the classification in samples, where it is comparable due to acceptable stretching. The latter can realize for different degrees of stretching - increasing or decreasing the length of the measuring interval. This solves the problem of observations being used for classification only in a specific distorted state.\n",
    "\n",
    "### Dynamic Splitting with varying acceptable stretching parameter\n",
    "\n",
    "As shown above, the difference in length of these comparable subsets changes quite substantially and does not react to the density of observations having similar measurement intervals. A deviation from this appraoch might be desirable for multiple reasons:\n",
    "* If there are many observations with a very similar measuring interval, including observations with a more dissimilar interval (in terms of the animation above, a midpoint farther away) might be detrimental to the procedure's performance. If there are fewer observations close by one might want to allow for a bigger acceptable stretching parameter to allow for more comparisons.\n",
    "* In tandem with the advantage above one could also change the sample size to suit specific needs of the procedure.\n",
    "\n",
    "It would be possible to use a **local acceptable stretching parameter** that changes as a function of the estimated density of end points (since zeroing was admissible in this example).\n",
    "Using a rather simple function determining the local acceptable stretching parameter to serve as an example leads to the following choice of comparable subsets.\n",
    "\n",
    "<img src=\"material/dyn_splits2.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "In this example the effect is quite subtle, but in comparison to the previous animation, one can see that the expansion of the interval of end points of comparable observations is slower in regions, where the estimated density of end points is higher. <br>\n",
    "There are two reasons why I decided against making acceptable stretching a varying parameter in my implementation:\n",
    "1. It introduces another complication as the function to determine the local acceptable stretching has to be chosen.\n",
    "2. It makes the updating procedure described later more difficult, as adding more observations will change the estimated density of the end points and thereby change the comparable subsets.\n",
    "\n",
    "### Choice for Implementation\n",
    "Due to the advantages and disadvantages described above, I decided to implement Dynamic Splitting with a global acceptable stretching parameter. <br>\n",
    "This makes it easier to later justify an updating procedure and still does not create distorted results as described for Static Splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f2ce3-b4be-4c0b-b107-9f7001831d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The graphics and gifs have been rendered using function from auxiliary/observation_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/distribution_vis.R\")\n",
    "# dist_vis()\n",
    "# static_splits_vis()\n",
    "# dynamic_splits_vis()\n",
    "# dynamic_splits2_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2980c-964d-4c2f-bbf0-d849e57d438b",
   "metadata": {},
   "source": [
    "## Description of Full Procedure for Existing Data sets <a name=\"procedure\"></a>\n",
    "---\n",
    "Having explained\n",
    "* The algorithm by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008)\n",
    "* The sampling approach \n",
    "* The procedure for selecting comparable subsets with dynamic splitting of the data set \n",
    "\n",
    "I am going to explain the full procedure applied to an existing data set assuming that zeroing is admissible. <br>\n",
    "\n",
    "### Definition of Objects\n",
    "* Let $x_1,\\: \\dots,\\: x_n$ be the full data set of observations as described in the beginning\n",
    "* Let $I_1 = [s_1, e_1], \\dots, I_n = [s_n, e_n]$ be the measuring intervals of $x_1, \\dots, x_n$\n",
    "\n",
    "Assuming that zeroing is admissible:\n",
    "* Let $\\bar{x}_1,\\: \\dots, \\:\\bar{x}_n$ be the corresponding zeroed observations\n",
    "* Let $\\bar{I}_1 = [0, \\: \\bar{e}_1], \\: \\dots, \\bar{I}_n = [0, \\bar{e}_n]$ be the corresponding zeroed measuring intervals\n",
    "* Let $\\bar{E} = \\{\\bar{e} \\: | \\: \\exists k \\in \\{1, \\dots, n\\} \\: \\text{s.t.} \\: \\bar{e} = \\max(\\bar{I}_k)\\}$ be the set of measuring interval end points occurring in $\\bar{I}_1,\\: \\dots,\\: \\bar{I}_n$\n",
    "* Let $\\bar{\\Lambda}(\\bar{x}, \\bar{e})$ be the resulting object when zeroed observation $\\bar{x}$ is stretched to fit zeroed measuring interval $\\bar{I} = [0, \\bar{e}]$\n",
    "\n",
    "Again assuming that zeroing is admissible, one can reasonably define the following objects.\n",
    "* Let $\\bar{Z}(\\bar{e}, \\lambda) = \\{\\bar{x}_k \\: | \\: \\frac{\\bar{e}_k}{\\bar{e}} \\in [\\frac{1}{\\lambda},\\lambda]\\}$ be the set of zeroed observations that can be compared to a zeroed observation with measuring interval $\\bar{I} = [0, \\bar{e}]$ with an acceptable stretching factor of $\\lambda$.\n",
    "* Let $\\bar{S}(\\bar{e}, \\lambda) = \\{\\bar{\\Lambda}(\\bar{e}, \\bar{x}) \\: | \\: \\bar{x} \\in \\bar{Z}(\\bar{e}, \\lambda)\\}$ be the set of zeroed observations that have been stretched with an admissible stretching factor to be comparable to a zeroed observation with zeroed measuring interval $\\bar{I} = [0, \\bar{e}]$.\n",
    "\n",
    "If zeroing is not a valid approach, corresponding objects dependent on acceptable shifting and acceptable stretching have to be defined, which will be one challenge in generalizing this method.\n",
    "\n",
    "### Procedure\n",
    "1. Choose parameters:\n",
    "    * $\\lambda$ acceptable stretching\n",
    "    * $L$ sample size in sampling procedure (may also be varying depending on chosen approach for sampling)*\n",
    "    * $K$ number of equidistant points in grid for approximation by linear interpolation*\n",
    "    * $\\alpha$ fraction of observations to drop in approximation of cutoff value $C$ in outlier classification algorithm*\n",
    "    * $B$ sample size in approximation of cutoff value $C$ in outlier classification algorithm*\n",
    "    * $\\gamma$ tuning parameter in approximation of cutoff value $C$ in outlier classification algorithm*\n",
    "2. Initialize the following vectors:\n",
    "    * *num\\_samples* $ = (a_1,\\:\\dots,\\: a_n)\\in \\mathbb{N}_0^n \\quad$ with all entries being 0\n",
    "    * *num\\_outliers* $ = (b_1,\\:\\dots,\\: b_n)\\in \\mathbb{N}_0^n \\quad$ with all entries being 0\n",
    "    * *frac\\_outliers* $ = (c_1,\\:\\dots,\\: c_n) \\in \\mathbb{R}_{\\geq 0}^n \\quad$ with all entries being 1\n",
    "3. Iterate through the elements of $\\bar{E}$ doing the following:\n",
    "    * Let $\\hat{e}$ be the element of $\\bar{E}$ currently looked at\n",
    "    * Determine $\\bar{S}(\\hat{e}, \\lambda)$ and perform the sampling based outlier identification procedure on this set for some predetermined stopping condition\n",
    "    * Update *num\\_samples*, *num\\_outliers*, and *frac\\_outliers* for both the stretched and non-stretched observations in $\\bar{S}(\\hat{e}, \\lambda)$ as described in the section about sampling\n",
    "    * Go to the next element of $\\bar{E}$ and repeat until all elements have been reached.\n",
    "4. Report *frac\\_outliers* as a measure of outlyingness for the observations.\n",
    "\n",
    "*not explicitly mentioned in the procedure below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d441b-7b69-41d2-ba6b-c05f1ae73b78",
   "metadata": {},
   "source": [
    "## Updating <a name=\"updating\"></a>\n",
    "---\n",
    "The procedure described above is constructed to work for a full data set. In a day-to-day setting the data set will not be static. Instead, new observations will be added and it would be a problem, if all calculations had to be done all over again only to incorporate a comparatively small number of new observations.\n",
    "\n",
    "Instead, I devise mechanisms to assign comparable values of *frac\\_outliers* to the newly added observations and to possibly also update the pre-existing observations due to the presence of the newly added ones. In the following assume that new observations are added sequentially. Two ways came to my mind to approach this updating procedure, one more true to the original process (1) and the other one less computationally expensive (2).\n",
    "\n",
    "### Version 1:\n",
    "This procedure involves additional samples from all sets the new observation could have been part of. So both in a stretched form or in its original form <br>\n",
    "* Let $x'$ be the new observation and $\\bar{x}'$ its zeroed counterpart. Define $I'$, $\\bar{I'}$ and $\\bar{e}'$ accordingly.\n",
    "* Determine all elements $\\bar{e} \\in \\bar{E}$ such that $\\bar{x}' \\in \\bar{Z}(\\bar{e}, \\lambda)$. Define $\\bar{U}(\\bar{x}', \\lambda) = \\{\\bar{e} \\in \\bar{E} \\: | \\: \\bar{x}' \\in \\bar{Z}(\\bar{e}, \\lambda)\\}$ as the subset of $\\bar{E}$ called the Updating Window. <br> In this setting where zeroing is admissible, this can be simplified to $\\bar{U}(\\bar{x}', \\lambda) = \\{\\bar{e} \\in \\bar{E} \\: | \\: \\bar{e} \\in [\\frac{1}{\\lambda} \\bar{e}', \\lambda \\bar{e}']\\} = \\bar{E} \\cap [\\frac{1}{\\lambda} \\bar{e}', \\lambda \\bar{e}']$\n",
    "    \n",
    "<img src=\"material/update_1.gif\" width=\"1000\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400468a-f7cf-426d-b4dd-7cb79ea9baa9",
   "metadata": {},
   "source": [
    "For all $\\tilde{e} \\in \\bar{U}(\\bar{x}', \\lambda)$ perform a sampling procedure as follows: <br>\n",
    "* $\\bar{\\Lambda}(\\bar{x}', \\tilde{e})$ is part of each sample \n",
    "* The size of each sample is identical to the one used in the original procedure\n",
    "* The number of samples for each $\\tilde{e}$ is the expected value of the number of samples the new observation would have been part of, if it had been in the original data set\n",
    "* The updating procedure of the vectors works as before. Not only the entry of the new observation in each vector is added and updated, also the entries for the original observations included in the new samples are updated.\n",
    "\n",
    "So the set of zeroed observations that is potentially updated during this procedure is the following: <br> \n",
    "$$\\bigcup_{\\tilde{e}\\in \\bar{U}(\\bar{x}', \\lambda)} \\bar{Z}(\\tilde{e}, \\lambda) = \\Big\\{\\bar{x} \\: | \\: \\bar{e} \\in \\big[\\frac{1}{\\lambda}\\min\\{\\bar{U}(\\bar{x}', \\lambda)\\}, \\lambda \\max\\{\\bar{U}(\\bar{x}', \\lambda)\\}\\big]\\Big\\}$$\n",
    "<br>\n",
    "Which is visualized in the following animation. (The endpoints of the potentially updated zeroed observations are marked by the red rectangle.)\n",
    "\n",
    "<img src=\"material/update_2.gif\" width=\"1000\" align=\"center\"> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dce8b0-35e1-4ad9-ac68-88e51625c847",
   "metadata": {},
   "source": [
    "### Version 2:\n",
    "In comparison the other procedure involves only additional sampling from the set where the new observation is non-stretched.\n",
    "* Let $x'$ be the new observation and $\\bar{x}'$ its zeroed counterpart. Define $I'$, $\\bar{I'}$ and $\\bar{e}'$ accordingly.\n",
    "* Determine $\\bar{S}(\\bar{e}', \\lambda)$ and perform additional sampling as follows:\n",
    "    * $\\bar{x}'$ is part of each sample\n",
    "    * The number of samples drawn is the expected value of samples the new observation would have been part of, if it had been in the original data set\n",
    "    * The updating procedure works as before. Not only the entry of the new observation in each vector is added and updated, also the entries for the original observations are updated.\n",
    "\n",
    "The following graphic shows the equivalent objects of version 1:\n",
    "\n",
    "<img src=\"material/update_3.png\" width=\"1000\" align=\"center\"> \n",
    "\n",
    "This procedure is less true to the values calculated in the original data set, as the new observation is only taken into consideration in its non-stretched form. Additionally, pre-existing observations could be taken into consideration in their stretched form more frequently depending on the structure of new data being added which could lead to additional distortions. Therefore, I decided to start by implementing the first method and possibly include comparisons of both methods in future revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18c20b-ce5c-42fa-82bd-7d3a9719fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The graphics and gifs have been rendered using functions from auxiliary/updating_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/updating_vis.R\")\n",
    "# upd_1_vis()\n",
    "# pot_upd_obs_vis()\n",
    "# upd_2_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14cf48-cfeb-48d9-9cf6-a07e4c2adaaa",
   "metadata": {},
   "source": [
    "## Implementation <a name=\"implementation\"></a>\n",
    "---\n",
    "I decided to implement these methods in **R** and **C++** and to employ parallelization where possible, to strike a balance between speed and ease of use. All of these functions are implemented in the [**Rcpp**](https://cran.r-project.org/web/packages/Rcpp/) package **OutDetectR** that is contained in the code folder and can be installed as shown in the first code block. Documentation in the package is realized with [**roxygen2**](https://cran.r-project.org/web/packages/roxygen2/) so help files for the functions can be accessed using the notation ?\\<function_name\\>.\n",
    "\n",
    "The following section will follow a similar structure as the description of the procedure above ordered as follows:\n",
    "\n",
    "1. [Grid Approximation](#impl_grid) \n",
    "2. [Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) for Observations on a Shared Grid](#impl_algorithm)\n",
    "3. [Sampling approach](#impl_sampling)\n",
    "4. [Dynamic Splitting and Finding Comparable Subsets](#impl_splitting)\n",
    "5. [Full Procedure](#impl_full)\n",
    "6. [Updating](#impl_updating)\n",
    "\n",
    "All of these functions can also be found in */auxiliary/R_functions.R* and will in the future be available as an Rcpp-package the tar-ball of which will be included in the repo. <br>\n",
    "In future revisions there will be a secondary notebook with the details of the implementation. This notebook will instead focus on explaining the method and evaluating its perfomance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd2215-83cf-4cc9-be4a-efde96532674",
   "metadata": {},
   "source": [
    "### Grid Approximation <a name=\"impl_grid\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4219ea9-c2fc-4839-a70c-d51fc40f0cad",
   "metadata": {},
   "source": [
    "#### Finding a grid for Approximation\n",
    "Assuming that the observations already share the measuring intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed04b9-932c-4d92-a820-d5ec89d6daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function creates a grid that is used for the piece wise linear\n",
    "#' approximation of functions. Currently this assumes that the observations\n",
    "#' share a measuring interval and constructs an equdistant grid of specified\n",
    "#' length.\n",
    "#'\n",
    "#' @param func_dat: Data used by functional procedures\n",
    "#' @param length.out: number of points in the grid\n",
    "#'\n",
    "#' @return A vector that contains the grid points\n",
    "#' @export\n",
    "grid_finder <- function(func_dat, length.out = 100) {\n",
    "\n",
    "  # Extract Measuring Interval from first functional observation\n",
    "  measuring_interval <- c(min(func_dat[[1]]$args), max(func_dat[[1]]$args))\n",
    "  # Create Equdistant grid\n",
    "  grid <- seq(measuring_interval[1], measuring_interval[2], length.out = length.out)\n",
    "  # Return grid\n",
    "  return(grid)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb6d5c-015b-4e78-a576-104146f19bbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Grid approximation by Linear Interpolation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79687402-6024-4e44-b5e8-b253ae285907",
   "metadata": {},
   "source": [
    "double interpolate(double x_left, double x_right, double y_left, double y_right, double x){\n",
    "  if(x == x_left) return(y_left);\n",
    "  double m = (y_right - y_left) / (x_right - x_left);\n",
    "  double y = y_left + m * (x - x_left);\n",
    "  return(y);\n",
    "}\n",
    "\n",
    "double x_lneigh(NumericVector args, double x) {\n",
    "  int len = args.size();\n",
    "  int i = 0;\n",
    "  \n",
    "  while(i < len){\n",
    "    if(x >= args[i]){\n",
    "      if(x == args[i]) return(i);\n",
    "      i++;\n",
    "    }\n",
    "    else{\n",
    "      break;\n",
    "    }\n",
    "  } \n",
    "  return(i-1);\n",
    "}\n",
    "\n",
    "// [[Rcpp::export]]\n",
    "NumericVector grid_approx_obs(NumericVector args, NumericVector vals, NumericVector grid) {\n",
    "  \n",
    "  int n_points = grid.size();\n",
    "  NumericVector output(n_points);\n",
    "  int x_l = 0;\n",
    "  \n",
    "  for(int i = 0; i < n_points; i++){\n",
    "    x_l = x_lneigh(args, grid[i]);\n",
    "    output(i) = interpolate(args[x_l], args[x_l + 1], vals[x_l], vals[x_l + 1], grid[i]);\n",
    "  }\n",
    "  \n",
    "  return(output);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5469b-f7a6-475a-864a-eed8878e8f23",
   "metadata": {
    "tags": []
   },
   "source": [
    "This following function acts as a wrapper for the **C++** function grid_approx_obs() shown above, that given a vector of arguments, a vector of values and a vector that represents the grid to be used for approximation, performs the desired approximation and returns the vector of approximated values taken at the grid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4c12c-647c-4b11-b529-5f52abc331f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function approximates the functional observations given to it using a piece\n",
    "#' wise linear approximation on a specified grid. The format of the data is unchanged\n",
    "#'\n",
    "#' @param func_dat: list that contains the observations:\n",
    "#' each observation is a list, that contains two vectors of identical length: args and vals\n",
    "#' @param grid: grid to use for approximation\n",
    "#'\n",
    "#' @return A list of identical length to func_dat. Each entry corresponds to the \n",
    "#' same entry in func_dat but is an approximation using linear interpolation on\n",
    "#' the points given in grid.\n",
    "#' @export\n",
    "grid_approx_set_obs <- function(func_dat, grid) {\n",
    "  # for each observation perform an approximation on the grid points\n",
    "  approx_list <- map(\n",
    "    .x = func_dat,\n",
    "    .f = function(obs) {\n",
    "      list(\n",
    "        args = grid,\n",
    "        vals = grid_approx_obs(obs$args, obs$vals, grid)\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  # return the approximated observations\n",
    "  return(approx_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03a84a-4e3e-4d15-9d53-5ee25e39450c",
   "metadata": {},
   "source": [
    "The second wrapper then combines those values to a matrix, where each row represents the approximated values of an observation at the given grid points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9dd255-7769-4aa7-af7f-874638f14f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function is a wrapper around grid_approx_set_obs.\n",
    "#' It takes the approximated observations of identical length and transforms \n",
    "#' them into a matrix\n",
    "#'\n",
    "#' @param func_dat: list that contains the observations:\n",
    "#' each observation is a list, that contains two vectors of identical length: args and vals\n",
    "#' @param grid: grid to use for approximation\n",
    "#'\n",
    "#' @return A matrix of dimensions length(func_dat)xlength(grid) that contains \n",
    "#' the approximated values of the functional observations at the grid points\n",
    "#' in each row.\n",
    "#' @export\n",
    "grid_approx_mat <- function(func_dat, grid){\n",
    "  # call approximation in list form\n",
    "  list_approx <- grid_approx_set_obs(func_dat = func_dat,\n",
    "                                     grid = grid)\n",
    "  # extract values\n",
    "  approx_values <- map(.x = list_approx,\n",
    "                       .f = function(obs) obs$vals)\n",
    "  # combine in matrix\n",
    "  mat_approx <- matrix(data = unlist(approx_values),\n",
    "                       nrow = length(list_approx), ncol = length(grid),\n",
    "                       byrow = TRUE)\n",
    "  # return matrix\n",
    "  return(mat_approx)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91236f-8fa4-4112-a429-cd342f24dcf9",
   "metadata": {},
   "source": [
    "### Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) for Observations on a Shared Grid <a name=\"impl_algorithm\"></a>\n",
    "The following functions implement the algorithm described above for functional observations that are observed at a common set of discrete points. The grid approximation above serves as a preparation to make these functions applicable. <br> \n",
    "\n",
    "#### approx_C\n",
    "The function *approx_C* implements the approximation of the cutoff value $C$ by bootstrapping described in Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a9c15-bf97-4bff-b405-b2aadadc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Uses a bootstrapping procedure to approximate the cutoff value for the identification\n",
    "#' of outliers\n",
    "#'\n",
    "#' @param matr_dat: data in matrix form - each row contains the grid approximations of one observation\n",
    "#' @param fdepths: corresponding depths for the observations\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' @param B: number of smoothed bootstrap samples to use\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#' @param grid: grid used in approximation of matr_dat\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "approx_C <- function(matr_dat, fdepths, alpha, B, gamma, grid) {\n",
    "\n",
    "  # set smoothing mode to on\n",
    "  smoothing <- TRUE\n",
    "  # infer number of observations from length of depth vector\n",
    "  n <- length(fdepths)\n",
    "  # Get number of elements in grid\n",
    "  grid_length <- length(grid)\n",
    "  # determine threshold to drop observations with lowest depth values\n",
    "  depth_thr <- tryCatch(\n",
    "    {\n",
    "      quantile(x = fdepths, probs = alpha, na.rm = FALSE)\n",
    "    },\n",
    "    error = function(cond){\n",
    "      stop(fdepths, 'approx_C - I ran into this error!')\n",
    "    }\n",
    "  )\n",
    "  # drop observations for bootstrapping\n",
    "  matr_dat_red <- matr_dat[fdepths >= depth_thr, ]\n",
    "\n",
    "  n_red <- dim(matr_dat_red)[1]\n",
    "\n",
    "  # Determine vcov-matrix for smoothed bootstrapping\n",
    "  Sigma_x <- cov(matr_dat_red)\n",
    "\n",
    "  my_vcov <- gamma * Sigma_x\n",
    "  \n",
    "  # Draw bootstrap samples from data set\n",
    "  fsamples <- map(\n",
    "    .x = 1:B,\n",
    "    .f = function(inds) matr_dat_red[sample(x = 1:n_red, size = n, replace = TRUE), ]\n",
    "  )\n",
    "\n",
    "  # Create smoothing components for bootstrapping\n",
    "  smoothing_components <- map(\n",
    "    .x = 1:B,\n",
    "    .f = function(x) mvrnorm(n = n, mu = rep(0, times = grid_length), Sigma = my_vcov)\n",
    "  )\n",
    "\n",
    "  # Obtain smoothed bootstrap samples\n",
    "  smoothed_BS_samples <- map(\n",
    "    .x = 1:B,\n",
    "    .f = function(b) fsamples[[b]] + smoothing_components[[b]]\n",
    "  )\n",
    "\n",
    "  # Calculate depths for each smoothed bootstrap sample\n",
    "  bootstrap_depths <- map(\n",
    "    .x = 1:B,\n",
    "    .f = function(b) hM_depth(smoothed_BS_samples[[b]], grid)\n",
    "  )\n",
    "\n",
    "  # Calculate first percentile from depths of smoothed bootstrap samples\n",
    "  one_perc_quantiles <- unlist(map(\n",
    "    .x = bootstrap_depths,\n",
    "    .f = function(sample) quantile(sample, probs = 0.01, na.rm = FALSE)\n",
    "  ))\n",
    "\n",
    "  # return median of first percentiles\n",
    "  return(median(one_perc_quantiles))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db35bb-bf34-40ac-a5a4-68c9a80746a3",
   "metadata": {},
   "source": [
    "#### outlier_iteration\n",
    "This function performs one iteration of the algorithm, including the calculation of functional depths, the approximation of $C$ and the flagging of observations with depths lower than $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753f804-049a-4734-9e0f-68965e4183d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function performs one iteration of the algorithm, including the \n",
    "#' calculation of functional depths, the approximation of C and the \n",
    "#' flagging of observations with depths lower than C\n",
    "#'\n",
    "#' @param matr_dat: data in matrix form - each row contains the grid approximations of one observation\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' (in approximation of C - optional if C is specified)\n",
    "#' @param: B: number of smoothed bootstrap samples to use\n",
    "#' (in approximation of C - optional if C is specified)\n",
    "#' @param: gamma: tuning parameter for smoothed bootstrap\n",
    "#' @param: ids: identifiers of individual observations\n",
    "#' @param: grid: grid used in approximation of matr_dat\n",
    "#' @param: C: should be provided. Otherwise C will be approximated in each step of the iteration\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "outlier_iteration <- function(matr_dat, alpha = 0.05, B = 50, gamma, ids, grid, C = NULL) {\n",
    "\n",
    "  # Calculating functional depths using a function from ./auxiliary/Rcpp_functions.cpp\n",
    "  fdepths <- hM_depth(matr_dat, grid)\n",
    "\n",
    "  if (missing(C)) {\n",
    "    # Approximating C\n",
    "    C <- approx_C(\n",
    "      matr_dat = matr_dat, fdepths = fdepths, alpha = alpha,\n",
    "      B = B, gamma = gamma, grid = grid\n",
    "    )\n",
    "  }\n",
    "\n",
    "  # Flagging observations with depths lower than the cutoff value C\n",
    "  outliers <- which(fdepths < C)\n",
    "\n",
    "  return(list(\n",
    "    matr_dat = matr_dat[-outliers, ],\n",
    "    ids = ids[-outliers],\n",
    "    outlier_ids = ids[outliers]\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef7dcb-976d-459a-8a3f-42ea8c15d1e5",
   "metadata": {},
   "source": [
    " #### outlier_detection\n",
    " This function serves as a wrapper for outlier_iteration and iterates the process until no new observations are flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e769044-6adb-4019-ac8e-be7c038ce47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'  This function serves as a wrapper for outlier_iteration and iterates the process until no new observations are flagged.\n",
    "#'\n",
    "#' @param matr_dat: data in matrix form - each row contains the grid approximations of one observation\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' (in approximation of C - optional if C is specified)\n",
    "#' @param B: number of smoothed bootstrap samples to use\n",
    "#' (in approximation of C - optional if C is specified)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#' @param ids: identifiers of individual observations\n",
    "#' @param grid: grid used in approximation of matr_dat\n",
    "#' @param C: should be provided. Otherwise C will be approximated in each step of the iteration\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "outlier_detection <- function(matr_dat, alpha = 0.05, B = 100, gamma = 0.05, ids, grid, C = NULL) {\n",
    "  tmp_ids <- ids\n",
    "  # Initialize empty vectors for position of flagged observations in func_dat\n",
    "  # and ids of flagged observations\n",
    "  outlier_ids <- c()\n",
    "\n",
    "  # loop that continues until an iteration does not flag any new observations\n",
    "  condition <- TRUE\n",
    "  while (condition) {\n",
    "\n",
    "    # perform iteration\n",
    "    iter_res <- outlier_iteration(\n",
    "      matr_dat = matr_dat, alpha = alpha, B = B, gamma = gamma, \n",
    "      ids = tmp_ids, grid = grid, C = C)\n",
    "    \n",
    "    new_outliers <- iter_res$outlier_ids\n",
    "\n",
    "    # if there are no new flagged observations stop loop\n",
    "    if (length(new_outliers) == 0) {\n",
    "      condition <- FALSE\n",
    "    } else {\n",
    "      # otherwise: add flagged observations to vector\n",
    "      outlier_ids <- c(outlier_ids, new_outliers)\n",
    "      # reduce data to non-flagged observations\n",
    "      matr_dat <- iter_res$matr_dat\n",
    "      # reduce ids to non-flagged observations\n",
    "      tmp_ids <- iter_res$ids\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # return identifiers of flagged observations and position of these flagged observations in the data set\n",
    "  return(list(\n",
    "    outlier_ids = outlier_ids,\n",
    "    outlier_ind = which(is.element(ids, outlier_ids))\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b8374-5439-4c3d-9394-d796bb55bb01",
   "metadata": {},
   "source": [
    "#### Outlier Detection - Wrapper\n",
    "The following function acts as a wrapper to the previous one in case, $C$ should not be recalculated in each iteration. My recommendation would be to use this function as recalculation of $C$ in each iteration can lead to classifying unreasonably many observations as outliers. Due to problems with the parallelization, non-reproducible errors appear in some cases, making a tryCatch necessary.\n",
    "In case of a reproducible error a counter will reach a specified value after a number of iterations, throwing the error to the overarching procedure, terminating the outlier detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f24fd-c04d-418f-b609-e044958e993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function performs the identification procedure implemented in this package\n",
    "#' in the simplest scenario. It serves as an important building block for the\n",
    "#' more high-level functions.\n",
    "#' Important assumptions are the following:\n",
    "#' - observations in func_dat are zeroed and have the same measuring interval\n",
    "#'\n",
    "#' @param func_dat: list that contains the observations\n",
    "#' each observation is a list, that contains two vectors of identical length: args and vals\n",
    "#' @param ids: identifiers of individual observations\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' (in approximation of C - optional if C is specified)\n",
    "#' @param ids: B: number of smoothed bootstrap samples to use\n",
    "#' (in approximation of C - optional if C is specified)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#'\n",
    "#' @return Returns a list containing two objects:\n",
    "#' 1. outlier_ids: a vector containing the ids of the observations identified as outliers\n",
    "#' 2. outlier_ind: a vector containing the position of the the observations identified as outliers in the func_dat list\n",
    "#' @export\n",
    "detection_wrap <- function(func_dat, ids, alpha, B, gamma = 0.05) {\n",
    "\n",
    "  # determine the grid for approximation\n",
    "  grid <- grid_finder(func_dat = func_dat)\n",
    "\n",
    "  # Approximate by linear interpolation\n",
    "  matr_dat <- grid_approx_mat(func_dat = func_dat, grid = grid)\n",
    "\n",
    "  # tmp redo variable\n",
    "  redos <- 0\n",
    "  \n",
    "  while(redos < 10){\n",
    "    # calculate h-modal depths\n",
    "    fdepths <- hM_depth(valueMatrix = matr_dat, grid = grid)\n",
    "    if(all(is.na(fdepths))){\n",
    "      redos <- redos + 1\n",
    "    } else{\n",
    "      redos <- 10\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # create temporary variable indicating if error occured\n",
    "  n_error <- 0\n",
    "\n",
    "  # Approximate a value of C\n",
    "  while (n_error < 10) {\n",
    "    add <- FALSE\n",
    "    C_appr <- tryCatch(\n",
    "      {\n",
    "        approx_C(\n",
    "          matr_dat = matr_dat, fdepths = fdepths, alpha = alpha,\n",
    "          B = B, gamma = gamma, grid = grid\n",
    "        )\n",
    "      },\n",
    "      error = function(cond) {\n",
    "        warning(cond)\n",
    "        add <<- TRUE\n",
    "      }\n",
    "    )\n",
    "    if(add == TRUE){\n",
    "      n_error <- n_error + 1\n",
    "    } else{\n",
    "      n_error <- 10\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Perform the outlier classification procedure for the approximated value of C\n",
    "  flagged <- outlier_detection(\n",
    "    matr_dat = matr_dat, ids = ids, grid = grid, C = C_appr\n",
    "  )\n",
    "\n",
    "  # Return the list of outlier ids and outlier indices - these are useful in different cases\n",
    "  # contains the objects outlier_ids and outlier_ind\n",
    "  return(flagged)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad396bf5-6d75-41a5-ba54-bc23e7b04a12",
   "metadata": {},
   "source": [
    "### Sampling Approach <a name=\"impl_sampling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c93a0-e70b-44b7-a1b5-a7e7096ebaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function is a convenience wrapper around largeList::saveList\n",
    "#'\n",
    "#'@param func_dat: list that contains the observations\n",
    "#' each observation is a list, that contains two vectors of identical length: args and vals\n",
    "#'@param path: path to where the file shall be saved, including a filename and the \n",
    "#' ending .llo\n",
    "#'\n",
    "#'@return no return value\n",
    "#'@export\n",
    "largeListify <- function(func_dat, path){\n",
    "  tryCatch(\n",
    "    {\n",
    "      saveList(object = func_dat, file = path, append = FALSE, compress = FALSE)\n",
    "    },\n",
    "    error = function(cond){\n",
    "      message('There was an Error saving the list.')\n",
    "      message('Here is the original error message:')\n",
    "      message(cond)\n",
    "    },\n",
    "    warning = function(cond){\n",
    "      message('A warning was issued while saving.')\n",
    "      message('Here is the original warning message:')\n",
    "      message(cond)\n",
    "    }\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af4de4-90b9-4519-8666-f3b587fa20cd",
   "metadata": {},
   "source": [
    "#### Helper function for parallelization\n",
    "Since data sets can get large very quickly, it is useful to perform parallelization in this sampling approach in a less memory intensive way. Therefore I decided to write the data set in its prepared form to the disk and use a format that supports random access in lists, to only read in the observations that are part of the sample. After reading in those observations, it performs the outlier detection procedure implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2e8be-5a08-42ca-b7b2-2be4d8173e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' placeholder\n",
    "#'\n",
    "#' @param list_path: path to the random access list of the data set (generated by package largeList)\n",
    "#' @param index: index of observations to use in the procedure\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C)\n",
    "#' @param B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#'\n",
    "#' @return placeholder\n",
    "sample_helper <- function(list_path, ids, index, alpha, B, gamma) {\n",
    "\n",
    "  # read in the observations identified by the variable index\n",
    "  sample_dat <- readList(file = list_path, index = index)\n",
    "\n",
    "  # perform the outlier detection procedure on the sample\n",
    "  # in a tryCatch statement as the procedure creates notamatrix errors in random cases\n",
    "  sample_flagged <- tryCatch(\n",
    "    {\n",
    "      detection_wrap(func_dat = sample_dat, ids = ids, alpha = alpha, B = B, gamma = gamma)\n",
    "    },\n",
    "    error = function(cond) {\n",
    "      return(list(outlier_ids = c(), outlier_ind = c()))\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # return the object generated by the outlier detection procedure\n",
    "  return(sample_flagged$ids)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98ed1d-6acc-496b-b8a2-8ac30ebcd7c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33885ca1-37d3-41c5-b304-f4df25aa84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' placeholder\n",
    "#'\n",
    "#' @param cl: cluster object generated by parallel package\n",
    "#' @param list_path: path to the random access list of the data set (generated by package largeList)\n",
    "#' assumes that the observations are already zeroed and share the same measuring\n",
    "#' interval\n",
    "#' @param indices: Vector containing the indeces of observations that are\n",
    "#' included in the sampling procedure\n",
    "#' @param n_samples: number of samples to use\n",
    "#' @param sample_size: number of observations to use in each sample\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' (in approximation of C)\n",
    "#' @param B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "sample_wrap <- function(cl, list_path, indeces = NULL, n_samples, sample_size, alpha, B, gamma) {\n",
    "\n",
    "  # get number of observations from the largeList file if indices is missing\n",
    "  # Assumption: if no indices are given, all observations are being considered\n",
    "  if (missing(indeces)) {\n",
    "    indices <- 1:largeList::getListLength(list_path)\n",
    "  }\n",
    "\n",
    "  n_obs <- length(indeces)\n",
    "  tmp_ids <- 1:n_obs\n",
    "\n",
    "  # Initialize vectors described in the theoretical section\n",
    "  num_samples <- rep(x = 0, times = n_obs)\n",
    "  num_outliers <- rep(x = 0, times = n_obs)\n",
    "  frac_outliers <- rep(x = 1, times = n_obs)\n",
    "\n",
    "  # Draw indexes for sampling from functional data without replacement\n",
    "  sample_inds <- map(\n",
    "    .x = 1:n_samples,\n",
    "    .f = function(i) sample(x = tmp_ids, size = sample_size, replace = FALSE)\n",
    "  )\n",
    "\n",
    "  # Determine how often each observation appeared in the samples and update the vector\n",
    "  freq_samples <- tabulate(as.numeric(unlist(sample_inds)))\n",
    "  num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples\n",
    "\n",
    "  # Perform the outlier classification procedure on the chosen samples parallelized\n",
    "  # with the function clusterApplyLB() from the parallel package\n",
    "  sample_flagged_par <- clusterApplyLB(\n",
    "    cl = cl,\n",
    "    x = sample_inds,\n",
    "    fun = function(smpl) {\n",
    "      sample_helper(\n",
    "        list_path = list_path, ids = tmp_ids[smpl], index = indeces[smpl],\n",
    "        alpha = alpha, B = B, gamma = gamma\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # Determine how often each observation were flagged in the samples and update the vector\n",
    "  freq_outliers <- tabulate(as.numeric(unlist(sample_flagged_par)))\n",
    "  num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "\n",
    "  # termine fraction of samples each observation was flagged as an outlier in\n",
    "  certainties <- unlist(map(\n",
    "    .x = 1:n_obs,\n",
    "    .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i] / num_samples[i], 1)\n",
    "  ))\n",
    "\n",
    "  # Return list containing the three central vectors: num_samples, num_outliers, certainties\n",
    "  return(list(\n",
    "    num_samples = num_samples,\n",
    "    num_outliers = num_outliers,\n",
    "    certainties = certainties\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b231a-67a6-4ec5-b32a-13b232c1376b",
   "metadata": {},
   "source": [
    "#### How to use this function?\n",
    "Using the function *sample_wrap()* is not as straight-forward as using the previous as multiple steps have to be performed before and after using this function to ensure a problem free experience.\n",
    "I will explain the following code fragments in detail, as depending on which operating system a user employs modifications have to be made. For this reason, the code is commented out in these parts, in order not to cause technical problems that are difficult to replicate. The following code segments were written on a Linux machine and will work on UNIX systems. Since forking is not supported under windows, alternatives would have to be used, like using PSOCK-Clusters instead of ForkClusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56eed5e-9c0c-45f7-9b9e-b2548bdcf3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adfd7fa4-4b6c-4859-88b7-ad7f48ab6024",
   "metadata": {},
   "source": [
    "* *num_cores()* detects the number of logical cores\n",
    "* *makeForkCluster()* creates a virtual cluster object that can serve as an argument to functions from the parallel package to perform parallelized computations\n",
    "* the *clusterCall()* calls execute the command inside on each individual node of the virtual cluster to ensure that all necessary packages are loaded in each instance\n",
    "* *clusterExport* this exports objects from an environment to each of the nodes. These can be functions or objects.\n",
    "\n",
    "(These last to steps are technically not necessary in case of a fork cluster, but will navigate around some hard to troubleshoot problems that can occur.)\n",
    "\n",
    "* *sampling_wrap()* performs the actions implemented above on the virtual cluster *cl*\n",
    "* *stopCluster()* stops the cluster, to ensure that it does not clog up the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf73e0d-2a33-46d4-9ba5-d03810d70cec",
   "metadata": {},
   "source": [
    "### Dynamic Splitting <a name=\"impl_splitting\"></a>\n",
    "\n",
    "#### Zeroing\n",
    "To use the dynamic splitting procedure in the previously described settings, it is first necessary to zero all observations. This is implemented for the functional observations and its results should be saved as a separate object for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613631d-d655-48f7-a3bb-4ae8e1665703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function zeroes an observation by shifting the arg values to begin at zero.\n",
    "#'\n",
    "#'@param func_obs: a list, that contains two vectors of identical length: args and vals\n",
    "#'\n",
    "#'@return a list, that contains two vectors of identical length: args and vals,\n",
    "#' where args begins with zero.\n",
    "#'@export\n",
    "zero_obs <- function(func_obs){\n",
    "  # create zeroed observation in usual format\n",
    "  zeroed_func_obs <- list(args = func_obs$args - func_obs$args[1], \n",
    "                          vals = func_obs$vals)\n",
    "  # return list\n",
    "  return(zeroed_func_obs)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69486f93-11fd-48a2-a739-d11f7527b5fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa48557-1f1c-4470-bf65-60ca64b51c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function is a wrapper around zero_obs and zeroes all \n",
    "#' observations in the data set\n",
    "#'\n",
    "#'@param func_dat: list that contains the observations\n",
    "#' each observation is a list, that contains two vectors of identical length: args and vals\n",
    "#'\n",
    "#'@return A list of zeroed observations\n",
    "#'@export\n",
    "zero_data <- function(func_dat){\n",
    "  \n",
    "  # create zeroed observation in usual format\n",
    "  zeroed_func_dat <- purrr::map(.x = func_dat,\n",
    "                                .f = zero_obs)\n",
    "  # return list\n",
    "  return(zeroed_func_dat)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e40641-da71-4619-8333-248fc3566faf",
   "metadata": {},
   "source": [
    "#### Determine measuring intervals\n",
    "This function returns a matrix containing in each row the beginning and end point of the measuring interval of the corresponding observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b34836-2da1-43f9-ae33-21ef6464973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function extracts the measuring interval from a functional observation.\n",
    "#'\n",
    "#'@param func_obs: a list, that contains two vectors of identical length: args and vals\n",
    "#'\n",
    "#'@return A vector of two elements, the start and endpoints of the measuring interval\n",
    "#'@export\n",
    "measuring_int <- function(func_obs){\n",
    "  # Extract Interval from functional observation\n",
    "  interval <-  c(min(func_obs$args), max(func_obs$args))\n",
    "  # return interval vector\n",
    "  return(interval)                     \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db2085-1f79-4d45-aa0d-02e1af31451d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca900e-9260-4b3b-9b18-027b6f5495ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function extracts the measuring interval from a set of functional observations.\n",
    "#'\n",
    "#'@param func_dat: a list, that contains two vectors of identical length: args and vals\n",
    "#'\n",
    "#'@return A list of vectors containing two elements, the start and endpoints of the measuring intervals\n",
    "#'@export\n",
    "measuring_int_list <- function(func_dat){\n",
    "  # Extract Intervals from functional observation\n",
    "  intervals_list <-  map(.x = func_dat,\n",
    "                         .f = measuring_int)\n",
    "  # return list of interval vectors\n",
    "  return(intervals_list)                     \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174d573-c49b-4a50-954e-2c4e5018169d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120288e1-945c-4351-b0b6-444f56e5b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function is a wrapper around measuring_int_list and combines the intervals\n",
    "#' into a length(func_dat)x2 matrix\n",
    "#'\n",
    "#'@param func_dat: a list, that contains two vectors of identical length: args and vals\n",
    "#'\n",
    "#'@return A list of vectors containing two elements, the start and endpoints of the measuring intervals\n",
    "#'@export\n",
    "measuring_int_mat <- function(func_dat){\n",
    "  # Extract Intervals from functional observation\n",
    "  intervals_list <- measuring_int_list(func_dat)\n",
    "  # combine into matrix\n",
    "  intervals_matrix <- matrix(data = unlist(intervals_list), \n",
    "                             nrow = length(func_dat), ncol = 2,\n",
    "                             byrow = TRUE)\n",
    "  # return matrix\n",
    "  return(intervals_matrix)                     \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdd45f-5bc8-4a1a-a8c7-a4c6db291303",
   "metadata": {},
   "source": [
    "#### Create a list of measuring intervals that occur in the data set\n",
    "This set is needed later on for iterating through all possible realizations of the measuring interval to determine the comparable sets for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5646a9-cfb0-4757-bb32-cec133ce39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Given a matrix of measuring intervals as generated by measuring_int_mat\n",
    "#' this function extracts the unique intervals in the data set and returns\n",
    "#' a reduced matrix containing only the unique intervals\n",
    "#'\n",
    "#'@param interval_matrix: A matrix with 2 columns containing in its rows\n",
    "#' the measuring intervals of the observations\n",
    "#'\n",
    "#'@return A matrix containing the unique measuring intervals found \n",
    "#' in the data set\n",
    "#'@export\n",
    "unique_intervals <- function(interval_matrix){\n",
    "  \n",
    "  # for finding unique entries transforming to a list is easier\n",
    "  interval_list <- map(.x = seq_len(nrow(interval_matrix)), \n",
    "                       .f = function(i) interval_matrix[i,])\n",
    "  \n",
    "  # find unique entries                             \n",
    "  unique_intervals <- unique(interval_list)\n",
    "  \n",
    "  # combine into matrix again                         \n",
    "  unique_matrix <- matrix(data = unlist(unique_intervals), \n",
    "                          nrow = length(unique_intervals),\n",
    "                          byrow = TRUE)\n",
    "  \n",
    "  # return matrix where each row contains the beginning and end points of a unique measuring interval\n",
    "  # from the data set\n",
    "  return(unique_matrix)                         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c3399-37e5-40ef-bc43-e454de94e124",
   "metadata": {},
   "source": [
    "#### Find comparable observations for one measuring interval\n",
    "Given a measuring interval that is currently under consideration and the matrix of all measuring intervals, determine the indices of the observations that are comparable given an acceptable stretching factor $\\lambda$. Here zeroing is assumed, such that the condition simplifies to a condition on the endpoint of the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90253bb5-aa19-4f78-aa24-5a0d3a4cc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Checks which measuring intervals are suitable to be compared to a given measuring interval.\n",
    "#'\n",
    "#'@param main_interval: vector of two elements: starting and end point of measuring interval\n",
    "#'@param measuring_intervals: use output from measuring_int_mat\n",
    "#'@param lambda: acceptable stretching parameter\n",
    "#'@param ids: identifiers of individual observations\n",
    "#'\n",
    "#'@return A list containing two objects: ind, which contains the rownumber of the \n",
    "#' comparable observations in the matrix measuring_intervals and ids, which contains\n",
    "#' the identifiers of the comparable observations\n",
    "#'@export\n",
    "comparable_obs_finder <- function(main_interval, measuring_intervals, lambda, ids){\n",
    "  \n",
    "  # Determine comparable observations by checking interval endpoints\n",
    "  comparable <- which(measuring_intervals[,2] >= main_interval[2]/lambda \n",
    "                      & measuring_intervals[,2] <= main_interval[2]*lambda)\n",
    "  \n",
    "  # Return the correspoding indices and the ids of the comparable observations\n",
    "  return(list(ind = comparable,\n",
    "              ids = ids[comparable]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279ce02-3501-4fdd-992d-7c9c252f1830",
   "metadata": {},
   "source": [
    "This function can then be used for determining which sets to sample from using the sampling procedure implemented above while iterating through the measuring intervals that occur in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfecc4-ecff-4944-b503-d714efcc4f63",
   "metadata": {},
   "source": [
    "### Full Procedure <a name=\"impl_full\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515d128-c80d-4afc-8aa6-72cd7da197c6",
   "metadata": {},
   "source": [
    "#### Stretching an observation\n",
    "The first function needed for the full procedure is the ability to stretch an observation to be comparable to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321db26e-9905-41ae-97bc-d251cc1885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function takes a functional observation func_obs and stretches it to\n",
    "#' fit a given measuring interval\n",
    "#'\n",
    "#' @param func_obs: a list that contains two vectors of identical length: args and vals\n",
    "#' @param measuring_interval: a vector with 2 elements,\n",
    "#' the start and end points of the desired measuring interval\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "stretch_obs <- function(func_obs, measuring_interval) {\n",
    "\n",
    "\n",
    "  # calculate stretching factor\n",
    "  phi <- {\n",
    "    (measuring_interval[2] - measuring_interval[1]) /\n",
    "      (max(func_obs$args) - min(func_obs$args))\n",
    "  }\n",
    "\n",
    "  # stretch arguments by appropriate factor\n",
    "  args_stretched <- func_obs$args * phi\n",
    "\n",
    "  # return in the format for functional observations\n",
    "  return(list(\n",
    "    args = args_stretched,\n",
    "    vals = func_obs$vals\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b89e3-31da-4ca8-aff3-3e0a9474d92b",
   "metadata": {},
   "source": [
    "#### Stretching a set of observations to prepare sampling procedure\n",
    "This function acts as a wrapper for the previous function and applies it to a set of observations that are stretched to the same measuring interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0b376-5ceb-4fa2-81d4-896e7106eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' This function is a wrapper around stretch_obs. It takes a set of functional\n",
    "#' observations func_dat and stretches each observation to fit a given measuring interval\n",
    "#'\n",
    "#' @param func_dat: list that contains the observations\n",
    "#' each observation is a list, that contains two vectors of identical length: args and vals\n",
    "#' @param measuring_interval: a vector with 2 elements,\n",
    "#' the start and end points of the desired measuring interval\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "stretch_data <- function(func_dat, measuring_interval) {\n",
    "\n",
    "  # stretch all observations\n",
    "  stretch_dat <- map(\n",
    "    .x = func_dat,\n",
    "    .f = function(func_obs) stretch_obs(func_obs, measuring_interval)\n",
    "  )\n",
    "\n",
    "  # return in the format for functional observations\n",
    "  return(stretch_dat)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c239b-8964-460f-9ce7-03cabebe5a61",
   "metadata": {},
   "source": [
    "#### Performing stretching and sampling procedure on set of observations\n",
    "This is nearly identical to the functions for the sampling procedure itself and could easily be included as a subcase. For the sake of clarity, I nevertheless decided to make these separate functions.\n",
    "Understanding the sampling procedure and the stretching functions will make these functions easy to understand.\n",
    "\n",
    "#### Helper function for parallelization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d7672-af0c-44fc-a654-13cef7a18d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' placeholder\n",
    "#'\n",
    "#' @param list_path: path to the random access list of the data set (generated by package largeList)\n",
    "#' @param main_interval: Measuring interval currently under consideration\n",
    "#' @param index: index of observations to use in the procedure \n",
    "#' (decided to be comparable in the calling function)\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C)\n",
    "#' @param B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#'\n",
    "#' @return placeholder\n",
    "stretch_sample_helper <- function(list_path, main_interval,\n",
    "                                  ids, index, alpha, B, gamma) {\n",
    "\n",
    "  # read in the observations identified by the variable index\n",
    "  sample_dat <- readList(file = list_path, index = index)\n",
    "\n",
    "  # stretch observations to the chosen measuring interval\n",
    "  sample_dat_stretched <- stretch_data(\n",
    "    func_dat = sample_dat, measuring_interval = main_interval\n",
    "  )\n",
    "\n",
    "  # perform the outlier detection procedure on the sample\n",
    "  # in a tryCatch statement as the procedure creates notamatrix errors in random cases\n",
    "  sample_flagged <- tryCatch(\n",
    "    {\n",
    "      detection_wrap(\n",
    "        func_dat = sample_dat, ids = ids, \n",
    "        alpha = alpha, B = B, gamma = gamma)$outlier_ids\n",
    "    },\n",
    "    error = function(cond) {\n",
    "      return(list(outlier_ids = c(), outlier_ind = c()))\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # return the object generated by the outlier detection procedure\n",
    "  return(sample_flagged)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1e6aa-f097-439c-a2a8-de137a33b0bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f03fb-0ae2-490c-b6ff-4ae8ec57cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' placeholder\n",
    "#'\n",
    "#' @param cl: cluster object generated by parallel package\n",
    "#' @param list_path: path to the random access list of the data set (generated by package largeList)\n",
    "#' @param main_interval: measuring interval the observations are stretched to\n",
    "#' @param indices: Vector containing the indeces of observations that are\n",
    "#' included in the sampling procedure\n",
    "#' @param n_samples: number of samples to use\n",
    "#' @param sample_size: number of observations to use in each sample\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' (in approximation of C)\n",
    "#' @param B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "stretch_sample_wrap <- function(cl, list_path, main_interval,\n",
    "                                indeces = NULL, n_samples, sample_size, alpha, B, gamma) {\n",
    "\n",
    "  # get number of observations from the largeList file if indices is missing\n",
    "  # Assumption: if no indices are given, all observations are being considered\n",
    "  if (missing(indeces)) {\n",
    "    indices <- 1:largeList::getListLength(list_path)\n",
    "  }\n",
    "\n",
    "  n_obs <- length(indeces)\n",
    "  tmp_ids <- 1:n_obs\n",
    "\n",
    "  # Initialize vectors described in the theoretical section\n",
    "  num_samples <- rep(x = 0, times = n_obs)\n",
    "  num_outliers <- rep(x = 0, times = n_obs)\n",
    "  frac_outliers <- rep(x = 1, times = n_obs)\n",
    "\n",
    "  # Draw indexes for sampling from functional data without replacement\n",
    "  sample_inds <- map(\n",
    "    .x = 1:n_samples,\n",
    "    .f = function(i) sample(x = tmp_ids, size = sample_size, replace = FALSE)\n",
    "  )\n",
    "\n",
    "  # Determine how often each observation appeared in the samples and update the vector\n",
    "  freq_samples <- tabulate(as.numeric(unlist(sample_inds)))\n",
    "  num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples\n",
    "\n",
    "  # Perform the outlier classification procedure on the chosen samples parallelized\n",
    "  # with the function clusterApplyLB() from the parallel package\n",
    "  sample_flagged_par <- clusterApplyLB(\n",
    "    cl = cl,\n",
    "    x = sample_inds,\n",
    "    fun = function(smpl) {\n",
    "      stretch_sample_helper(\n",
    "        list_path = list_path, main_interval = main_interval,\n",
    "        ids = tmp_ids[smpl], index = indeces[smpl], alpha = alpha, B = B, gamma = gamma\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # Determine how often each observation were flagged in the samples and update the vector\n",
    "  # this check is a hack. Should be solved differently in the future\n",
    "  if(length(sample_flagged_par) != 0){\n",
    "    freq_outliers <- tabulate(as.numeric(unlist(sample_flagged_par)))\n",
    "    num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "  }\n",
    "  \n",
    "  # determine fraction of samples each observation was flagged as an outlier in\n",
    "  certainties <- unlist(map(\n",
    "    .x = 1:n_obs,\n",
    "    .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i] / num_samples[i], 1)\n",
    "  ))\n",
    "\n",
    "  # Return list containing the three central vectors: num_samples, num_outliers, certainties\n",
    "  return(list(\n",
    "    num_samples = num_samples,\n",
    "    num_outliers = num_outliers,\n",
    "    certainties = certainties\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7ad7d-d9e4-4c18-89ee-f6c30927e23f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab354d-39cb-4c74-87a8-385673789f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' placeholder\n",
    "#'\n",
    "#' @param cl: cluster object generated by parallel package\n",
    "#' @param list_path: path to the random access list of the data set (generated by package largeList)\n",
    "#' this assumes that all observations are already zeroed and for every observation\n",
    "#' there are more comparable counterparts than the parameter sample_size\n",
    "#' @param lambda: allowed stretching parameter\n",
    "#' @param n_samples: number of samples to use\n",
    "#' @param sample_size: number of observations to use in each sample\n",
    "#' @param expn: Chosen expected number of draws any observation appears in per\n",
    "#' unique interval it is comparable to.\n",
    "#' @param alpha: quantile of least deep observations to drop before bootstrapping\n",
    "#' (in approximation of C)\n",
    "#' @param B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "#' @param gamma: tuning parameter for smoothed bootstrap\n",
    "#' @param debug: choose whether text outputs are given\n",
    "#'\n",
    "#' @return placeholder\n",
    "#' @export\n",
    "stretch_sample_detection <- function(cl, list_path, lambda, measuring_intervals,\n",
    "                                     n_samples = NULL, sample_size, expn = NULL,\n",
    "                                     alpha, B, gamma, debug = FALSE) {\n",
    "  # Set up boolean for later\n",
    "  if (missing(n_samples)) {\n",
    "    n_sample_bool <- FALSE\n",
    "  } else {\n",
    "    n_sample_bool <- TRUE\n",
    "  }\n",
    "\n",
    "  # get number of observations in list\n",
    "  n_obs <- getListLength(file = list_path)\n",
    "  tmp_ids <- 1:n_obs\n",
    "\n",
    "  # find unique measuring intervals\n",
    "  unique_intervals <- unique_intervals(interval_matrix = measuring_intervals)\n",
    "  n_unique_int <- dim(unique_intervals)[1]\n",
    "\n",
    "  if (debug) {\n",
    "    print(paste0(\"There are \", n_unique_int, \" unique measuring intervals.\"))\n",
    "  }\n",
    "\n",
    "  # create vectors as described in the description part\n",
    "  num_samples <- rep(x = 0, times = n_obs)\n",
    "  num_outliers <- rep(x = 0, times = n_obs)\n",
    "  frac_outliers <- rep(x = 1, times = n_obs)\n",
    "\n",
    "  # iteration process\n",
    "  for (i in 1:n_unique_int) {\n",
    "    current_interval <- unique_intervals[i, ]\n",
    "\n",
    "    # set fail counter to 0 update with try catch procedure\n",
    "    fail_counter <- 0\n",
    "\n",
    "    # print out current measuring interval\n",
    "    if (debug) {\n",
    "      print(paste0(\"Interval \", i, \" out of \", n_unique_int))\n",
    "      print(paste0(\"Current Interval: \", current_interval[1], \" to \", current_interval[2]))\n",
    "    }\n",
    "\n",
    "    # find comparable observations\n",
    "    comparable <- comparable_obs_finder(\n",
    "      main_interval = unique_intervals[i, ],\n",
    "      measuring_intervals = measuring_intervals,\n",
    "      lambda = lambda, ids = tmp_ids\n",
    "    )$ind\n",
    "\n",
    "    # find number of comparable observations\n",
    "    n_comparables <- length(comparable)\n",
    "\n",
    "    # determine n_samples dynamically\n",
    "    if (n_sample_bool == FALSE) {\n",
    "      if (missing(expn)) {\n",
    "        stop(\"Either n_samples or expn has to be provided.\")\n",
    "      } else {\n",
    "        n_samples <- sampling_number(\n",
    "          n_comparables = n_comparables,\n",
    "          sample_size = sample_size,\n",
    "          expn = expn\n",
    "        )\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # print out number of comparables and chosen number of samples\n",
    "    if (debug) {\n",
    "      print(paste0(\"Number of comparable observations: \", n_comparables))\n",
    "      print(paste0(\"Number of samples: \", n_samples))\n",
    "    }\n",
    "\n",
    "    # switch cases if sample_size > n_comparables\n",
    "    if (n_comparables > sample_size) {\n",
    "\n",
    "      # use stretching and sampling procedure for those comparable sets\n",
    "      tmp_sample_res <- stretch_sample_wrap(\n",
    "        cl = cl, list_path = list_path, main_interval = current_interval,\n",
    "        indeces = comparable, n_samples = n_samples, sample_size = sample_size,\n",
    "        alpha = alpha, B = B, gamma = gamma\n",
    "      )\n",
    "\n",
    "      # update the vectors\n",
    "      num_samples[comparable] <- num_samples[comparable] + tmp_sample_res$num_samples\n",
    "      num_outliers[comparable] <- num_outliers[comparable] + tmp_sample_res$num_outliers\n",
    "    } else {\n",
    "\n",
    "      # determine factor for num_samples and num_outliers\n",
    "      if (missing(expn)) {\n",
    "        tmp_factor <- sampling_factor(\n",
    "          n_comparables = n_comparables, sample_size = sample_size, n_samples = n_samples\n",
    "        )\n",
    "      } else {\n",
    "        tmp_factor <- expn\n",
    "      }\n",
    "\n",
    "      if (n_comparables == 1) {\n",
    "        # if no other comparable observation exists classify\n",
    "        # observation as outlier\n",
    "        num_samples[comparable] <- num_samples[comparable] + tmp_factor\n",
    "        num_outliers[comparable] <- num_outliers[comparable] + tmp_factor\n",
    "      } else {\n",
    "        # load data from large list\n",
    "        tmp_dat <- readList(file = list_path, index = comparable)\n",
    "\n",
    "        # stretch data to main interval\n",
    "        tmp_stretch <- stretch_data(\n",
    "          func_dat = tmp_dat, measuring_interval = current_interval\n",
    "        )\n",
    "\n",
    "        # use detection procedure to identify outliers\n",
    "        tmp_res <- detection_wrap(\n",
    "          func_dat = tmp_stretch, ids = comparable,\n",
    "          alpha = alpha, B = B, gamma = gamma\n",
    "        )$outlier_ids\n",
    "\n",
    "        # update vectors accordingly\n",
    "        num_samples[comparable] <- num_samples[comparable] + tmp_factor\n",
    "        num_outliers[tmp_res] <- num_outliers[tmp_res] + tmp_factor\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # calculate the relative frequency of outliers\n",
    "  frac_outliers <- unlist(map(\n",
    "    .x = 1:n_obs,\n",
    "    .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i] / num_samples[i], 1)\n",
    "  ))\n",
    "\n",
    "  # Return the three vectors\n",
    "  return(list(\n",
    "    num_samples = num_samples,\n",
    "    num_outliers = num_outliers,\n",
    "    certainties = frac_outliers\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97a136-e4d1-412c-83c6-d80d4ca17bae",
   "metadata": {},
   "source": [
    "### Updating <a name=\"impl_upd\"></a>\n",
    "The updating procedure is not yet implemented. This is why this part is a description of the functions and their implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80c8c6-f907-432a-ab94-de95c840b0b6",
   "metadata": {},
   "source": [
    "#### Find measuring intervals with possible occurences of new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f6e2-6870-4de6-b5d3-9c38e8d2c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_func_obs: list with two elements: vectors of equal length called args and vals\n",
    "# new_id: identifier for the new observation (be careful not to create duplicates - for example just consecutive intergers would be fine)\n",
    "# list_path: path to the largeList object, where data set is saved\n",
    "# id_path: path to the RDS file with the identifiers\n",
    "\n",
    "obs_append <- function(new_func_obs, new_id, list_path, id_path){\n",
    "    \n",
    "    # read in previous ids\n",
    "    ids <- readRDS(file = id_path)\n",
    "    # append new ID\n",
    "    new_ids <- c(ids, new_id)\n",
    "    # overwrite old ID file\n",
    "    saveRDS(object = new_ids, file = id_path)\n",
    "    \n",
    "    # largeLists support appending\n",
    "    saveList(object = list(new_func_obs), file = list_path, append = TRUE) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf20fd-082a-400f-a140-01aabffb6d9b",
   "metadata": {},
   "source": [
    "#### Appending observation to list of functional data and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e681007-2e36-4d5c-a911-3963e6dee1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_func_obs: list with two elements: vectors of equal length called args and vals\n",
    "# unique_intervals: matrix containing measuring intervals that occur in the data set (one in each row)\n",
    "# lambda: acceptable stretching parameter\n",
    "\n",
    "possible_occurences <- function(new_func_obs, unique_intervals, lambda){\n",
    "    \n",
    "    # determine measuring interval of new observation\n",
    "    new_measuring_interval <- c(min(new_func_obs$args),  max(new_func_obs$args))\n",
    "    \n",
    "    # determine for all previously used measuring intervals if the new\n",
    "    # observation could have been part of the stretching and sampling procedure\n",
    "    occurs <- map(.x = 1:(dim(unique_intervals)[1]),\n",
    "                      .f = function(i) ifelse(new_measuring_interval[2] >= unique_intervals[i,2]/lambda \n",
    "                                              & new_measuring_interval[2] <= unique_intervals[i,2]*lambda,\n",
    "                                              TRUE, FALSE))\n",
    "                  \n",
    "    # return the indices and intervals that the new observation could have been a part of\n",
    "    return(list(occurs = occurs,\n",
    "                occurs_intervals = unique_intervals[occurs, ]))                         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c054c-9d96-49ae-8bf7-8bbc184e35a9",
   "metadata": {},
   "source": [
    "#### Determine expected number of occurences in this measuring interval sampling run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b7d1a-6eec-41ac-b08b-3320d5808f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_n_obs: number of observations in the original comparable set (use output from comparable_obs_finder to determine this)\n",
    "# orig_n_samples: number of samples drawn in the original procedure (this can be updated by adding those drawn now for future updates)\n",
    "# orig_sample_size: sample size used in the original prrocedure for this comparable subset\n",
    "\n",
    "exp_num_samples <- function(orig_n_obs, orig_n_samples, orig_sample_size){\n",
    "    \n",
    "    # determine the probability that the observation would have been part of any original sample,\n",
    "    # if it had been part of the data set\n",
    "    # exp_per_sample <- choose(orig_n_obs, orig_sample_size - 1) / choose(orig_n_obs + 1, orig_sample_size)\n",
    "    # simplifies to:\n",
    "    exp_per_sample <- orig_sample_size / orig_n_obs\n",
    "    \n",
    "    # multiplay with the number of samples and return the number rounded to the next whole integer\n",
    "    return(ceiling(orig_n_samples * exp_per_sample))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd71ea-89f7-4c5a-b956-7bbd3484954f",
   "metadata": {},
   "source": [
    "#### Update one measuring interval\n",
    "\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a3968-f150-454b-92d0-ed82f0090b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c71800b-7462-43c5-8c5f-d5ad29c4bbd8",
   "metadata": {},
   "source": [
    "#### Updating for one new observation\n",
    "\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb4245-ce92-4b70-8106-39eb39b17932",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_update <- function(cl, new_func_obs, list_path, measuring_intervals, lambda, n_samples = NULL, \n",
    "                       prev_num_samples, prev_num_outliers, sample_size = NULL, alpha = NULL, B = NULL, \n",
    "                       gamma = 0.05){\n",
    "    \n",
    "    main_interval <- c(min(new_func_obs$args),  max(new_func_obs$args))\n",
    "    \n",
    "    comparable <- comparable_obs_finder\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c356220-5fef-40ba-adb6-47fc1b157731",
   "metadata": {},
   "source": [
    "## How to use the package?<a name=\"how_to\"></a>\n",
    "---\n",
    "After showing the implementation of the procedure in the package **OutDetectR** the following segement will show a typical workflow to identify outliers in a data set of appropriate format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bbade-ea3b-4a38-b2f9-45ce8cfdc83c",
   "metadata": {},
   "source": [
    "## Simulated Data <a name=\"simulated\"></a>\n",
    "---\n",
    "To show possible usecases of the implementation I create three datasets:\n",
    "1. **No sampling** <br> This data set shows the outlier classification procedure of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) in action. It is characterized by:\n",
    "    * Observations with identical measuring intervals \n",
    "    * Relatively few observations meaning that the algorithm can be applied without sampling <br>\n",
    "2. **Sampling** <br>A data set where observations share the measuring interval but which is large enough that sampling is necessary to use the algorithm <br>\n",
    "3. **Full procedure**\n",
    "    * A dataset where observations share the starting point of the measuring interval but have different endpoints. \n",
    "    * This is meant to show the full procedure on a data set that is effectively already zeroed.\n",
    "\n",
    "The functional form of the non-outliers in these data sets is approximately **linear** which has two reasons: <br>\n",
    "1. Simplicity \n",
    "2. The approximate linearity of the relationship that is fundamental to the main data set explored in this project (angle and torque when tightening a bolt). \n",
    "\n",
    "All of these data sets and the results of the outlier classification procedures applied to them can be seen in the **shiny app** provided in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbf22e-8b68-4476-a17f-4dd738487334",
   "metadata": {},
   "source": [
    "### No sampling:\n",
    "This data set is based on a simple data generating process. Each of the 500 observations is generated as follows:\n",
    "\n",
    "* Determine if the observation is generated as an outlier by drawing from a Bernoulli distributed random variable with parameter 0.05.\n",
    "* Determine the number of points $k$ where the function is observed. (In the terms of the method above the number of points where measurements are taken) This is drawn from a discrete uniform with elements $10, \\dots, 100$\n",
    "\n",
    "The generation of dependent variables depends on whether the observation is generated as an outlier. The general process is as follows: <br>\n",
    "\n",
    "* Draw k-2 realizations of $p \\sim U[0, 1] \\quad \\text{s.t.} \\quad p_1, \\dots, p_k \\quad \\text{i.i.d.}\\quad$ Let $p_{(1)}, \\dots, p_{(k-2)}$ be the sorted realizations and define $(0, \\: p_{(1)},\\: \\dots, \\: p_{(k-2)},\\: 1)^T = \\vec{p}$ <br>(This is equivalent to our grid of measuring points.)<br>\n",
    "* Draw k realizations of $s \\sim U[\\underset{\\bar{}}{s}, \\bar{s}] \\quad \\text{s.t.} \\quad s_1, \\dots, s_k \\quad \\text{i.i.d.} \\quad$ and define $(s_1,\\: \\dots, \\: s_k)^T = \\vec{s}$\n",
    "* Draw k realizations of $\\epsilon \\sim \\mathcal{N}[0, \\sigma] \\quad \\text{s.t.} \\quad \\epsilon_1, \\dots, \\epsilon_k \\quad \\text{i.i.d.} \\quad$ and define $(\\epsilon_1,\\: \\dots, \\: \\epsilon_k)^T = \\vec{\\epsilon}$\n",
    "* Let $\\vec{y} = m \\vec{s} \\odot \\vec{p} + \\vec{\\epsilon}$ be the vector of realizations of the dependent variable, where $\\odot$ is the component-wise (or Hadamard) product.\n",
    "\n",
    "The parameters are different for non-outliers and outliers: <br>\n",
    "For non-outliers:\n",
    "* $\\underset{\\bar{}}{s} = 0.8, \\quad \\bar{s} = 1.2$\n",
    "* $\\sigma = 0.05$\n",
    "* $m = 1.02$\n",
    "\n",
    "and for outliers:\n",
    "* $\\underset{\\bar{}}{s} = 1, \\quad \\bar{s} = 1.4$\n",
    "* $\\sigma = 0.1$\n",
    "* $m = 1.2 * 1.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09765d-54e6-4bcc-9bfb-6cb92533309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_1.R\n",
    "# Data set is also saved to ./data/Set_1 and if existent read from there instead\n",
    "# source(\"auxiliary/generate_set_1.R\")\n",
    "\n",
    "if(file.exists(\"./data/Set_1/functional.llo\") && \n",
    "   file.exists(\"./data/Set_1/ids.RDS\") && \n",
    "   file.exists(\"./data/Set_1/outliers.RDS\")){\n",
    "    data_set_1 <- list(data = readList(file = \"./data/Set_1/functional.llo\"),\n",
    "                       ids = readRDS(file = \"./data/Set_1/ids.RDS\"),\n",
    "                       outliers = readRDS(file = \"./data/Set_1/outliers.RDS\"))\n",
    "\n",
    "} else{\n",
    "    data_set_1 <- generate_set_1()\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_1/tibble.RDS\")){\n",
    "   tidy_set_1 <- readRDS(\"./data/Set_1/tibble.RDS\") \n",
    "} else{\n",
    "   tidy_set_1 <- tidify_1(data_set_1$data, data_set_1$ids)\n",
    "   saveRDS(object = tidy_set_1, file = \"./data/Set_1/tibble.RDS\")\n",
    "}\n",
    "                        \n",
    "vis_1(tidy_set_1)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c8411-cfe8-4051-bc0d-b09531fd2407",
   "metadata": {},
   "source": [
    "Prepare data for visualization in the shiny app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603716c4-100a-48d2-8af8-e19d0153f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(file.exists(\"./data/Set_1/detection.RDS\")){\n",
    "    set_1_detection <- readRDS(\"./data/Set_1/detection.RDS\")\n",
    "} else{\n",
    "    set_1_detection <- detection_wrap(func_dat = data_set_1$data, alpha = 0.05, B = 50, gamma = 0.05, ids = data_set_1$ids)\n",
    "    saveRDS(object = set_1_detection, file = \"./data/Set_1/detection.RDS\")\n",
    "}\n",
    "\n",
    "if(file.exists(\"./data/Set_1/summary.RDS\")){\n",
    "    set_1_summary <- readRDS(\"./data/Set_1/summary.RDS\")\n",
    "} else{\n",
    "    \n",
    "    missed_outliers = setdiff(data_set_1$outliers, set_1_detection$outlier_ind)\n",
    "    false_outliers = setdiff(set_1_detection$outlier_ind, data_set_1$outliers)\n",
    "    \n",
    "    set_1_summary <- list(flagged = set_1_detection$outlier_ind,\n",
    "                          original = data_set_1$outliers,\n",
    "                          missed = missed_outliers,\n",
    "                          false = false_outliers)\n",
    "    \n",
    "    saveRDS(object = set_1_summary, file = \"./data/Set_1/summary.RDS\")\n",
    "}\n",
    "\n",
    "if(file.exists(\"./data/Set_1/shiny_tibble.RDS\")){\n",
    "    shiny_tibble_1 <- readRDS(\"./data/Set_1/shiny_tibble.RDS\")\n",
    "} else{\n",
    "    shiny_tibble_1 <- tidy_set_1 %>%\n",
    "        mutate(outlier = ifelse(ids %in% data_set_1$outliers, TRUE, FALSE),\n",
    "               flagged = ifelse(ids %in% set_1_detection$outlier_ind, TRUE, FALSE))\n",
    "    saveRDS(object = shiny_tibble_1, file = \"./data/Set_1/shiny_tibble.RDS\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6584ad-0190-40ab-b03d-dde1a9167a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sampling:\n",
    "This second data set consists of 10000 observations where 95% are generated by the same process as the non-outlier variant in the previous data set. \n",
    "So a vector of arguments is drawn from the continuous uniform on $[0,1]$ of random length. Call this vector *args* in the following description.\n",
    "\n",
    "But the outliers take 5 different forms each appearing in about one percent of cases. I will describe their data generating processes in mathematical form in the following:\n",
    "\n",
    "1. Exactly as the outliers in data set 1.\n",
    "\n",
    "2. Sigmoid function: $\\frac{1}{\\mathbb{1} + \\exp{(-3 * \\text{args})}} + \\epsilon$ where $\\epsilon$ is a vector of appropriate length containing i.i.d. elements drawn from $\\mathcal{N}(0, 0.05^2)$\n",
    "\n",
    "3. Half sigmoid function: $\\frac{2}{\\mathbb{1} + \\exp{(-3 * \\text{args})}} - \\mathbb{1} + \\epsilon$ where $\\epsilon$ is a vector of appropriate length containing i.i.d. elements drawn from $\\mathcal{N}(0, 0.05^2)$\n",
    "\n",
    "4. Exponential function: $\\frac{\\exp(args) - \\mathbb{1}}{e - 1} + \\epsilon$ where $\\epsilon$ is a vector of appropriate length containing i.i.d. elements drawn from $\\mathcal{N}(0, 0.05^2)$\n",
    "\n",
    "5. Noise: i.i.d. draws from $U[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b225ffc-0baf-4164-862d-d86ee32d2bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_2.R\n",
    "# Data set is also saved to ./data/Set_2 and if existent read from there instead\n",
    "# source(\"auxiliary/generate_set_2.R\")\n",
    "\n",
    "if(file.exists(\"./data/Set_2/functional.llo\") && \n",
    "   file.exists(\"./data/Set_2/ids.RDS\") && \n",
    "   file.exists(\"./data/Set_2/outliers.RDS\")){\n",
    "    data_set_2 <- list(data = readList(file = \"./data/Set_2/functional.llo\"),\n",
    "                       ids = readRDS(file = \"./data/Set_2/ids.RDS\"),\n",
    "                       outliers = readRDS(file = \"./data/Set_2/outliers.RDS\"))\n",
    "\n",
    "} else{\n",
    "    data_set_2 <- generate_set_2()\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_2/tibble.RDS\")){\n",
    "   tidy_set_2 <- readRDS(\"./data/Set_2/tibble.RDS\") \n",
    "} else{\n",
    "   tidy_set_2 <- tidify_2(data_set_2$data, data_set_2$ids)\n",
    "   saveRDS(object = tidy_set_2, file = \"./data/Set_2/tibble.RDS\")\n",
    "}\n",
    "\n",
    "vis_2(tidy_set_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8a82d-c406-4f52-94ac-3835ab71f880",
   "metadata": {
    "tags": []
   },
   "source": [
    "Due to the computational cost of this classifying procedure I saved its results and only perform the classification, if those results could not be found. In the case of 10000 observations can still be done on the full set and I opted to use this lower number of observations not due to computational problems with larger data sets but due to overplotting in the visualizations, that has to be addressed, before large data sets can be appropriately visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c92f19-6228-4c42-9dda-55012e4c5182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(file.exists(\"./data/Set_2/results.RDS\")){\n",
    "    set_2_results <- readRDS(file = \"./data/Set_2/results.RDS\")\n",
    "\n",
    "} else{\n",
    "    num_cores <- detectCores()\n",
    "    cl <- makeForkCluster(nnodes = num_cores)\n",
    "                   \n",
    "    clusterExport(cl, varlist = list(\"grid_approx_set_obs\",\n",
    "                                     \"approx_C\",\n",
    "                                     \"grid_finder\",\n",
    "                                     \"outlier_iteration\",\n",
    "                                     \"outlier_detection\",\n",
    "                                     \"detection_wrap\",\n",
    "                                     \"random_access_par_helper\"),\n",
    "                                      envir = .GlobalEnv)\n",
    "                          \n",
    "    set_2_results <- sampling_wrap(cl = cl, n_obs = 10000, n_samples = 150, \n",
    "                               sample_size = 500, alpha = 0.05, B = 100, gamma = 0.05, \n",
    "                               list_path = \"./data/Set_2/functional.llo\")   \n",
    "    \n",
    "    stopCluster(cl)    \n",
    "    \n",
    "    saveRDS(object = set_2_results, file = \"./data/Set_2/results.RDS\")\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_2/shiny_tibble.RDS\")){\n",
    "    shiny_tibble_2 <- readRDS(\"./data/Set_2/shiny_tibble.RDS\")\n",
    "} else{\n",
    "    shiny_tibble_2 <- tidy_set_2 %>%\n",
    "        mutate(outlier = ifelse(ids %in% data_set_2$outliers, TRUE, FALSE))\n",
    "    \n",
    "    lengths <- unlist(map(.x = 1:10000,\n",
    "                          .f = function(i) length(data_set_2$data[[i]]$vals)))\n",
    "                          \n",
    "    shiny_tibble_2$cert <- unlist(map(.x = 1:10000,\n",
    "                                      .f = function(i) rep(set_2_results$certainties[i], times = lengths[i])))\n",
    "    \n",
    "    saveRDS(object = shiny_tibble_2, file = \"./data/Set_2/shiny_tibble.RDS\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90385-2b80-447f-8cbb-d7e6fde77e58",
   "metadata": {
    "tags": []
   },
   "source": [
    "To see the results of this classification procedure for different values of the certainty threshold, you can use the shiny app provided in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230fcd-42c5-411b-926d-b18fc2068b93",
   "metadata": {},
   "source": [
    "### Full Procedure:\n",
    "This data set is similar to the sampling data set in its data generating process. There are 30000 observations each generated by a similar process as in data set 2.\n",
    "\n",
    "The main difference is, that the measuring intervals are not identical and are drawn from the following possibilities:\n",
    "\n",
    "| Endpoint of Measuring Interval \t| 0.9  \t| 1   \t| 1.1  \t| 1.5  \t| 1.6  \t| 1.7  \t| 1.9 \t| 2    \t| 2.1  \t|\n",
    "|:------------------------------:\t|------\t|-----\t|------\t|------\t|------\t|------\t|-----\t|------\t|------\t|\n",
    "|           Probability          \t| 0.05 \t| 0.2 \t| 0.05 \t| 0.07 \t| 0.15 \t| 0.08 \t| 0.1 \t| 0.25 \t| 0.05 \t|\n",
    "\n",
    "Also the data generating processes for both the outliers and the non-outliers are scaled such that the expected realizations at the beginning and end of the data set are identical. (Especially important for the exponential and sigmoidal outliers.) An exception from this are the type 5 outliers, which stay uniformly distributed - but the upper border of the support is made to fit the expected realization of the non-outlier process at the end point of the measuring interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9bd7e-e050-435a-a5e6-7cd6a650948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_3.R\n",
    "# Data set is also saved to ./data/Set_3 and if existent read from there instead\n",
    "source(\"auxiliary/generate_set_3.R\")\n",
    "\n",
    "if(file.exists(\"./data/Set_3/functional.llo\") && \n",
    "   file.exists(\"./data/Set_3/ids.RDS\") && \n",
    "   file.exists(\"./data/Set_3/outliers.RDS\")){\n",
    "    data_set_3 <- list(data = readList(file = \"./data/Set_3/functional.llo\"),\n",
    "                       ids = readRDS(file = \"./data/Set_3/ids.RDS\"),\n",
    "                       outliers = readRDS(file = \"./data/Set_3/outliers.RDS\"))\n",
    "\n",
    "} else{\n",
    "    data_set_3 <- generate_set_3()\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_3/tibble.RDS\")){\n",
    "   tidy_set_3 <- readRDS(\"./data/Set_3/tibble.RDS\") \n",
    "} else{\n",
    "   tidy_set_3 <- tidify_3(data_set_3$data, data_set_3$ids)\n",
    "   saveRDS(object = tidy_set_3, file = \"./data/Set_3/tibble.RDS\")\n",
    "}\n",
    "\n",
    "### Due to a lengthy plotting time and overplotting, this plot is saved an inserted as a png\n",
    "# vis_3(tidy_set_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee7ba9-9249-49e0-a5e7-0cc8f656d567",
   "metadata": {},
   "source": [
    "<img src=\"material/set_3.png\" width=\"1000\" align=\"center\"> \n",
    "\n",
    "Full procedure works - update this!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcb226-20c2-451c-bc02-ad9134a0f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(file.exists(\"./data/Set_3/results.RDS\")){\n",
    "   set_3_results <- readRDS(file = \"./data/Set_3/results.RDS\")\n",
    "} else{\n",
    "    num_cores <- detectCores()\n",
    "    cl <- makeForkCluster(nnodes = num_cores)\n",
    "                   \n",
    "    clusterExport(cl, varlist = list(\"grid_approx_set_obs\",\n",
    "                                     \"approx_C\",\n",
    "                                     \"grid_finder\",\n",
    "                                     \"outlier_iteration\",\n",
    "                                     \"outlier_detection\",\n",
    "                                     \"detection_wrap\",\n",
    "                                     \"random_access_par_helper\"),\n",
    "                                      envir = .GlobalEnv)\n",
    "    \n",
    "    measuring_intervals <- measuring_int(func_dat = data_set_3$data)\n",
    "\n",
    "    comp_test <- comparable_obs_finder(main_interval = c(0,1), measuring_intervals = measuring_intervals,  lambda = 1.2, ids = 1:30000)$ids\n",
    "    \n",
    "    #test <- stretch_and_sample(cl = cl, n_samples = 100, sample_size = 100, alpha = 0.02, B = 100, gamma = 0.05, n_obs = 30000,\n",
    "    #                           list_path = \"./data/Set_3/functional.llo\", measuring_interval = c(0,1), \n",
    "    #                           comparable = comp_test)\n",
    "    \n",
    "    set_3_results <- dectection_zr_smpl(cl = cl, list_path = \"./data/Set_3/functional.llo\", measuring_intervals = measuring_intervals,\n",
    "                                        n_obs = 30000, lambda = 1.2, n_samples = 300, sample_size = 300, alpha = 0.02, B = 100, gamma = 0.05)\n",
    "    \n",
    "    stopCluster(cl)    \n",
    "    \n",
    "   saveRDS(object = set_3_results, file = \"./data/Set_3/results.RDS\")\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_3/shiny_tibble.RDS\")){\n",
    "    shiny_tibble_3 <- readRDS(\"./data/Set_3/shiny_tibble.RDS\")\n",
    "} else{\n",
    "    shiny_tibble_3 <- tidy_set_3 %>%\n",
    "        mutate(outlier = ifelse(ids %in% data_set_3$outliers, TRUE, FALSE))\n",
    "    \n",
    "    lengths <- unlist(map(.x = 1:30000,\n",
    "                          .f = function(i) length(data_set_3$data[[i]]$vals)))\n",
    "                          \n",
    "    shiny_tibble_3$cert <- unlist(map(.x = 1:30000,\n",
    "                                      .f = function(i) rep(set_3_results$certainties[i], times = lengths[i])))\n",
    "    \n",
    "    saveRDS(object = shiny_tibble_3, file = \"./data/Set_3/shiny_tibble.RDS\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968a0b3-972f-4761-abe3-c4c7758a5cf5",
   "metadata": {},
   "source": [
    "## Endanzug-Data <a name=\"application\"></a>\n",
    "---\n",
    "When applying the procedure to the real world data from the Endanzug dataset, I decided to use a subset of 20000 randomly chosen observations. Using the whole dataset is unfeasible with a typical computer and would require further optimizations and the usage of high-performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0314d9f-f149-4d25-80c0-9f05e1813e3f",
   "metadata": {},
   "source": [
    "## Shiny App <a name=\"shiny\"></a>\n",
    "---\n",
    "Instead of presenting the visualizations for the classification procedure in static graphics, I decided to use a shiny web app instead. This has multiple advantages in the setting of this final project, but the main motivation behind it was the use case described to me:<br> **An engineer wants to get a preselection of suspicious observations they should have a look at.**\n",
    "\n",
    "In this context, having a raw R file as output without an easy way to interact with it, would not be particularly useful. Instead, being able to run this shiny app on a local server with the precalculated values stored in a database, would be more in line with the idea of the job. This also motivates the features I implemented for the app (and plan to implement in future updates) like:\n",
    "* setting the focus to single observations\n",
    "* changing the plotting window\n",
    "* changing the centrainty threshold for observations to be classified as atypical\n",
    "\n",
    "etc.\n",
    "\n",
    "To start the shiny app, I recommend cloning this repository and executing the file **app.R** locally. This will start the shiny app on a local server. Instead, one can choose to use the **binder** button on the repo site to start it. Due to a lengthy build process, this is not the recommended way to look at it.\n",
    "\n",
    "The shiny app serves as the visualization for all results of the previously explained simulation studies and shows what a possible deployment of the method in a real world scenario could look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19790688-55b5-4fc3-9c4a-1d33ab43a429",
   "metadata": {},
   "source": [
    "## Outlook <a name=\"outlook\"></a>\n",
    "---\n",
    "The current state of the project and the implementation can serve as an example for the capabilities of the functional depth approach described by \n",
    "\n",
    "### Finalize Implementation of Updating Procedure\n",
    "In the current state of the project, the implementation of the updating procedure is incomplete. Future revisions will fill this incompleteness and add simulations to show the process of updating and compare cases where observations were added in an updating procedure to data sets containing them from the beginning. This can serve as a device to check the validity of the updating procedure.\n",
    "\n",
    "### Improvements of Implementation\n",
    "The implementation above still has some other problems. The most striking is the tendency of the sampling procedure to mark a full sample as atypical, if it contains a lower than expected fraction of atypical observations. <br>\n",
    "I plan to address this by introducing a rescaling factor to the cutoff threshold $C$ that adjusts for the removal of observations from the sample currently under consideration. This seems reasonable, as adding a new observation to a data set will always increase calculated depths for all observations. Therefore, removing observations can lead to overall lower depths and incrementally lower all observations under the cutoff that determines if they are classified as atypical in a sample. This is not described in the paper this project is based on - so it will make some theoretical work necessary to develop an appropritae correction.\n",
    "\n",
    "### Generalizations\n",
    "As described in a previous section, the method described in this project for data sets which allow for zeroing of the observations could be generalized by introducing another parameter **acceptable shifting** in addition to **acceptable stretching**. This could then be used to define a method that finds comparable subsets in data sets that do not allow zeroing. As this introduces more variables and makes a more sophisticated splitting algorithm necessary, it was out of the scope of this project, but will be addressed in the future.\n",
    "\n",
    "### Parameter Choice\n",
    "An important part of this procedure will be the choice of its tuning parameters: \n",
    "* In its purest form the algorithm needs a choice for $\\alpha$, $\\gamma$, $B$ and a grid to use for approximation purposes\n",
    "* Adding sampling adds the choice of sample size and number of samples\n",
    "* The full procedure then additionally needs $\\lambda$\n",
    "\n",
    "Some of these like sample size and number of samples could be made dependent on the structure of comparable subsets and even change when switching from one comparable subset to the next. One goal of this could be to make each observation appear in a similar number of samples overall. But other reasonable procedures are possible. <br>\n",
    "Others like $\\alpha$ and $\\lambda$ could be chosen by a simulation method. Constructing a similar but smaller data set with intentionally added outliers to perform cross validation or a similar procedure could be an approach for this case. A more sophisticated and detailed description of this method will be part of future revisions.\n",
    "\n",
    "### Making Results reproducible\n",
    "Currently, the results of the method are mostly non-reproducible when taking about exact numbers. Qualitative results of the procedure will be similar, but due to randomization in the method the replication of exact numbers is currently not possible. This can be addressed with some work to allow for manually setting seeds in the classification algorithm without sampling and in the choice of samples. Because of parallelization this cannot be done be setting the seed once in the beginning.\n",
    "\n",
    "### Performance Measures for the Algorithm & Benchmarking\n",
    "Tightly connected to the point of parameter choice is the question of how to measure the performance of the outlier detection procedure in different settings. \n",
    "* First, I am going to look for existing data sets that are commonly used to benchmark outlier classification procedures. The performance of this algorithm in these preclassified settings can serve as grounds for determining in which cases and using which parameter choices the method performs well and compare it to existing methods that are applicable in comparable scenarios. <br>\n",
    "* Second, as can be seen by the previously generated data, an ex ante classification of realizations created by different dgps may not appropriately cover the idea of outlyingness. Some realizations in previous data sets look very typical for the non-outlier dgp and a classification as atypical due to the different dgp could lead to an underestimation of the procedures performance. Therefore, a comparison to established methods can serve as a better tool to judge the effectiveness.\n",
    "\n",
    "### Identifying what contributes to outlyingness of an observation\n",
    "Once atypical observations in a data set are identified, it is very interesting to see what contributes to their outlyingness in the eyes of the algorithm. To create some ex-post explanation for why a classification decision was made would be a useful tool to inform future real-world decisions or improve the procedure itself by incorporating that information into the mechanism. Some interesting approaches to create an ex-post explanation are the following:\n",
    "\n",
    "1. Create slightly altered realizations of an observation that has been marked as an outlier and see what effect different alterations have on its classification (or certainty in case of the sampling-based methods).\n",
    "2. Compare locally similar observations that have different outcomes in the overall procedure.\n",
    "\n",
    "There are more ways to gain information on what features contribute to outlyingness, but due to the current scope of this project, more in-depth considerations of this approach will only be part of future revisions.\n",
    "\n",
    "### Creation of an Rcpp-Package to improve usability\n",
    "Further iterations of this project will be different in the way the functions are made available. Instead of implementing the functions directly in the main notebook, there will be an additional notebook explaining the implementation and an R package that can be installed directly that contains all functionality provided above. This will make it easier to use the functions in the future.\n",
    "\n",
    "### Improvements to the Shiny App\n",
    "The visualizations of the shiny app will also undergo considerable overhauls, starting with changes to which observations are plotted when focus is set to a single observation. In this case for example plotting more observations in the close vicinity of the observation might be of greater interest, whereas observations farther away may provide little information of use and could be plotted less frequently to avoid overplotting. <br>\n",
    "There are further improvements planned but due to the interactive process of designing an interface that is meant for direct interaction with the user, it is difficult to predict the exact nature of the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453be942-e54a-45c6-a95c-96e68244202c",
   "metadata": {},
   "source": [
    "## Sources <a name=\"sources\"></a>\n",
    "---\n",
    "* Cuevas, A. & Febrero-Bande, M. & Fraiman, R. (2006). On the use of bootstrap for estimating functions with functional data. Computational Statistics & Data Analysis. 51. 1063-1074.\n",
    "* Febrero-Bande, M. & Galeano, P. & Gonzàlez-Manteiga, W. (2008). Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels. Environmetrics. 19. 331 - 345.\n",
    "* Gijbels, I. & Nagy, S. (2017). On a General Definition of Depth for Functional Data. Statistical Science. 32. 630-639."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b06320-862b-409a-9d6e-e462e2de810d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b129f37-77fc-410b-a8eb-340ab5d4a010",
   "metadata": {},
   "source": [
    "Notebook by **Jakob R. Jürgens** <br>\n",
    "Final project for the course **OSE - data science** in the summer semester 2021<br>\n",
    "Find me at [jakobjuergens.com](https://jakobjuergens.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
