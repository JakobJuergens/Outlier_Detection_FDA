{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e686c32d-9705-46d5-8900-70267c5e2de8",
   "metadata": {},
   "source": [
    "My idea was to approach the problem of classifying outliers in these data sets using methods from functional data analysis. The procedure is based on what is called functional depth which was originally conceived to develop centerality measures for functional data akin to the median in scalar data for example. Using these functional centrality measures to detect outliers in functional data sets is currently one of the most promising approaches in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936d24b8-d34d-4fb1-a1ef-abe697fd0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(library(MASS))\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(shiny))\n",
    "suppressMessages(library(shinydashboard))\n",
    "suppressMessages(library(largeList))\n",
    "suppressMessages(library(parallel))\n",
    "suppressMessages(library(Rcpp))\n",
    "\n",
    "set.seed(17203476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4538296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the simulation:\n",
    "n <- 2000\n",
    "ids <- as.character(1:n)\n",
    "outliers <- rbinom(n = n, size = 1, prob = 0.05)\n",
    "\n",
    "lengths <- sample(x = 10:100, size = n, replace = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91c9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process for drawing one realisation of functional data\n",
    "random_dat <- function(grid, slope, out){\n",
    "    args <- grid\n",
    "    if(out == 0){\n",
    "        vals <- runif(n = length(grid), min = 0, max = slope) * slope * args + rnorm(n = length(grid), mean = 0, sd = 0.05)\n",
    "    }\n",
    "    else{\n",
    "        vals <- runif(n = length(grid), min = 0, max = 1.25 * slope) * 1.25 *slope * args + rnorm(n = length(grid), mean = 0, sd = 0.05)\n",
    "    }\n",
    "    \n",
    "    return(list(args = grid,\n",
    "                vals = vals))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f53645-117d-4404-992a-1974f4f3de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions <- map(.x = 1:n,\n",
    "                 .f = function(x) random_dat(grid = seq(from = 0, to = 1, length.out = lengths[x]), slope = 1.02, out = outliers[x]))\n",
    "\n",
    "### Allows for random access in parallelization (saves memory)\n",
    "saveList(functions, \"data/functions.llo\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a9bba-5f06-43e2-84a3-202afaa9826d",
   "metadata": {},
   "source": [
    "This is one example of a process that could be analyzed using depth measures for functional data.\n",
    "The algorithm described by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) then works as follows:\n",
    "(Quoted from the paper)\n",
    "\n",
    "1. Obtain the functional depths $D_n(x_i), \\dots ,D_n(x_n)$ for one of the functional depths [...]\n",
    "2. Let $x_{i_1}, \\dots, x_{i_k}$ be the k curves such that $D_n(x_{i_k}) \\leq C$, for a given cutoff C. Then, assume that $x_{i_1}, \\dots, x_{i_k}$ are outliers and delete them from the sample.\n",
    "3. Then, come back to step 1 with the new dataset after deleting the outliers found in step 2. Repeat this until no more outliers are found.\n",
    "  \n",
    "I chose to use the so called h-mode depth due to its performance in the simulation part of the paper and its simplicity. C is obtained via a bootstrap procedure since theoretical derivation of the functional depths is infeasible in the general case. \n",
    "The procedure works as follows:\n",
    "(Quoted form the paper)\n",
    "\n",
    "1. Obtain the functional depths $D_n(x_i), \\dots ,D_n(x_n)$ for one of the functional depths [...]\n",
    "2. Obtain B standard bootstrap samples of size n from the dataset of curves obtained after deleting the $\\alpha \\%$ less deepest curves. The bootstrap samples are denoted by $x_i^b$ for $i = 1, \\dots, n$ and $b = 1, \\dots, B$.\n",
    "3. Obtain smoothed bootstrap samples $y_i^b = x_i^b + z_i^b$, where $z_i^b$ is such that $(z_i^b(t_1), \\dots, z_i^b(t_m)$ is normally distributed with mean 0 and covariance matrix $\\gamma\\Sigma_x$ where $\\Sigma_x$ is the covariance matrix of $x(t_1), \\dots, x(t_m)$ and $\\gamma$ is a smoothing parameter. Let $y_i^b$, $i = 1, \\dots,n$ and $b = 1,\\dots, B$ be these samples. \\[At this point we assume that our functional observations are observed at a set of discrete points $t_1,\\dots,t_m$, which I implemented by using linear interpolation of neighbouring points, if the chosen grid points are not original observations.\\]\n",
    "4. For each bootstrap set $b = 1,\\dots,B$, obtain $C^b$ as the empirical 1% percentile of the distribution of the depths $D(y_i^b)$, $i = 1, \\dots, n$.\n",
    "5. Take C as the median of the values of $C^b$, $b = 1, \\dots, B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691f97c-011e-48e7-b7df-bf43f4b22e1e",
   "metadata": {},
   "source": [
    "In the setting of the \"Endanzugsproblem\" this has some caveats:\n",
    "1. The runtime complexity of this algorithm is O(n²) and therefore infeasible for the whole data set.\n",
    "2. The observations often do not even share the same interval of arguments.\n",
    "3. The functional observations are not observed at a shared set of discrete arguments.\n",
    " \n",
    "My ideas to address these problems:\n",
    "1. Instead of performing this outlier detection procedure on the whole data set at once, I chose to instead follow a sampling approach:\n",
    "* The overall data set consists of n functional observations. \n",
    "* Draw $P$ many samples of size $q<<n$ from the data set and perform outlier detection on each sample.\n",
    "* For each observation i let $a_i$ be the number of samples it appeared in and $b_i \\leq a_i$ the number of samples it was marked as an outlier in.\n",
    "* For each observation report $c_i = \\frac{b_i}{a_i}$ as a measure of the certainty that an observation is an outlier.\n",
    "* Flag every observation i for which $c_i > W$ as a potential outlier for some prespecified $W \\in ]0,1[$\n",
    "\n",
    "This approach has the advantage of being easily parallelizable, so there is not only potential for speed up due to the runtime complexity of O(n²) but also due to parallelization.\n",
    "\n",
    "2. Due to the nature of the data set it is possible to perform some slight alterations to the data, which make this problem easier to deal with:\n",
    "* It is possible to set the beginning of each process to zero - meaning a subtraction of the first angle which was measured at the beginning of the \"Endanzugsprozess\" from all angles of an observation. So now the problem of differing intervals of arguments is only really a problem of differing interval lengths.\n",
    "* There are many subsets of the data set that share the same interval of arguments. So after the zeroing process described before these are comparable with the described procedure.\n",
    "* There are also a lot of observations that have very similar endpoints after the zeroing process. I suspect that an approximation could be made to make these comparable without losing an important amount of identifying accuracy.\n",
    "\n",
    "### Hier weiterarbeiten\n",
    "\n",
    "3.\n",
    "* It might be possible to modify this algorithm so that once new observations are added, the old scores only have to be updated and not calculated completely anew.\n",
    "* To make these observations with similar observation intervals tangible in this algorithm I used linear interpolation between the points at which the function was originally observed adjacent to the grid points. \n",
    "* When the observation intervals are too dissimilar my idea was to split up the groups and instead look for outliers in group, if the observations are not outliers in another way (for example extreme observation intervals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962a30c-fcbb-4f16-a261-2b6097c25a51",
   "metadata": {},
   "source": [
    "I implemented these functions as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23b9e4a-c803-4a2f-bd7c-4416bef4ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceCpp('auxiliary/rcpp_functions.cpp')\n",
    "# gives access to grid_approx_obs & hM_depth written in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2ab8ba-1483-4edc-9339-daef05ee7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_approx_single_obs <- function(func_obs, grid) {    \n",
    "  return(grid_approx_obs(func_obs$args, func_obs$vals, grid))\n",
    "}\n",
    "\n",
    "grid_approx_set_obs <- function(func_dat, grid) {\n",
    "  res_mat <- matrix(data = unlist(\n",
    "      map(.x = func_dat,\n",
    "          .f = function(obs) grid_approx_single_obs(obs, grid))\n",
    "    ), nrow = length(func_dat), byrow = TRUE)\n",
    "                    \n",
    "  return(res_mat)\n",
    "}\n",
    "\n",
    "# func_dat needs to be observed at the same discretized points for every observation\n",
    "approx_C <- function(matr_data, fdepths, alpha, B, gamma, grid) {\n",
    "    \n",
    "  n <- length(fdepths)\n",
    "  depth_thr <- quantile(x = fdepths, probs = alpha)\n",
    "  matr_data_red <- matr_data[fdepths >= depth_thr, ]\n",
    "  n_red <- dim(matr_data_red)[1]\n",
    "    \n",
    "  Sigma_x <- cov(matr_data_red)\n",
    "  my_vcov <- gamma*Sigma_x  \n",
    "  grid_length <- length(grid)  \n",
    "\n",
    "  sample_inds <- map(.x = 1:B,\n",
    "                     .f = function(x) sample(x = 1:n_red, size = n, replace = TRUE))\n",
    "  \n",
    "  fsamples <- map(.x = sample_inds,\n",
    "                  .f = function(inds) matr_data_red[inds, ])\n",
    "  \n",
    "  smoothing_components <- map(.x = 1:B,\n",
    "                              .f = function(x) mvrnorm(n = n, mu = rep(0, times = grid_length), Sigma = my_vcov))\n",
    "  \n",
    "  smoothed_BS_samples <- map(.x = 1:B,\n",
    "                             .f = function(b) fsamples[[b]] + smoothing_components[[b]])\n",
    "\n",
    "  bootstrap_depths <- map(.x = 1:B,\n",
    "                          .f = function(b) hM_depth(smoothed_BS_samples[[b]]))\n",
    "\n",
    "  one_perc_quantiles <- unlist(map(.x = bootstrap_depths,\n",
    "                                   .f = function(sample) quantile(sample, probs = 0.01)))\n",
    "  \n",
    "  return(median(one_perc_quantiles))\n",
    "}\n",
    "\n",
    "### Like this for testing purposes (plan a method later)\n",
    "grid_finder <- function(func_dat){\n",
    "  return(seq(0, 1, length.out = 100))\n",
    "}\n",
    "\n",
    "### function for the iterative process\n",
    "outlier_iteration <- function(matr_data, alpha, B, gamma, ids, grid){\n",
    "    \n",
    "  fdepths <- hM_depth(matr_data)\n",
    "    \n",
    "  C <- approx_C(matr_data = matr_data, fdepths = fdepths, alpha = alpha,\n",
    "                B = B, gamma = gamma, grid = grid)\n",
    "    \n",
    "  outliers <- which(fdepths < C)\n",
    "  return(list(matr_data = matr_data[-outliers, ],\n",
    "              outlier_ids = ids[outliers],\n",
    "              outliers = outliers))\n",
    "}\n",
    "                                   \n",
    "### function for the whole algorithm\n",
    "outlier_detection <- function(func_dat, alpha, B, gamma, ids){\n",
    "    \n",
    "    condition <- TRUE\n",
    "    outliers <- c()\n",
    "    tmp_ids = ids\n",
    "    i <- 1\n",
    "    \n",
    "    grid <- grid_finder(func_dat)\n",
    "    data <- grid_approx_set_obs(func_dat, grid)\n",
    "    \n",
    "    while(condition){\n",
    "        tmp <- outlier_iteration(matr_data = data, alpha = alpha, B = B, gamma = gamma, ids = tmp_ids, grid = grid)\n",
    "        new_outliers <- tmp$outlier_ids\n",
    "        if(length(new_outliers) == 0){condition <- FALSE}\n",
    "        else{\n",
    "          outliers <- c(outliers, new_outliers)\n",
    "          data <- tmp$matr_data\n",
    "          tmp_ids <- tmp$ids \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return(outliers)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd673af2-f460-4e0e-bf89-420d77e8d68c",
   "metadata": {},
   "source": [
    "It is infeasible to use this algorithm on the full data set due to its runtime complexity of O(n²). So I am going to use a sampling approach, that I will describe in the following:\n",
    "1. Choose a sample of size k from the overall data set (with or without replacement?) that is comparable using functional depth measures. (In the data set I generated here, each subsample is comparable due to their shared observation intervals, but this does not have to be the case.)\n",
    "2. Perform the outlier detection described above on the sample and mark the suspected outliers in the sample.\n",
    "3. Assume that an observation was part of $w$ samples in the overall process and was marked as an outlier in $v \\leq w$ of them. For each observation report $\\frac{v}{w}$ as a certainty measure that an observation is an outlier in the full data set.\n",
    "4. Fix a threshold $Z \\in [0,1]$ and report all observations for which $\\frac{v}{w} \\geq Z$ as suspected outliers.\n",
    "\n",
    "One big advantage of this procedure is that it can be easily parallelized, making it possible to examine multiple samples at once. For the sake of simplicity I am not going to implement this parallelization in this notebook but at a later stage in a more streamlined format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e849a5-63e1-4ae5-9212-9aa9bcdef2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Sys.getenv('CI') == \"true\"){\n",
    "    num_outliers <- readRDS(file = \"data/num_outliers.RDS\")\n",
    "    num_samples <- readRDS(file = \"data/num_samples.RDS\")\n",
    "    certainties <- readRDS(file = \"data/certainties.RDS\")\n",
    "} else{\n",
    "total_samples <- 500\n",
    "k <- 100\n",
    "num_samples <- rep(x = 0, times = n)\n",
    "num_outliers <- rep(x = 0, times = n)\n",
    "certainties <- rep(x = 1, times = n)\n",
    "\n",
    "sample_inds <- map(.x = 1:total_samples, \n",
    "                   .f = function(i) sample(x = 1:n, size = k, replace = FALSE))\n",
    "                   \n",
    "freq_samples <- tabulate(unlist(sample_inds))\n",
    "num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples    \n",
    "                      \n",
    "### Parallelization:\n",
    "\n",
    "par_helper_fun <- function(list_path, index, alpha, B, gamma){\n",
    "    dat <- readList(file = list_path, index = index)\n",
    "    sample_flagged <- outlier_detection(func_dat = dat, alpha = 0.05, B = 50, gamma = 2, ids = index)\n",
    "}\n",
    "                      \n",
    "num_cores <- detectCores()\n",
    "cl <- makeForkCluster(num_cores)\n",
    "                   \n",
    "invisible(clusterCall(cl, fun = function() library('largeList')))\n",
    "invisible(clusterCall(cl, fun = function() library('Rcpp')))\n",
    "                   \n",
    "clusterExport(cl, varlist = list(\"grid_approx_single_obs\",\n",
    "                                 \"grid_approx_set_obs\",\n",
    "                                 \"approx_C\",\n",
    "                                 \"grid_finder\",\n",
    "                                 \"outlier_iteration\",\n",
    "                                 \"outlier_detection\",\n",
    "                                 \"par_helper_fun\"),\n",
    "             envir = .GlobalEnv)\n",
    "                      \n",
    "sample_flagged_par <- clusterApplyLB(cl = cl,\n",
    "                                     x = sample_inds,\n",
    "                                     fun = function(smp) par_helper_fun(list_path = \"data/functions.llo\", \n",
    "                                                                      index = smp, alpha = 0.05, \n",
    "                                                                      B = 50, gamma = 2))\n",
    "\n",
    "                      \n",
    "stopCluster(cl)\n",
    "\n",
    "### Only works if ids are integers (else slighty change function outlier detection to give back indices and not ids)                      \n",
    "#ind_outliers <- map(.x = sample_flagged,\n",
    "#                    .f = as.integer)                \n",
    "\n",
    "#freq_outliers <- tabulate(unlist(ind_outliers))\n",
    "                   \n",
    "freq_outliers <- tabulate(unlist(sample_flagged_par))\n",
    "num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "                \n",
    "certainties <- unlist(map(.x = 1:n,\n",
    "                          .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i]/num_samples[i], 1)))    \n",
    "\n",
    "saveRDS(object = num_outliers, file = \"data/num_outliers.RDS\")                          \n",
    "saveRDS(object = num_samples, file = \"data/num_samples.RDS\")\n",
    "saveRDS(object = certainties, file = \"data/certainties.RDS\")\n",
    "}                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4d297-711b-46c9-8fde-2153b3d8fe7f",
   "metadata": {},
   "source": [
    "Applying these to the original simulated data set yields the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33fbd8b6-4095-4534-b44b-15f28bb05646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer(0)\n"
     ]
    }
   ],
   "source": [
    "num_outliers <- readRDS(file = \"data/num_outliers.RDS\")\n",
    "num_samples <- readRDS(file = \"data/num_samples.RDS\")\n",
    "certainties <- readRDS(file = \"data/certainties.RDS\")\n",
    "\n",
    "print(which(num_samples == 0))                          \n",
    "flagged <- which(certainties > 0.5)\n",
    "original_outliers <- which(outliers == 1)\n",
    "missed_outliers <- setdiff(original_outliers, flagged)\n",
    "false_outliers <- setdiff(flagged, original_outliers)\n",
    "\n",
    "saveRDS(object = list(flagged = flagged,\n",
    "                      original = original_outliers,\n",
    "                      missed = missed_outliers,\n",
    "                      false = false_outliers),\n",
    "        file = \"data/outliers_info.RDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "492fd93b-1f95-4c4b-84ba-54545fe688fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tibble for visualization\n",
    "\n",
    "my_tibble <- bind_rows(map(\n",
    "  .x = 1:n,\n",
    "  .f = function(i) {\n",
    "    tibble(\n",
    "      args = functions[[i]]$args,\n",
    "      vals = functions[[i]]$vals,\n",
    "      id = rep(i, times = length(args)),\n",
    "      cert = rep(certainties[i], times = length(args))\n",
    "    )\n",
    "  }\n",
    "))\n",
    "\n",
    "saveRDS(object = my_tibble, file = \"data/shiny_tibble.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d06ea8-97b9-47f0-9d54-c669acc5a38b",
   "metadata": {},
   "source": [
    "So at least in this simple setting the algorithm identifies nearly all generated outliers and does not classify any regular point as an outlier. This is not always the case and careful choice of the tuning parameters is necessary to ensure a good performance. In the case of other data generating processes, this method can also perform worse, but is applicable in very general settings due to its functional nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c02253f-4b88-4eb0-98d2-f915d9d7156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Listening on http://127.0.0.1:7525\n",
      "\n",
     ]
    }
   ],
   "source": [
    "if(Sys.getenv('CI') != \"true\"){suppressWarnings(runApp('visual'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d90624-bfa6-41c4-bec9-764e45c08260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
