{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ed4bb4-18b8-4b7c-b8b8-4d267b5e5940",
   "metadata": {},
   "source": [
    "Notebook by **Jakob R. Jürgens** - Final project for the courses **OSE - Data Science** in the summer semester 2021 and **OSE - Scientific Computing** in the winter semester 2021/2022 - Find me at [jakobjuergens.com](https://jakobjuergens.com) <br>\n",
    "# Outlier Detection in Sensor Data using Functional Depth Measures\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499e3e4-1138-4526-b80b-c43b9ab3ebbf",
   "metadata": {},
   "source": [
    "The best way to access this project is to clone this repository and execute the jupyter notebook and the shiny app locally. Alternatively, on the main site of this repository, there are nbviewer and binder badges set up to directly look at them in the browser.\n",
    "\n",
    "The following packages and their dependencies are necessary to execute the notebook and the shiny app. If you are executing the code locally, make sure that these packages are provided and that an R Kernel (like irkernel) is activated in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91968e22-6c4e-425c-8c63-d35c9792ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(library(devtools))\n",
    "suppressMessages(library(MASS))\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(shiny))\n",
    "suppressMessages(library(shinydashboard))\n",
    "suppressMessages(library(largeList))\n",
    "suppressMessages(library(parallel))\n",
    "suppressMessages(library(Rcpp))\n",
    "suppressMessages(library(repr))\n",
    "\n",
    "options(repr.plot.width=30, repr.plot.height=8)\n",
    "\n",
    "# suppressMessages(library(gganimate)) #needed only to generate the gifs\n",
    "\n",
    "# If direct interaction with the parquet-files is wished for. Install the full version of arrow and its c++ dependencies\n",
    "# Sys.setenv(LIBARROW_MINIMAL='FALSE')\n",
    "# install.packages('arrow')\n",
    "# suppressMessages(library(arrow))\n",
    "\n",
    "install.packages('Code/OutDetectR_1.0.tar.gz', repos = NULL, type = 'source')\n",
    "library(OutDetectR)\n",
    "\n",
    "source(\"Code/auxiliary/observation_vis.R\")\n",
    "source(\"Code/auxiliary/distribution_vis.R\")\n",
    "source(\"Code/auxiliary/updating_vis.R\")\n",
    "source(\"Code/auxiliary/generate_set_1.R\")\n",
    "source(\"Code/auxiliary/generate_set_2.R\")\n",
    "source(\"Code/auxiliary/generate_set_3.R\")\n",
    "\n",
    "#sourceCpp(\"Code/auxiliary/rcpp_functions.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11165dab-5b70-4287-a181-2acdda5fe636",
   "metadata": {},
   "source": [
    "If you are in posession of the original Endanzug data set, convert it to R using the provided function in the package. Set the path to the RDS-object here. As it is property of Daimler AG, it is not included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b349707-0ed0-4a32-9834-1f54ed16b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data <- TRUE\n",
    "if(real_data){\n",
    "    endanzug_path <- '~/F/data_local/Projekt_AMEIUS_Daten/schra.RDS'\n",
    "    endanzug_data <- readRDS(file = endanzug_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692058a-65c4-483b-8b9a-ac8f1dc005c9",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "---\n",
    "1. [Introduction](#introduction)\n",
    "2. [Observation Structure](#observation)\n",
    "3. [The Algorithm](#algorithm)\n",
    "4. [h-modal depth](#h-modal)\n",
    "5. [Difficulties due to the Data](#difficulties)\n",
    "6. [Sampling Approach](#sampling)\n",
    "7. [Finding Comparable Sets of Observations](#comparables)\n",
    "8. [Description of Full Procedure for Existing Data sets](#procedure)\n",
    "9. [Updating](#updating)\n",
    "10. [Implementation](#implementation)\n",
    "    1. [Grid Approximation](#impl_grid) \n",
    "    2. [Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) for Observations on a Shared Grid](#impl_algorithm)\n",
    "    3. [Sampling Approach](#impl_sampling)\n",
    "    4. [Dynamic Splitting](#impl_splitting)\n",
    "    5. [Full Procedure](#impl_full)\n",
    "    6. [Updating](#impl_upd)\n",
    "11. [Simulated Data](#simulated)\n",
    "12. [Shiny App](#shiny)\n",
    "13. [Outlook](#outlook)\n",
    "14. [Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a2cc7-bc7f-4b0b-884c-e60b46529dc2",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "---\n",
    "This project is part of a cooperation with **Daimler AG** and deals with outlier detection in sensor data from production processes. <br>\n",
    "Since this project will take place over the courses **OSE - Data Science** (summer semester 2021) and **OSE - Scientific Computing for Economists** (winter semester 2021/2022), the current state of the project can be seen as a description of the progress at the half time point. Much of the following will be developed further and is therefore subject to change in future revisions.\n",
    "\n",
    "The main data set that is used in this project deals with the relation of angle and torque during the process of tightening a bolt in a screwing connection. The corresponding data set will be called \"Endanzugsproblem\" in the following notebook and contains ~350000 observations of what can be imagined as a function that maps angles to torque. The following schematic will give an idea of what the data set represents and what the problem is:\n",
    "\n",
    "<img src=\"material/SchraubdatenPrinzipskizze.png\">\n",
    "\n",
    "To clarify some things about this simplified schematic:\n",
    "* The so called Endanzug is only part of the tightening process, but the parts of the observation happening before it are not subject of this analysis\n",
    "* The focus of this project lies on curves that are \"In Ordnung\", so observations that do not immediately disqualify themselves in some way by for example not reaching the fixed window of acceptable final values\n",
    "* The observations typically have a high frequency of measuring torque, but the measuring points are not equidistant\n",
    "* The angles where torque is measured are not shared between observations, but the measuring interval of angles might overlap between observations\n",
    "* The Endanzug does not start at the same angle for every observation and also does not necessarily start at the same torque\n",
    "* Outliers can be very general, so methods based on detecting only specific types of outliers may not be able to effectively filter out other suspicious observations. So the optimum would be to have some kind of Omnibus test for outliers\n",
    "\n",
    "Especially due to the high frequency of measurement and the non-identical points where torque is measured, the idea of interpreting each observation as a function and therefore approaching the problem from a standpoint of functional data analysis comes to mind. One method that is used in functional data analysis to identify outliers is based on what is called a \"functional depth measure\". Gijbels and Nagy (2017) introduces the idea of depth as follows and then elaborate on the theoretical properties a depth function for functional data should possess:\n",
    "\n",
    "> For univariate data the sample median is well known\n",
    "to be appropriately describing the centre of a data\n",
    "cloud. An extension of this concept for multivariate\n",
    "data (say p-dimensional) is the notion of a point (in\n",
    "$\\mathbb{R}^p$) for which a statistical depth function is maximized. \n",
    "\n",
    "The idea is to define an analogous concept to centrality measures (such as the distance from some central tendency like the median) in a scalar setting for functional data and then use those to determine which functions in a set are typical for the whole population. Due to the more applied nature of this project, I will not go into detail on the theoretical properties of the methods used, but focus on giving intuition why the chosen methods make sense in this context.\n",
    "\n",
    "The main inspiration for my approach to the problem of detecting outliers in a data set such as the one described above is the paper \"**Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels**\" by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I am going to first describe their algorithm, then present my extensions and my implementation and finally apply it to three simulated data sets mimicking the \"Endanzugsproblem\" as the original data is property of Daimler AG, which I cannot make public."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b574b3d-f896-4b6b-94bc-4d674ac8f1d5",
   "metadata": {},
   "source": [
    "## Observation Structure <a name=\"observation\"></a>\n",
    "---\n",
    "For the sake of clarity, I will show the typical structure of one observation and define a couple of objects I will refer to later. To give context for the later choices of data generating processes, it is useful to know that the physical process of tightening a bolt is typically associated with a linear relationship between angle and torque (at least in the relevant parts of the tightening process) - this approximation is good for the parts of the tightening process that are part of the \"Endanzug\". Therefore, simulations in the later parts of this notebook typically assume an approximately linear process as the data generating process for non-outliers.\n",
    "\n",
    "One observation in a data set might look as follows.\n",
    "\n",
    "<img src=\"material/observation.png\" width=\"1000\" align=\"center\">\n",
    "\n",
    "| Observation \t| 1    \t| 2    \t| 3    \t| 4    \t| 5    \t| 6    \t| 7    \t| 8    \t| 9    \t| 10   \t| 11   \t| 12   \t| 13   \t| 14   \t| 15   \t| 16   \t| 17   \t| 18   \t| 19   \t| 20   \t|\n",
    "|:-----------:\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|\n",
    "|    Angle    \t| 2.01 \t| 2.21 \t| 2.91 \t| 3.00 \t| 3.07 \t| 3.95 \t| 4.33 \t| 4.35 \t| 4.41 \t| 4.74 \t| 4.77 \t| 5.06 \t| 6.33 \t| 6.37 \t| 6.41 \t| 6.57 \t| 7.25 \t| 7.32 \t| 7.71 \t| 7.94 \t|\n",
    "|    Torque   \t| 2.02 \t| 2.61 \t| 3.05 \t| 3.16 \t| 2.99 \t| 4.19 \t| 4.24 \t| 4.37 \t| 4.73 \t| 4.89 \t| 5.03 \t| 5.45 \t| 6.32 \t| 6.18 \t| 6.22 \t| 7.06 \t| 7.30 \t| 7.59 \t| 7.99 \t| 8.06 \t|\n",
    "\n",
    "* The red diamonds represent measurements of torque that were taken at a recorded angle.\n",
    "* The blue lines are an example of **linear interpolation** between the points that were actually measured. This will become important later on.\n",
    "* The **measuring interval** marked in green is the convex hull of angles where measurements were taken for this observation.\n",
    "\n",
    "The data set contains many of these objects, that do not necessarily share these characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1d4ed-40ca-461f-8031-5e6fa2a02e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated using function from auxiliary/observation_vis.R\n",
    "# source(\"auxiliary/observation_vis.R\")\n",
    "# obs_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bdb4b-cd4a-43ec-99d6-3d072fbd5d1f",
   "metadata": {},
   "source": [
    "## The Algorithm <a name=\"algorithm\"></a>\n",
    "---\n",
    "The idea of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) is an iterative process that classifies observations as outliers if their functional depth lies below a threshold C, which is determined using a bootstrapping procedure in each iteration. <br> \n",
    "The algorithm can be decomposed into two parts:\n",
    "\n",
    "1. **The iterative process**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i),\\: \\dots \\: ,D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Let $x_{i_1},\\: \\dots,\\: x_{i_k}$ be the k curves such that $D_n(x_{i_k}) \\leq C$, for a given cutoff $C$. Then, assume that $x_{i_1},\\: \\dots,\\: x_{i_k}$ are outliers and delete them from the sample.\n",
    "    3. Then, come back to step 1 with the new dataset after deleting the outliers found in step 2. Repeat this until no more outliers are found. \n",
    "    \n",
    "The underlying idea is that observations that are more central in the sense of the chosen depth will have higher depth assigned to them. So choosing the least deep observations from a data set is a reasonable way to search for atypical observations. The question of where to draw the border for observations to be classified as abnormal is done using a bootstrapping procedure described next. <br>\n",
    "\n",
    "2. **Determining C**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i),\\: \\dots ,\\:D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Obtain B standard bootstrap samples of size n from the dataset of curves obtained after deleting the $\\alpha \\%$ less deepest curves. The bootstrap samples are denoted by $x_i^b$ for $i = 1,\\: \\dots,\\: n$ and $b = 1,\\: \\dots,\\: B$.\n",
    "    3. Obtain smoothed bootstrap samples $y_i^b = x_i^b + z_i^b$, where $z_i^b$ is such that $(z_i^b(t_1), \\dots, z_i^b(t_m))$ is normally distributed with mean 0 and covariance matrix $\\gamma\\Sigma_x$ where $\\Sigma_x$ is the covariance matrix of $x(t_1),\\: \\dots,\\: x(t_m)$ and $\\gamma$ is a smoothing parameter. Let $y_i^b$, $i = 1,\\: \\dots,\\:n$ and $b = 1,\\:\\dots,\\: B$ be these samples. *\n",
    "    4. For each bootstrap set $b = 1,\\:\\dots,\\:B$, obtain $C^b$ as the empirical 1% percentile of the distribution of the depths $D(y_i^b)$, $i = 1,\\: \\dots,\\: n$.\n",
    "    5. Take $C$ as the median of the values of $C^b$, $b = 1,\\: \\dots,\\: B$. \n",
    "<br>\n",
    "\n",
    "This is done, as a theoretical derivation of the distribution of depth values for data generated by a specific data generating process is often infeasible. Instead, one drops a fraction of observations $\\alpha$ and uses a smoothed bootstrapping algorithm to approximate the corresponding threshold value to remove approximately that fraction of observations in the procedure listed under 1.\n",
    "\n",
    "*At this point in the algorithm the assumption that the functional observations are observed at a set of discrete points $t_1,\\:\\dots,\\:t_m$ has already been made. <br>\n",
    "\n",
    "Some important points:\n",
    "* I decided to deviate from the original procedure proposed by the authors by allowing the user to decide whether to reestimate $C$ in each iteration of the process. The authors present good arguments, why keeping $C$ fixed is the better approach and my testing confirms those. But to keep the possibilities for experimentation open, I opted to implement it in this way nevertheless. In later stages of this project I will go into detail on why keeping $C$ constant also has some problems and try to introduce corrections to improve the method developped below.\n",
    "* The authors recommend a choice of $\\gamma = 0.05$ as a smoothing parameter for the bootstrap which I adopted in my applications.\n",
    "* For the choice of $\\alpha$ preexisting information on the data should be used if available. A good way to choose this is the expected fraction of outliers in the data. However, my testing showed that in some cases the choice of this parameter had to be lower than the actual fraction of observations generated by abnormal processes when using a sampling procedure to get better results.\n",
    "\n",
    "The authors propose three functional depth measures and benchmark them in a simulation setting. Because of their results and the computational cost which are comparatively small, I chose to use **h-modal depth** for my implementation, which I will introduce in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63443be-611f-4d03-b664-93d445943d38",
   "metadata": {},
   "source": [
    "## h-modal depth <a name=\"h-modal\"></a>\n",
    "---\n",
    "\n",
    "Introduced by Cuevas, Febrero-Bande, and Fraiman (2006) h-modal depth is one of three depth measures covered in Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I will follow the summary in the latter paper for my overview.  <br> The idea behind this depth is that a curve is central in a set of curves if it is closely surrounded by other curves. <br>\n",
    "\n",
    "In mathematical terms, the h-modal depth of a curve $x_i$ in relation to a set of curves $x_1, \\dots, x_n$ is defined as follows: <br>\n",
    "\\begin{equation}\n",
    "    MD_m(x_i,h) = \\sum_{k = 1}^{n} K(\\frac{||x_i - x_k||}{h})\n",
    "\\end{equation}\n",
    "\n",
    "where $K: \\mathbb{R^{+}} \\rightarrow \\mathbb{R^{+}}$ is a kernel function and $h$ is a bandwidth. <br>\n",
    "The authors recommend using the truncated Gaussian kernel, which is defined as follows:\n",
    "\\begin{equation}\n",
    "    K(t) = \\frac{2}{\\sqrt{2\\pi}} \\exp(-\\frac{t^2}{2}) \\quad \\text{for} \\quad t>0 \\quad \\text{and} \\quad 0 \\quad \\text{otherwise}\n",
    "\\end{equation}\n",
    "\n",
    "and to choose $h$ as the 15th percentile of the empirical distribution of $\\{||x_i - x_k|| \\, | \\, i,k = 1,\\:\\dots,\\:n\\}$\n",
    "\n",
    "I chose to implement the $L^2$ norm - one of the norms recommended by the authors - as it performed better than the $L^{\\infty}$ norm (which was also recommended) in my preliminary tests. In a functional setting $L^2$ is defined by:\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\int_a^b (x_i(t) - x_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "where a and b are the boundaries of the measurement interval. This can be replaced by its empirical version\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 2}^m \\Delta_j(x_i(t_j) - x_k(t_j))^2} = \\sqrt{\\sum_{j = 2}^m (t_j - t_{j-1})(x_i(t_j) - x_k(t_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "in case of a discrete set of $m$ observation points shared between observations.\n",
    "\n",
    "The choice of the functional norm could be adjusted to deal with data resembling different functional forms. This could be part of a possible extension, where different norms are implemented and compared. This might become part of future iteratons of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17ebc8-b676-4fef-b044-afd3a3a3e6c3",
   "metadata": {},
   "source": [
    "## Difficulties due to the Data <a name=\"difficulties\"></a>\n",
    "---\n",
    "### The Endanzug does not start at the same angle for every observation.\n",
    "In this setting the fact that the first measurement is taken at different angles is not of a problem, since the property of interest is the shape of the curve after the Endanzug has started. In real world terms, the beginning of the \"Endanzug\" is determined by the first angle where a specific torque is exceeded and the change in torque during the \"Endanzug\" is of interest and not the position of the \"Endanzug\" in the whole tightening process. Therefore, all observations can be modified by subtracting the first angle of the \"Endanzug\" from all angles, effectively **zeroing** the observations. \n",
    "\n",
    "At this time, I am going to focus on problems where zeroing is admissible and elaborate on how to possibly extend the method to scenarios where it is not. Those generalizations will be part of further work on this project.\n",
    "\n",
    "<img src=\"material/zero.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "### After zeroing, the measurement intervals might still not be identical due to differing lengths.\n",
    "I decided to try to remedy this remaining problem in the zeroed data set by stretching the measuring intervals to a shared interval while leaving the observed torques identical. Excessive stretching however is problematic, as it can lead to similar observations ending up very different. Putting a conservative limit on stretching should however conserve the quality of relationships between observations. This should lead to limited influence on the calculated depths of the functional observations. To do so, I define a parameter $\\lambda \\geq 1$ called **acceptable stretching** and make observations comparable by stretching their measuring intervals by a factor $\\psi_i \\in [1/\\lambda, \\lambda]$ before approximating them using linear interpolation. In an optimal setting stretching would not be necessary and employing it leads to trade offs, so this parameter should be chosen conservatively.\n",
    "\n",
    "If zeroing as described in 1. is not appropriate, a combination of **acceptable stretching** and **acceptable shifting** could be implemented to increase the size of sets of pairwise comparable functions. This will be part of future extensions of this project.\n",
    "\n",
    "<img src=\"material/stretch.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "As you can already see in this animation, the acceptable stretching introduces inaccuracies even in an approximately linear setting. Outlier classifications could be quite sensitive to this parameter.\n",
    "\n",
    "### The angles where torque is measured are not shared between observations.\n",
    "Assuming that the measuring intervals are identical, I decided to use **linear interpolation** to approximate the observations. This is done to treat them as if they had been observed at a **shared grid of angles** to make them compatible with the simplification of the **h-modal norm** described above. This is only an approximation, but choosing an appropriately fine grid to approximate the observations should limit the influence of this procedure on the calculated functional depths. <br>\n",
    "Espacially in case of the dataset under consideration in this project, performing this approximation by linear interpolation should not result in large distortions due to the linearity of the described physical process. In other settings this approximation could lead to bad performance. One example that came to my mind is if most observations are zero, the frequency of taking measurements is comparatively low and the relevant parts of the observations are instantaneous deviations from zero (or spikes that have infinitesimally small duration). In cases like the one described, it would probably be a better idea to choose a different method or at least choose a different functional norm more appropriate for the dgp due to the necessity of very fine grids to achieve a appropriate approximation of the data.<br> Benchmarking of the method for different data generating processes will be part of future revisions to explore the applicability of the developed method.<br>\n",
    "\n",
    "For sufficiently smooth processes with a high measuring frequency this approximation should however not result in huge distortions.\n",
    "\n",
    "<img src=\"material/grid_approx.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "Another possibility to approach this third problem would be to use a different version of the norm for discretized points I described above. <br>\n",
    "Instead of calculating \n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 2}^m \\Delta_j(x_i(t_j) - x_k(t_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "for a set of points on a grid approximated by linear interpolation, one could instead define functions $\\tilde{x}_i$ which are just the piece wise linear functions defined by connecting the observed points of $x_i$. A norm based on this could be constructed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{\\tilde{2}} = \\sqrt{\\int_a^b (\\tilde{x}_i(t) - \\tilde{x}_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "For very fine grid approximations these criteria should result in similar depths, as under some mild conditions, the first approach will converge to the second for increasingly fine grids. In a sense the first approach is similar to a Riemann sum which for increasingly fine grids converges to the corresponding Riemann integral under the necessary assumptions.\n",
    "\n",
    "### Runtime Complexity\n",
    "The runtime complexity of this algorithm is at least $O(n^2)$ and I concluded that using my implementation it is infeasible to use it on a very large data set such as the \"Endanzugsproblem\" (assuming that all observations are comparable at once). Even when splitting up the observations as proposed above into comparable subsets, some of them will be too large to directly approach with this method. <br>\n",
    "To solve this problem, I instead opted to use a **sampling approach** which I explain in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8799d-49f3-40d2-9ff4-5d86db584d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The gifs have been rendered using function from auxiliary/observation_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/observation_vis.R\")\n",
    "# stretching_vis()\n",
    "# zeroing_vis()\n",
    "# lin_approx_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98161569-efdf-4525-ba07-b39c722aab8c",
   "metadata": {},
   "source": [
    "## Sampling Approach <a name=\"sampling\"></a>\n",
    "---\n",
    "Since it is infeasible to use this method on very large data sets at once, I chose to pursue a sampling-based approach instead. Intuitively this is reasonable as in many cases observations that are atypical in the full data set will be classified as atypical in its subsamples more often than \"typical\" observations. If this principle does not apply to the data set, a sampling-based approach is difficult to justify. In a case like that it is more reasonable to choose differnet methods to identify abnormal observations. <br>\n",
    "\n",
    "As the assumption seems reasonable in case of the \"Endanzugsproblem\", instead of performing the algorithm described above on the whole data set (or its comparable subsets), I devise a procedure as described in the following: <br>\n",
    "\n",
    "Let $\\{x_1, \\dots, x_L\\}$ be a set of observations that are comparable using the algorithm but too large to perform this procedure in reality.\n",
    "1. Define the following objects:\n",
    "    * Let *num\\_samples* $ = (a_1,\\:\\dots,\\: a_L) \\in \\mathbb{N}_0^L$ where $a_i$ is the number of samples $x_i$ was part of $\\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$. <br>(Initialize all entries as 0)\n",
    "    * Let *num\\_outliers* $ = (b_1,\\:\\dots,\\: b_L) \\in \\mathbb{N}_0^L$ where $b_i$ is the number of samples $x_i$ was classified as an outlier in $\\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$. <br>(Initialize all as entries 0)\n",
    "    * Let *frac\\_outliers* $ = (c_1,\\:\\dots,\\: c_L) \\in \\mathbb{R}_{\\geq 0}^L$ where $c_i = \\begin{cases}1 & a_i = 0 \\\\ \\frac{b_i}{a_i} & a_i > 0\\end{cases} \\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$  <br>(Initialize all entries as 1)\n",
    "2. Draw a sample of size K from $\\{x_1,\\: \\dots,\\: x_L\\}$ without replacement\n",
    "3. Perform the outlier detection procedure on this sample and update the vectors according to your results.\n",
    "4. Go back to two and iterate this process until some condition is fulfilled.\n",
    "\n",
    "Typical conditions could be:\n",
    "* A specified number of iterations was reached\n",
    "* Every observation was part of more than a specified number of samples\n",
    "* The vector of certainties did not change enough according to some criterion over a specified number of iterations\n",
    "\n",
    "In the end, the entries of *frac\\_outliers* can be used as a metric for the outlyingness of an observation. If a binary decision rule is needed, every observation with an entry over some specified threshold could be classified as an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fadb66-508b-4504-93d2-5b2ddc39365c",
   "metadata": {},
   "source": [
    "## Finding Comparable Sets of Observations <a name=\"comparables\"></a>\n",
    "---\n",
    "Assume for the sake of simplicity that **zeroing** is reasonable so that the minima of the measurement intervals of the observations are all zero. Therefore, the measurement intervals only differ in their end points or lengths which is identical in this case. <br>\n",
    "As an example to visualize possible ways to find comparable subsets assume that the empirical distribution of endpoints looks as follows:\n",
    "\n",
    "<img src=\"material/dist.png\" width=\"1000\" align=\"center\">\n",
    "\n",
    "In the following I describe three methods to find comparable subsets to perform the sampling procedure on and explain why I chose the method that I ultimately implemented.\n",
    "\n",
    "### Static Splitting\n",
    "\n",
    "For **Static Splitting** the idea is to partition the whole data set into pairwise disjunct subsets of pairwise comparable observations. This partition depends on the acceptable stretching parameter and is not necessarily unique for a choice of the acceptable stretching parameter. Therefore, a choice procedure would have to be introduced if this approach were to be taken. <br>\n",
    "Some possible partitions of the set above (not necessarily consistent with the same acceptable stretching parameter) could look like this: \n",
    "\n",
    "<img src=\"material/static_splits.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "However, this idea has some problems:\n",
    "* The choice of subsets could introduce a new source of distortions in addition to the acceptable stretching parameter.\n",
    "* Adding new observations could change the chosen subsets, making an updating procedure difficult to realize.\n",
    "* In each individual subset, the observations that are changed due to stretching are identical over samples. This could lead to distortions since for some observations not the original but only the stretched observations are taken into account in the classification.\n",
    "\n",
    "### Dynamic Splitting\n",
    "\n",
    "In **Dynamic Splitting** the allocation of comparable subsets takes place dynamically for each realization of the observation interval. The procedure is as follows: <br>\\\n",
    "For each realization of the endpoint, determine the subset of comparable observations and perform the sampling approach on this subset but keep the parameter for acceptable stretching constant for all end points.\n",
    "\n",
    "<img src=\"material/dyn_splits.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "This approach has some advantages over the first one:\n",
    "* The choice of subsets becomes only a question of the acceptable stretching parameter and not of the choice of algorithm that chooses the partition of the data set.\n",
    "* Adding new observations is unproblematic, as new observations do not influence the allocation of comparable subsets. Therefore additional samples containing the new observation can be drawn without creating problems in the comparability to previous sampling-based results. A procedure like this is described in the next section.\n",
    "* Each observation can enter the classification procedure undistorted in at least the comparable subset corresponding to its own endpoint. Additionally, it can enter the classification in samples, where it is comparable due to acceptable stretching. The latter can realize for different degrees of stretching - increasing or decreasing the length of the measuring interval. This solves the problem of observations being used for classification only in a specific distorted state.\n",
    "\n",
    "### Dynamic Splitting with varying acceptable stretching parameter\n",
    "\n",
    "As shown above, the difference in length of these comparable subsets changes quite substantially and does not react to the density of observations having similar measurement intervals. A deviation from this appraoch might be desirable for multiple reasons:\n",
    "* If there are many observations with a very similar measuring interval, including observations with a more dissimilar interval (in terms of the animation above, a midpoint farther away) might be detrimental to the procedure's performance. If there are fewer observations close by one might want to allow for a bigger acceptable stretching parameter to allow for more comparisons.\n",
    "* In tandem with the advantage above one could also change the sample size to suit specific needs of the procedure.\n",
    "\n",
    "It would be possible to use a **local acceptable stretching parameter** that changes as a function of the estimated density of end points (since zeroing was admissible in this example).\n",
    "Using a rather simple function determining the local acceptable stretching parameter to serve as an example leads to the following choice of comparable subsets.\n",
    "\n",
    "<img src=\"material/dyn_splits2.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "In this example the effect is quite subtle, but in comparison to the previous animation, one can see that the expansion of the interval of end points of comparable observations is slower in regions, where the estimated density of end points is higher. <br>\n",
    "There are two reasons why I decided against making acceptable stretching a varying parameter in my implementation:\n",
    "1. It introduces another complication as the function to determine the local acceptable stretching has to be chosen.\n",
    "2. It makes the updating procedure described later more difficult, as adding more observations will change the estimated density of the end points and thereby change the comparable subsets.\n",
    "\n",
    "### Choice for Implementation\n",
    "Due to the advantages and disadvantages described above, I decided to implement Dynamic Splitting with a global acceptable stretching parameter. <br>\n",
    "This makes it easier to later justify an updating procedure and still does not create distorted results as described for Static Splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f2ce3-b4be-4c0b-b107-9f7001831d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The graphics and gifs have been rendered using function from auxiliary/observation_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/distribution_vis.R\")\n",
    "# dist_vis()\n",
    "# static_splits_vis()\n",
    "# dynamic_splits_vis()\n",
    "# dynamic_splits2_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2980c-964d-4c2f-bbf0-d849e57d438b",
   "metadata": {},
   "source": [
    "## Description of Full Procedure for Existing Data sets <a name=\"procedure\"></a>\n",
    "---\n",
    "Having explained\n",
    "* The algorithm by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008)\n",
    "* The sampling approach \n",
    "* The procedure for selecting comparable subsets with dynamic splitting of the data set \n",
    "\n",
    "I am going to explain the full procedure applied to an existing data set assuming that zeroing is admissible. <br>\n",
    "\n",
    "### Definition of Objects\n",
    "* Let $x_1,\\: \\dots,\\: x_n$ be the full data set of observations as described in the beginning\n",
    "* Let $I_1 = [s_1, e_1], \\dots, I_n = [s_n, e_n]$ be the measuring intervals of $x_1, \\dots, x_n$\n",
    "\n",
    "Assuming that zeroing is admissible:\n",
    "* Let $\\bar{x}_1,\\: \\dots, \\:\\bar{x}_n$ be the corresponding zeroed observations\n",
    "* Let $\\bar{I}_1 = [0, \\: \\bar{e}_1], \\: \\dots, \\bar{I}_n = [0, \\bar{e}_n]$ be the corresponding zeroed measuring intervals\n",
    "* Let $\\bar{E} = \\{\\bar{e} \\: | \\: \\exists k \\in \\{1, \\dots, n\\} \\: \\text{s.t.} \\: \\bar{e} = \\max(\\bar{I}_k)\\}$ be the set of measuring interval end points occurring in $\\bar{I}_1,\\: \\dots,\\: \\bar{I}_n$\n",
    "* Let $\\bar{\\Lambda}(\\bar{x}, \\bar{e})$ be the resulting object when zeroed observation $\\bar{x}$ is stretched to fit zeroed measuring interval $\\bar{I} = [0, \\bar{e}]$\n",
    "\n",
    "Again assuming that zeroing is admissible, one can reasonably define the following objects.\n",
    "* Let $\\bar{Z}(\\bar{e}, \\lambda) = \\{\\bar{x}_k \\: | \\: \\frac{\\bar{e}_k}{\\bar{e}} \\in [\\frac{1}{\\lambda},\\lambda]\\}$ be the set of zeroed observations that can be compared to a zeroed observation with measuring interval $\\bar{I} = [0, \\bar{e}]$ with an acceptable stretching factor of $\\lambda$.\n",
    "* Let $\\bar{S}(\\bar{e}, \\lambda) = \\{\\bar{\\Lambda}(\\bar{e}, \\bar{x}) \\: | \\: \\bar{x} \\in \\bar{Z}(\\bar{e}, \\lambda)\\}$ be the set of zeroed observations that have been stretched with an admissible stretching factor to be comparable to a zeroed observation with zeroed measuring interval $\\bar{I} = [0, \\bar{e}]$.\n",
    "\n",
    "If zeroing is not a valid approach, corresponding objects dependent on acceptable shifting and acceptable stretching have to be defined, which will be one challenge in generalizing this method.\n",
    "\n",
    "### Procedure\n",
    "1. Choose parameters:\n",
    "    * $\\lambda$ acceptable stretching\n",
    "    * $L$ sample size in sampling procedure (may also be varying depending on chosen approach for sampling)*\n",
    "    * $K$ number of equidistant points in grid for approximation by linear interpolation*\n",
    "    * $\\alpha$ fraction of observations to drop in approximation of cutoff value $C$ in outlier classification algorithm*\n",
    "    * $B$ sample size in approximation of cutoff value $C$ in outlier classification algorithm*\n",
    "    * $\\gamma$ tuning parameter in approximation of cutoff value $C$ in outlier classification algorithm*\n",
    "2. Initialize the following vectors:\n",
    "    * *num\\_samples* $ = (a_1,\\:\\dots,\\: a_n)\\in \\mathbb{N}_0^n \\quad$ with all entries being 0\n",
    "    * *num\\_outliers* $ = (b_1,\\:\\dots,\\: b_n)\\in \\mathbb{N}_0^n \\quad$ with all entries being 0\n",
    "    * *frac\\_outliers* $ = (c_1,\\:\\dots,\\: c_n) \\in \\mathbb{R}_{\\geq 0}^n \\quad$ with all entries being 1\n",
    "3. Iterate through the elements of $\\bar{E}$ doing the following:\n",
    "    * Let $\\hat{e}$ be the element of $\\bar{E}$ currently looked at\n",
    "    * Determine $\\bar{S}(\\hat{e}, \\lambda)$ and perform the sampling based outlier identification procedure on this set for some predetermined stopping condition\n",
    "    * Update *num\\_samples*, *num\\_outliers*, and *frac\\_outliers* for both the stretched and non-stretched observations in $\\bar{S}(\\hat{e}, \\lambda)$ as described in the section about sampling\n",
    "    * Go to the next element of $\\bar{E}$ and repeat until all elements have been reached.\n",
    "4. Report *frac\\_outliers* as a measure of outlyingness for the observations.\n",
    "\n",
    "*not explicitly mentioned in the procedure below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d441b-7b69-41d2-ba6b-c05f1ae73b78",
   "metadata": {},
   "source": [
    "## Updating <a name=\"updating\"></a>\n",
    "---\n",
    "The procedure described above is constructed to work for a full data set. In a day-to-day setting the data set will not be static. Instead, new observations will be added and it would be a problem, if all calculations had to be done all over again only to incorporate a comparatively small number of new observations.\n",
    "\n",
    "Instead, I devise mechanisms to assign comparable values of *frac\\_outliers* to the newly added observations and to possibly also update the pre-existing observations due to the presence of the newly added ones. In the following assume that new observations are added sequentially. Two ways came to my mind to approach this updating procedure, one more true to the original process (1) and the other one less computationally expensive (2).\n",
    "\n",
    "### Version 1:\n",
    "This procedure involves additional samples from all sets the new observation could have been part of. So both in a stretched form or in its original form <br>\n",
    "* Let $x'$ be the new observation and $\\bar{x}'$ its zeroed counterpart. Define $I'$, $\\bar{I'}$ and $\\bar{e}'$ accordingly.\n",
    "* Determine all elements $\\bar{e} \\in \\bar{E}$ such that $\\bar{x}' \\in \\bar{Z}(\\bar{e}, \\lambda)$. Define $\\bar{U}(\\bar{x}', \\lambda) = \\{\\bar{e} \\in \\bar{E} \\: | \\: \\bar{x}' \\in \\bar{Z}(\\bar{e}, \\lambda)\\}$ as the subset of $\\bar{E}$ called the Updating Window. <br> In this setting where zeroing is admissible, this can be simplified to $\\bar{U}(\\bar{x}', \\lambda) = \\{\\bar{e} \\in \\bar{E} \\: | \\: \\bar{e} \\in [\\frac{1}{\\lambda} \\bar{e}', \\lambda \\bar{e}']\\} = \\bar{E} \\cap [\\frac{1}{\\lambda} \\bar{e}', \\lambda \\bar{e}']$\n",
    "    \n",
    "<img src=\"material/update_1.gif\" width=\"1000\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400468a-f7cf-426d-b4dd-7cb79ea9baa9",
   "metadata": {},
   "source": [
    "For all $\\tilde{e} \\in \\bar{U}(\\bar{x}', \\lambda)$ perform a sampling procedure as follows: <br>\n",
    "* $\\bar{\\Lambda}(\\bar{x}', \\tilde{e})$ is part of each sample \n",
    "* The size of each sample is identical to the one used in the original procedure\n",
    "* The number of samples for each $\\tilde{e}$ is the expected value of the number of samples the new observation would have been part of, if it had been in the original data set\n",
    "* The updating procedure of the vectors works as before. Not only the entry of the new observation in each vector is added and updated, also the entries for the original observations included in the new samples are updated.\n",
    "\n",
    "So the set of zeroed observations that is potentially updated during this procedure is the following: <br> \n",
    "$$\\bigcup_{\\tilde{e}\\in \\bar{U}(\\bar{x}', \\lambda)} \\bar{Z}(\\tilde{e}, \\lambda) = \\Big\\{\\bar{x} \\: | \\: \\bar{e} \\in \\big[\\frac{1}{\\lambda}\\min\\{\\bar{U}(\\bar{x}', \\lambda)\\}, \\lambda \\max\\{\\bar{U}(\\bar{x}', \\lambda)\\}\\big]\\Big\\}$$\n",
    "<br>\n",
    "Which is visualized in the following animation. (The endpoints of the potentially updated zeroed observations are marked by the red rectangle.)\n",
    "\n",
    "<img src=\"material/update_2.gif\" width=\"1000\" align=\"center\"> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dce8b0-35e1-4ad9-ac68-88e51625c847",
   "metadata": {},
   "source": [
    "### Version 2:\n",
    "In comparison the other procedure involves only additional sampling from the set where the new observation is non-stretched.\n",
    "* Let $x'$ be the new observation and $\\bar{x}'$ its zeroed counterpart. Define $I'$, $\\bar{I'}$ and $\\bar{e}'$ accordingly.\n",
    "* Determine $\\bar{S}(\\bar{e}', \\lambda)$ and perform additional sampling as follows:\n",
    "    * $\\bar{x}'$ is part of each sample\n",
    "    * The number of samples drawn is the expected value of samples the new observation would have been part of, if it had been in the original data set\n",
    "    * The updating procedure works as before. Not only the entry of the new observation in each vector is added and updated, also the entries for the original observations are updated.\n",
    "\n",
    "The following graphic shows the equivalent objects of version 1:\n",
    "\n",
    "<img src=\"material/update_3.png\" width=\"1000\" align=\"center\"> \n",
    "\n",
    "This procedure is less true to the values calculated in the original data set, as the new observation is only taken into consideration in its non-stretched form. Additionally, pre-existing observations could be taken into consideration in their stretched form more frequently depending on the structure of new data being added which could lead to additional distortions. Therefore, I decided to start by implementing the first method and possibly include comparisons of both methods in future revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18c20b-ce5c-42fa-82bd-7d3a9719fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The graphics and gifs have been rendered using functions from auxiliary/updating_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/updating_vis.R\")\n",
    "# upd_1_vis()\n",
    "# pot_upd_obs_vis()\n",
    "# upd_2_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14cf48-cfeb-48d9-9cf6-a07e4c2adaaa",
   "metadata": {},
   "source": [
    "## Implementation <a name=\"implementation\"></a>\n",
    "---\n",
    "I decided to implement these methods in **R** and **C++** and to employ parallelization where possible, to strike a balance between speed and ease of use. All of these functions are implemented in the package **OSEproject** that is contained in the code folder and can be installed as shown in the first code block. The following section will follow a similar structure as the description of the procedure above ordered as follows:\n",
    "\n",
    "1. [Grid Approximation](#impl_grid) \n",
    "2. [Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) for Observations on a Shared Grid](#impl_algorithm)\n",
    "3. [Sampling approach](#impl_sampling)\n",
    "4. [Dynamic Splitting and Finding Comparable Subsets](#impl_splitting)\n",
    "5. [Full Procedure](#impl_full)\n",
    "6. [Updating](#impl_updating)\n",
    "\n",
    "All of these functions can also be found in */auxiliary/R_functions.R* and will in the future be available as an Rcpp-package the tar-ball of which will be included in the repo. <br>\n",
    "In future revisions there will be a secondary notebook with the details of the implementation. This notebook will instead focus on explaining the method and evaluating its perfomance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd2215-83cf-4cc9-be4a-efde96532674",
   "metadata": {},
   "source": [
    "### Grid Approximation <a name=\"impl_grid\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4219ea9-c2fc-4839-a70c-d51fc40f0cad",
   "metadata": {},
   "source": [
    "#### Finding a grid for Approximation\n",
    "Assuming that the observations already share the measuring intervals. (Same grid length still acts as a placeholder for a more sophisticated procedure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed04b9-932c-4d92-a820-d5ec89d6daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_dat:\n",
    "\n",
    "grid_finder <- function(func_dat){\n",
    "  measuring_interval <- c(min(func_dat[[1]]$args), max(func_dat[[1]]$args))\n",
    "  return(seq(measuring_interval[1], measuring_interval[2], length.out = 100))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be151560-dd82-4539-a951-a8431cd6e62f",
   "metadata": {},
   "source": [
    "#### Grid approximation by Linear Interpolation\n",
    "This function acts as a wrapper for a **C++** function, that given a vector of arguments, a vector of values and a vector that represents the grid to be used for approximation, performs the desired approximation and returns the vector of approximated values taken at the grid points. This wrapper then combines those values to a matrix, where each row represents the approximated values of an observation at the given grid points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7f09a-8bec-4923-9a4c-a2598b74c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_dat: list that contains the observations\n",
    "# each observation is a list, that conatins two vectors of identical length: args and vals\n",
    "# grid: grid to use for approximation\n",
    "\n",
    "grid_approx_set_obs <- function(func_dat, grid) {\n",
    "  res_mat <- matrix(data = unlist(\n",
    "      map(.x = func_dat,\n",
    "          .f = function(obs) grid_approx_obs(obs$args, obs$vals, grid))\n",
    "    ), nrow = length(func_dat), byrow = TRUE)\n",
    "                    \n",
    "  return(res_mat)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91236f-8fa4-4112-a429-cd342f24dcf9",
   "metadata": {},
   "source": [
    "### Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) for Observations on a Shared Grid <a name=\"impl_algorithm\"></a>\n",
    "The following functions implement the algorithm described above for functional observations that are observed at a common set of discrete points. The grid approximation above serves as a preparation to make these functions applicable. <br> \n",
    "\n",
    "#### approx_C\n",
    "The function *approx_C* implements the approximation of the cutoff value $C$ by bootstrapping described in Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a9c15-bf97-4bff-b405-b2aadadc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matr_dat: data in matrix form - each row contains the grid approximations of one observation\n",
    "# fdepths: corresponding depths for the observations\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping\n",
    "# B: number of smoothed bootstrap samples to use\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "# grid: grid used in approximation of matr_dat\n",
    "\n",
    "approx_C <- function(matr_dat, fdepths, alpha, B, gamma, grid) {\n",
    "  \n",
    "  # infer number of observations from length of depth vector\n",
    "  n <- length(fdepths)\n",
    "  # Get number of elements in grid\n",
    "  grid_length <- length(grid) \n",
    "  # determine threshold to drop observations with lowest depth values    \n",
    "  depth_thr <- quantile(x = fdepths, probs = alpha)\n",
    "  # drop observations for bootstrapping    \n",
    "  matr_dat_red <- matr_dat[fdepths >= depth_thr, ]\n",
    "  n_red <- dim(matr_dat_red)[1]\n",
    "    \n",
    "  # Determine vcov-matrix for smoothed bootstrapping    \n",
    "  Sigma_x <- cov(matr_dat_red)\n",
    "  my_vcov <- gamma*Sigma_x \n",
    "\n",
    "  # Draw bootstrap samples from data set  \n",
    "  fsamples <- map(.x = 1:B,\n",
    "                  .f = function(inds) matr_dat_red[sample(x = 1:n_red, size = n, replace = TRUE), ])\n",
    "  \n",
    "  # Create smoothing components for bootstrapping                  \n",
    "  smoothing_components <- map(.x = 1:B,\n",
    "                              .f = function(x) mvrnorm(n = n, mu = rep(0, times = grid_length), Sigma = my_vcov))\n",
    "  \n",
    "  # Obtain smoothed bootstrap samples                                  \n",
    "  smoothed_BS_samples <- map(.x = 1:B,\n",
    "                             .f = function(b) fsamples[[b]] + smoothing_components[[b]])\n",
    "\n",
    "  # Calculate depths for each smoothed bootstrap sample                             \n",
    "  bootstrap_depths <- map(.x = 1:B,\n",
    "                          .f = function(b) hM_depth(smoothed_BS_samples[[b]], grid))\n",
    "\n",
    "  # Calculate first percentile from depths of smoothed bootstrap samples                          \n",
    "  one_perc_quantiles <- unlist(map(.x = bootstrap_depths,\n",
    "                                   .f = function(sample) quantile(sample, probs = 0.01)))\n",
    "  \n",
    "  # return median of first percentiles                                   \n",
    "  return(median(one_perc_quantiles))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db35bb-bf34-40ac-a5a4-68c9a80746a3",
   "metadata": {},
   "source": [
    "#### outlier_iteration\n",
    "This function performs one iteration of the algorithm, including the calculation of functional depths, the approximation of $C$ and the flagging of observations with depths lower than $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753f804-049a-4734-9e0f-68965e4183d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matr_dat: data in matrix form - each row contains the grid approximations of one observation\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C - optional if C is specified)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C - optional if C is specified)\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "# ids: identifiers of individual observations\n",
    "# grid: grid used in approximation of matr_dat\n",
    "# C: should be provided. Otherwise C will be approximated in each step of the iteration\n",
    "\n",
    "outlier_iteration <- function(matr_dat, alpha = 0.05, B = 50, gamma, ids, grid, C = NULL){\n",
    "    \n",
    "  # Calculating functional depths using a function from ./auxiliary/Rcpp_functions.cpp  \n",
    "  fdepths <- hM_depth(matr_dat, grid)\n",
    "  \n",
    "  if(missing(C)){\n",
    "      # Approximating C  \n",
    "      C <- approx_C(matr_dat = matr_dat, fdepths = fdepths, alpha = alpha,\n",
    "                    B = B, gamma = gamma, grid = grid)\n",
    "  }\n",
    "  \n",
    "  # Flagging observations with depths lower than the cutoff value C  \n",
    "  outliers <- which(fdepths < C)\n",
    "    \n",
    "  return(list(matr_dat = matr_dat[-outliers, ],\n",
    "              ids = ids[-outliers],\n",
    "              outlier_ids = ids[outliers]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef7dcb-976d-459a-8a3f-42ea8c15d1e5",
   "metadata": {},
   "source": [
    " #### outlier_detection\n",
    " This function serves as a wrapper for the previous function and iterates the process until no new observations are flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e769044-6adb-4019-ac8e-be7c038ce47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matr_dat: data in matrix form - each row contains the grid approximations of one observation\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C - optional if C is specified)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C - optional if C is specified)\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "# ids: identifiers of individual observations\n",
    "# grid: grid used for the approximation\n",
    "# C: should be provided. Otherwise C will be approximated in each step of the iteration\n",
    "\n",
    "outlier_detection <- function(matr_dat, alpha = 0.05, B = 100, gamma = 0.05, ids, grid, C = NULL){\n",
    "    \n",
    "    tmp_ids <- ids\n",
    "    # Initialize empty vectors for position of flagged observations in func_dat and ids of flagged observations\n",
    "    outlier_ids <- c()\n",
    "    \n",
    "    # loop that continues until an iteration does not flag any new observations\n",
    "    condition <- TRUE\n",
    "    while(condition){\n",
    "        \n",
    "        # perform iteration\n",
    "        iter_res <- outlier_iteration(matr_dat = matr_dat, alpha = alpha, B = B, gamma = gamma, ids = tmp_ids, grid = grid, C = C)\n",
    "        new_outliers <- iter_res$outlier_ids\n",
    "        \n",
    "        # if there are no new flagged observations stop loop\n",
    "        if(length(new_outliers) == 0){condition <- FALSE}\n",
    "        else{\n",
    "          #otherwise: add flagged observations to vector\n",
    "          outlier_ids <- c(outlier_ids, new_outliers)\n",
    "          # reduce data to non-flagged observations\n",
    "          matr_dat <- iter_res$matr_dat\n",
    "          # reduce ids to non-flagged observations\n",
    "          tmp_ids <- iter_res$ids \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # return identifiers of flagged observations and position of these flagged observations in the data set\n",
    "    return(list(outlier_ids = outlier_ids,\n",
    "                outlier_ind = which(is.element(ids, outlier_ids))))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b8374-5439-4c3d-9394-d796bb55bb01",
   "metadata": {},
   "source": [
    "#### Outlier Detection - Wrapper\n",
    "The following function acts as a wrapper to the previous one in case, $C$ should not be recalculated in each iteration. My recommendation would be to use this function as recalculation of $C$ in each iteration can lead to classifying unreasonably many observations as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f24fd-c04d-418f-b609-e044958e993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_dat: list that contains the observations\n",
    "# each observation is a list, that contains two vectors of identical length: args and vals\n",
    "# ids: identifiers of individual observations\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C - optional if C is specified)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C - optional if C is specified)\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "\n",
    "detection_wrap <- function(func_dat, ids, alpha, B, gamma = 0.05){\n",
    "    \n",
    "    # determine the grid for approximation\n",
    "    grid <- grid_finder(func_dat)\n",
    "    \n",
    "    # Approximate by linear interpolation\n",
    "    matr_dat <- grid_approx_set_obs(func_dat, grid)\n",
    "    \n",
    "    # calculate h-modal depths\n",
    "    fdepths <- hM_depth(matr_dat, grid)\n",
    "    \n",
    "    # Approximate a value of C\n",
    "    C_appr <- approx_C(matr_dat = matr_dat, fdepths = fdepths, alpha = alpha, B = B, gamma = gamma, grid = grid)\n",
    "    \n",
    "    # Perform the outlier classification procedure for the approximated value of C\n",
    "    flagged <- outlier_detection(matr_dat = matr_dat, ids = ids, grid = grid, C = C_appr)\n",
    "    \n",
    "    # Return the list of outlier ids and outlier indices - these are useful in different cases\n",
    "    return(flagged)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420eb1c-a37b-4c87-a909-4961d6102c7a",
   "metadata": {},
   "source": [
    "### Sampling Approach <a name=\"impl_sampling\"></a>\n",
    "\n",
    "#### Helper function for parallelization\n",
    "Since data sets can get large very quickly, it is useful to perform parallelization in this sampling approach in a less memory intensive way. Therefore I decided to write the data set in its prepared form to the disk and use a format that supports random access in lists, to only read in the observations that are part of the sample. After reading in those observations, it performs the outlier detection procedure implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2e8be-5a08-42ca-b7b2-2be4d8173e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_path: path to the random access list of the data set (generated by package largeList)\n",
    "# index: index of observations to use in the procedure\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "\n",
    "random_access_par_helper <- function(list_path, ids, index, alpha, B, gamma){\n",
    "    \n",
    "    # read in the observations identified by the variable index\n",
    "    func_dat <- readList(file = list_path, index = index)\n",
    "\n",
    "    # perform the outlier detection procedure on the sample\n",
    "    # in a tryCatch statement as the procedure creates notamatrix errors in random cases\n",
    "    sample_flagged <- tryCatch(\n",
    "        {detection_wrap(func_dat = func_dat, ids = ids, alpha = alpha, B = B, gamma = gamma)},\n",
    "         error=function(cond){\n",
    "             return(list(outlier_ids = c(), outlier_ind = c()))}\n",
    "    )\n",
    "    \n",
    "    # return the object generated by the outlier detection procedure\n",
    "    return(sample_flagged$outlier_ids)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09ecdd-38a1-4bf8-973f-f0cc7519cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl: cluster object generated by parallel package\n",
    "# n_obs: number of observations in data set\n",
    "# n_samples: number of samples to use\n",
    "# sample_size: number of observations to use in each sample\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "# list_path: path to the random access list of the data set (generated by package largeList)\n",
    "\n",
    "sampling_wrap <- function(cl, n_obs, n_samples, sample_size, alpha, B, gamma, list_path){  \n",
    "    \n",
    "    ids <- 1:n_obs\n",
    "    \n",
    "    # Initialize vectors described in the theoretical section\n",
    "    num_samples <- rep(x = 0, times = n_obs)\n",
    "    num_outliers <- rep(x = 0, times = n_obs)\n",
    "    frac_outliers <- rep(x = 1, times = n_obs)\n",
    "    \n",
    "    # Draw indexes for sampling from functional data without replacement\n",
    "    sample_inds <- map(.x = 1:n_samples, \n",
    "                       .f = function(i) sample(x = ids, size = sample_size, replace = FALSE))\n",
    "    \n",
    "    # Determine how often each observation appeared in the samples and update the vector                      \n",
    "    freq_samples <- tabulate(unlist(sample_inds))  \n",
    "    num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples                   \n",
    "    \n",
    "    # Perform the outlier classification procedure on the chosen samples parallelized\n",
    "    # with the function clusterApplyLB() from the parallel package                       \n",
    "    sample_flagged_par <- clusterApplyLB(cl = cl,\n",
    "                                         x = sample_inds,\n",
    "                                         fun = function(smpl){\n",
    "                                             random_access_par_helper(list_path = list_path, ids = ids[smpl],\n",
    "                                                                      index = smpl, alpha = alpha, \n",
    "                                                                      B = B, gamma = gamma)})  \n",
    "    \n",
    "    # Determine how often each observation were flagged in the samples and update the vector                     \n",
    "    freq_outliers <- tabulate(unlist(sample_flagged_par))\n",
    "    num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "    \n",
    "    # termine fraction of samples each observation was flagged as an outlier in                       \n",
    "    certainties <- unlist(map(.x = 1:n_obs,\n",
    "                              .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i]/num_samples[i], 1)))\n",
    "    \n",
    "    # Return list containing the three central vectors: num_samples, num_outliers, certainties                                  \n",
    "    return(list(num_samples = num_samples,\n",
    "                num_outliers = num_outliers,\n",
    "                certainties = certainties))                              \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b231a-67a6-4ec5-b32a-13b232c1376b",
   "metadata": {},
   "source": [
    "#### How to use this function?\n",
    "Using the function *sampling_wrap()* is not as straight-forward as using the previous as multiple steps have to be performed before and after using this function to ensure a problem free experience.\n",
    "I will explain the following code fragments in detail, as depending on which operating system a user employs modifications have to be made. For this reason, the code is commented out in these parts, in order not to cause technical problems that are difficult to replicate. The following code segments were written on a Linux machine and will work on UNIX systems. Since forking is not supported under windows, alternatives would have to be used, like using PSOCK-Clusters instead of ForkClusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56eed5e-9c0c-45f7-9b9e-b2548bdcf3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cores <- detectCores()\n",
    "# cl <- makeForkCluster(num_cores)\n",
    "#\n",
    "#                \n",
    "# invisible(clusterCall(cl, fun = function() library('largeList')))\n",
    "# invisible(clusterCall(cl, fun = function() library('Rcpp')))\n",
    "# invisible(clusterCall(cl, fun = function() library('purrr')))\n",
    "# invisible(clusterCall(cl, fun = function() library('MASS')))\n",
    "# invisible(clusterCall(cl, fun = function() sourceCpp('auxiliary/rcpp_functions.cpp')))\n",
    "#                   \n",
    "# clusterExport(cl, varlist = list(\"grid_approx_set_obs\",\n",
    "#                                  \"approx_C\", \"grid_finder\",\n",
    "#                                  \"outlier_iteration\", \"outlier_detection\",\n",
    "#                                  \"detection_wrap\", \"random_access_par_helper\"),\n",
    "#                                  envir = .GlobalEnv)\n",
    "#\n",
    "# sampling_wrap(cl = cl, n_obs = n_obs, n_samples = n_samples, \n",
    "#               sample_size = sample_size, alpha = alpha, B = B, gamma = gamma, \n",
    "#               list_path = list_path)\n",
    "#\n",
    "# stopCluster(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd7fa4-4b6c-4859-88b7-ad7f48ab6024",
   "metadata": {},
   "source": [
    "* *num_cores()* detects the number of logical cores\n",
    "* *makeForkCluster()* creates a virtual cluster object that can serve as an argument to functions from the parallel package to perform parallelized computations\n",
    "* the *clusterCall()* calls execute the command inside on each individual node of the virtual cluster to ensure that all necessary packages are loaded in each instance\n",
    "* *clusterExport* this exports objects from an environment to each of the nodes. These can be functions or objects.\n",
    "\n",
    "(These last to steps are technically not necessary in case of a fork cluster, but will navigate around some hard to troubleshoot problems that can occur.)\n",
    "\n",
    "* *sampling_wrap()* performs the actions implemented above on the virtual cluster *cl*\n",
    "* *stopCluster()* stops the cluster, to ensure that it does not clog up the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf73e0d-2a33-46d4-9ba5-d03810d70cec",
   "metadata": {},
   "source": [
    "### Dynamic Splitting <a name=\"impl_splitting\"></a>\n",
    "\n",
    "#### Zeroing\n",
    "To use the dynamic splitting procedure in the previously described settings, it is first necessary to zero all observations. This is implemented for the functional observations and its results should be saved as a separate object for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623410f7-7159-45ae-89ea-9c2220f0352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_dat: list that contains the observations\n",
    "# each observation is a list, that contains two vectors of identical length: args and vals\n",
    "\n",
    "zero_observations <- function(func_dat){\n",
    "    zeroed_func_dat <- map(.x = func_dat,\n",
    "                           .f = function(fnc){\n",
    "                               args = fnc$args - fnc$args[1]\n",
    "                               return(args = args, vals = fnc$vals)\n",
    "                           })\n",
    "    return(zeroed_func_dat)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e40641-da71-4619-8333-248fc3566faf",
   "metadata": {},
   "source": [
    "#### Determine measuring intervals\n",
    "This function returns a matrix containing in each row the beginning and end point of the measuring interval of the corresponding observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f35dd0-fa3d-4478-b67d-1715b1fc148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_dat: list that contains the observations\n",
    "# each observation is a list, that conatins two vectors of identical length: args and vals\n",
    "\n",
    "measuring_int <- function(func_dat){\n",
    "    intervals <- matrix(data = unlist(map(.x = func_dat,\n",
    "                                          .f = function(obs) c(min(obs$args), max(obs$args)))),\n",
    "                        nrow = length(func_dat), byrow = TRUE)\n",
    "                     \n",
    "    return(intervals)                     \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdd45f-5bc8-4a1a-a8c7-a4c6db291303",
   "metadata": {},
   "source": [
    "#### Create a list of measuring intervals that occur in the data set\n",
    "This set is needed later on for iterating through all possible realizations of the measuring interval to determine the comparable sets for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5646a9-cfb0-4757-bb32-cec133ce39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring_intervals: use output from measuring_int()\n",
    "\n",
    "unique_intervals <- function(measuring_intervals){\n",
    "    \n",
    "    # for finding unique entries transforming to a list is easier\n",
    "    interval_list <- map(.x = seq_len(nrow(measuring_intervals)), \n",
    "                         .f = function(i) measuring_intervals[i,])\n",
    "                         \n",
    "    # find unique entries                             \n",
    "    unique_intervals <- unique(interval_list)\n",
    "    \n",
    "    # combine into matrix again                         \n",
    "    unique_matrix <- matrix(data = unlist(unique_intervals), \n",
    "                            nrow = length(unique_intervals),\n",
    "                            byrow = TRUE)\n",
    "                         \n",
    "    # return matrix where each row contains the beginning and end points of a unique measuring interval\n",
    "    # from the data set\n",
    "    return(unique_matrix)                         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c3399-37e5-40ef-bc43-e454de94e124",
   "metadata": {},
   "source": [
    "#### Find comparable observations for one measuring interval\n",
    "Given a measuring interval that is currently under consideration and the matrix of all measuring intervals, determine the indices of the observations that are comparable given an acceptable stretching factor $\\lambda$. Here zeroing is assumed, such that the condition simplifies to a condition on the endpoint of the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90253bb5-aa19-4f78-aa24-5a0d3a4cc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_interval: vector of two elements: starting and end point of measuring interval\n",
    "# measuring_intervals: use output from measuring_int()\n",
    "# lambda: acceptable stretching parameter\n",
    "# ids: identifiers of individual observations\n",
    "\n",
    "comparable_obs_finder <- function(main_interval, measuring_intervals, lambda, ids){\n",
    "    \n",
    "    # Determine comparable observations by checking interval endpoints\n",
    "    comparable <- which(measuring_intervals[,2] >= main_interval[2]/lambda \n",
    "                        & measuring_intervals[,2] <= main_interval[2]*lambda)\n",
    "    \n",
    "    # Return the correspoding indices and the ids of the comparable observations\n",
    "    return(list(ind = comparable,\n",
    "                ids = ids[comparable]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279ce02-3501-4fdd-992d-7c9c252f1830",
   "metadata": {},
   "source": [
    "This function can then be used for determining which sets to sample from using the sampling procedure implemented above while iterating through the measuring intervals that occur in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfecc4-ecff-4944-b503-d714efcc4f63",
   "metadata": {},
   "source": [
    "### Full Procedure <a name=\"impl_full\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515d128-c80d-4afc-8aa6-72cd7da197c6",
   "metadata": {},
   "source": [
    "#### Stretching an observation\n",
    "The first function needed for the full procedure is the ability to stretch an observation to be comparable to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321db26e-9905-41ae-97bc-d251cc1885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs: a list that conatins two vectors of identical length: args and vals\n",
    "# measuring_interval: a vector with 2 elements, the start and end points of the desired measuring interval\n",
    "\n",
    "stretch_obs <- function(obs, measuring_interval){\n",
    "    \n",
    "    # calculate stretching factor\n",
    "    phi <- (measuring_interval[2] - measuring_interval[1]) / (max(obs$args) - min(obs$args))\n",
    "    \n",
    "    # stretch arguments by appropriate factor\n",
    "    args_stretched <- obs$args * phi\n",
    "    \n",
    "    # return in the format for functional observations\n",
    "    return(list(args = args_stretched,\n",
    "                vals = obs$vals))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b89e3-31da-4ca8-aff3-3e0a9474d92b",
   "metadata": {},
   "source": [
    "#### Stretching a set of observations to prepare sampling procedure\n",
    "This function acts as a wrapper for the previous function and applies it to a set of observations that are stretched to the same measuring interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0b376-5ceb-4fa2-81d4-896e7106eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_dat: list that contains the observations\n",
    "# each observation is a list, that conatins two vectors of identical length: args and vals\n",
    "# measuring_interval: a vector with 2 elements, the start and end points of the desired measuring interval\n",
    "\n",
    "stretch_data <- function(func_dat, measuring_interval){\n",
    "    \n",
    "    # apply function stretch_obs() to each observation in the data set\n",
    "    stretch_dat <- map(.x = func_dat,\n",
    "                       .f = function(obs) stretch_obs(obs = obs, measuring_interval = measuring_interval))\n",
    "\n",
    "    # return list of stretched observations                       \n",
    "    return(stretch_dat)                       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c239b-8964-460f-9ce7-03cabebe5a61",
   "metadata": {},
   "source": [
    "#### Performing stretching and sampling procedure on set of observations\n",
    "This is nearly identical to the functions for the sampling procedure itself and could easily be included as a subcase. For the sake of clarity, I nevertheless decided to make these separate functions.\n",
    "Understanding the sampling procedure and the stretching functions will make these functions easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb28f25-857f-43b7-8635-394778e263c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_path: path to the random access list of the data set (generated by package largeList)\n",
    "# index: index of observations to use in the procedure\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "# gamma: tuning parameter for smoothed bootstrap\n",
    "# measuring_interval: a vector with 2 elements, the start and end points of the desired measuring interval\n",
    "\n",
    "random_access_par_stretch_helper <- function(list_path, ids, index, alpha, B, gamma, measuring_interval){\n",
    "    \n",
    "    # read in the observations identified by the variable index\n",
    "    func_dat <- stretch_data(func_dat = readList(file = list_path, index = index),\n",
    "                             measuring_interval = measuring_interval)\n",
    "    \n",
    "    # perform the outlier detection procedure on the sample\n",
    "    # in a tryCatch statement as the procedure creates notamatrix errors in random cases\n",
    "    sample_flagged <- tryCatch(\n",
    "        {detection_wrap(func_dat = func_dat, ids = ids, alpha = alpha, B = B, gamma = gamma)},\n",
    "         error=function(cond){\n",
    "             return(list(outlier_ids = c(0), outlier_ind = c(0)))}\n",
    "    )\n",
    "    \n",
    "    # return the object generated by the outlier detection procedure\n",
    "    return(sample_flagged$outlier_ids)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc16dcb-c1dc-452a-bab2-04c9dd25d77e",
   "metadata": {},
   "source": [
    "This function performing the parallelized sampling is again very similar. It only differs in four points:\n",
    "* argument measuring_interval: interval the observations are stretched to\n",
    "* argument comparable: ids of the observations to include in the process as now not all observations are to be sampled in the same process\n",
    "* no argument n_obs: this can be replaced by comparable\n",
    "* use of the appropriately changed helper function for the parallelization\n",
    "\n",
    "Be careful when using the following function. The vectors that are returned only relate to the set of comparable observations currently under consideration. <br>\n",
    "A value of 3 in the first entry of *num_samples* means that the first observation of the whole data set occurred in 3 samples, not that the first observation from the set of comparable observations occurred in 3 samples. This has to be kept in mind while later using the output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab354d-39cb-4c74-87a8-385673789f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl: cluster object generated by parallel package\n",
    "# n_samples: number of samples to use\n",
    "# sample_size: number of observations to use in each sample\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C)\n",
    "# gamma: tuning parameter for smoothed bootstrap (in approximation of C)\n",
    "# list_path: path to the random access list of the data set (generated by package largeList)\n",
    "# measuring_interval: a vector with 2 elements, the start and end points of the desired measuring interval\n",
    "# comparable: vector with the indices of comparable observations in the largelist\n",
    "\n",
    "stretch_and_sample <- function(cl, n_samples, sample_size, alpha, B, gamma, list_path, measuring_interval, comparable, n_obs){\n",
    "    \n",
    "    ids <- comparable\n",
    "    \n",
    "    # Initialize vectors described in the theoretical section\n",
    "    num_samples <- rep(x = 0, times = n_obs)\n",
    "    num_outliers <- rep(x = 0, times = n_obs)\n",
    "    \n",
    "    # Draw indexes for sampling from functional data without replacement\n",
    "    sample_inds <- map(.x = 1:n_samples, \n",
    "                       .f = function(i) sample(x = ids, size = sample_size, replace = FALSE))\n",
    "    \n",
    "    # Determine how often each observation appeared in the samples and update the vector                      \n",
    "    freq_samples <- tabulate(unlist(sample_inds))  \n",
    "    num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples                   \n",
    "    \n",
    "    # Perform the outlier classification procedure on the chosen samples parallelized\n",
    "    # with the function clusterApplyLB() from the parallel package                       \n",
    "    sample_flagged_par <- clusterApplyLB(cl = cl,\n",
    "                                         x = sample_inds,\n",
    "                                         fun = function(smpl){\n",
    "                                             random_access_par_stretch_helper(list_path = list_path, ids = smpl,\n",
    "                                                                              index = smpl, alpha = alpha, B = B, gamma = gamma, \n",
    "                                                                              measuring_interval = measuring_interval)})  \n",
    "    \n",
    "    # Determine how often each observation were flagged in the samples and update the vector                     \n",
    "    freq_outliers <- tabulate(unlist(sample_flagged_par))\n",
    "    num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "    \n",
    "    # Return list containing the three central vectors: num_samples, num_outliers, certainties                                  \n",
    "    return(list(num_samples = num_samples,\n",
    "                num_outliers = num_outliers))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d9ec4-cda2-48b0-ac8f-a5ff7a988e61",
   "metadata": {},
   "source": [
    "#### Putting the pieces together\n",
    "Now that there are functions that\n",
    "* Find the set of measuring intervals that occur in the data set\n",
    "* Find comparable observations given a specific measuring interval and an acceptable stretching factor\n",
    "* Perform the sampling procedure with acceptable stretching on comparable subsets of the data set\n",
    "\n",
    "it is possible to assemble the pieces into one cohesive function that performs the outlier classification procedure described above on a data set where zeroing is admissible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d789f-5737-4d55-bef5-f1cd63014e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl: cluster object generated by parallel package\n",
    "# list_path: path to the random access list of the data set (generated by package largeList)\n",
    "# measuring_intervals: matrix of measuring intervals\n",
    "# n_obs: number of observations in the data set\n",
    "# lambda: acceptable stretching parameter\n",
    "# n_samples: number of samples to use in each iteration (NULL for procedure determining value)\n",
    "# sample_size: number of observations to use in each sample in each iteration (NULL for procedure determining value)\n",
    "# alpha: quantile of least deep observations to drop before bootstrapping (in approximation of C) (NULL for procedure determining value)\n",
    "# B: number of smoothed bootstrap samples to use (in approximation of C) (NULL for procedure determining value)\n",
    "# gamma: tuning parameter for smoothed bootstrap (in approximation of C)\n",
    "\n",
    "dectection_zr_smpl <- function(cl, list_path, measuring_intervals, n_obs, lambda, n_samples = NULL, sample_size = NULL, alpha = NULL, B = NULL, gamma = 0.05){\n",
    "    \n",
    "    # generate useful identifies for vectors\n",
    "    ids <- 1:n_obs\n",
    "    \n",
    "    # create vectors as described in the description part\n",
    "    num_samples <- rep(x = 0, times = n_obs)\n",
    "    num_outliers <- rep(x = 0, times = n_obs)\n",
    "    frac_outliers <- rep(x = 1, times = n_obs)\n",
    "     \n",
    "    # determine unique intervals to iterate through\n",
    "    unique_intervals <- unique_intervals(measuring_intervals)\n",
    "    n_intervals <- dim(unique_intervals)[1]\n",
    "    \n",
    "    # iteration process\n",
    "    for(i in 1:n_intervals){\n",
    "        \n",
    "        # Possible output\n",
    "        # print(paste0(i, \" out of \", n_intervals))\n",
    "        \n",
    "        # find comparable observations\n",
    "        comparable <- comparable_obs_finder(main_interval = unique_intervals[i,], \n",
    "                                            measuring_intervals = measuring_intervals, \n",
    "                                            lambda = lambda, ids = ids)$ids\n",
    "        \n",
    "        # do stretching and sampling procedure on current comparable observations\n",
    "        intv_res <- stretch_and_sample(cl = cl, n_samples = n_samples, sample_size = sample_size, \n",
    "                                       alpha = alpha, B = B, gamma = gamma, list_path = list_path,\n",
    "                                       measuring_interval = measuring_intervals[i,], comparable = comparable,\n",
    "                                       n_obs = n_obs)\n",
    "        \n",
    "        # update the vectors\n",
    "        num_samples <- num_samples + intv_res$num_samples\n",
    "        num_outliers <- num_outliers + intv_res$num_outliers\n",
    "    }\n",
    "    \n",
    "    # calculate the relative frequency of outliers\n",
    "    frac_outliers <- unlist(map(.x = 1:n_obs,\n",
    "                                .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i]/num_samples[i], 1)))\n",
    "    \n",
    "    # Return the three vectors                                \n",
    "    return(list(num_samples = num_samples,\n",
    "                num_outliers = num_outliers,\n",
    "                certainties = frac_outliers))                                \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97a136-e4d1-412c-83c6-d80d4ca17bae",
   "metadata": {},
   "source": [
    "### Updating <a name=\"impl_upd\"></a>\n",
    "The updating procedure is currently incomplete but will be finalized in future revisions of this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80c8c6-f907-432a-ab94-de95c840b0b6",
   "metadata": {},
   "source": [
    "#### Find measuring intervals with possible occurences of new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f6e2-6870-4de6-b5d3-9c38e8d2c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_func_obs: list with two elements: vectors of equal length called args and vals\n",
    "# new_id: identifier for the new observation (be careful not to create duplicates - for example just consecutive intergers would be fine)\n",
    "# list_path: path to the largeList object, where data set is saved\n",
    "# id_path: path to the RDS file with the identifiers\n",
    "\n",
    "obs_append <- function(new_func_obs, new_id, list_path, id_path){\n",
    "    \n",
    "    # read in previous ids\n",
    "    ids <- readRDS(file = id_path)\n",
    "    # append new ID\n",
    "    new_ids <- c(ids, new_id)\n",
    "    # overwrite old ID file\n",
    "    saveRDS(object = new_ids, file = id_path)\n",
    "    \n",
    "    # largeLists support appending\n",
    "    saveList(object = list(new_func_obs), file = list_path, append = TRUE) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf20fd-082a-400f-a140-01aabffb6d9b",
   "metadata": {},
   "source": [
    "#### Appending observation to list of functional data and ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e681007-2e36-4d5c-a911-3963e6dee1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_func_obs: list with two elements: vectors of equal length called args and vals\n",
    "# unique_intervals: matrix containing measuring intervals that occur in the data set (one in each row)\n",
    "# lambda: acceptable stretching parameter\n",
    "\n",
    "possible_occurences <- function(new_func_obs, unique_intervals, lambda){\n",
    "    \n",
    "    # determine measuring interval of new observation\n",
    "    new_measuring_interval <- c(min(new_func_obs$args),  max(new_func_obs$args))\n",
    "    \n",
    "    # determine for all previously used measuring intervals if the new\n",
    "    # observation could have been part of the stretching and sampling procedure\n",
    "    occurs <- map(.x = 1:(dim(unique_intervals)[1]),\n",
    "                      .f = function(i) ifelse(new_measuring_interval[2] >= unique_intervals[i,2]/lambda \n",
    "                                              & new_measuring_interval[2] <= unique_intervals[i,2]*lambda,\n",
    "                                              TRUE, FALSE))\n",
    "                  \n",
    "    # return the indices and intervals that the new observation could have been a part of\n",
    "    return(list(occurs = occurs,\n",
    "                occurs_intervals = unique_intervals[occurs, ]))                         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c054c-9d96-49ae-8bf7-8bbc184e35a9",
   "metadata": {},
   "source": [
    "#### Determine expected number of occurences in this measuring interval sampling run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b7d1a-6eec-41ac-b08b-3320d5808f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_n_obs: number of observations in the original comparable set (use output from comparable_obs_finder to determine this)\n",
    "# orig_n_samples: number of samples drawn in the original procedure (this can be updated by adding those drawn now for future updates)\n",
    "# orig_sample_size: sample size used in the original prrocedure for this comparable subset\n",
    "\n",
    "exp_num_samples <- function(orig_n_obs, orig_n_samples, orig_sample_size){\n",
    "    \n",
    "    # determine the probability that the observation would have been part of any original sample,\n",
    "    # if it had been part of the data set\n",
    "    # exp_per_sample <- choose(orig_n_obs, orig_sample_size - 1) / choose(orig_n_obs + 1, orig_sample_size)\n",
    "    # simplifies to:\n",
    "    exp_per_sample <- orig_sample_size / orig_n_obs\n",
    "    \n",
    "    # multiplay with the number of samples and return the number rounded to the next whole integer\n",
    "    return(ceiling(orig_n_samples * exp_per_sample))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd71ea-89f7-4c5a-b956-7bbd3484954f",
   "metadata": {},
   "source": [
    "#### Update one measuring interval\n",
    "\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a3968-f150-454b-92d0-ed82f0090b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c71800b-7462-43c5-8c5f-d5ad29c4bbd8",
   "metadata": {},
   "source": [
    "#### Updating for one new observation\n",
    "\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2e485-f748-4ccb-94c0-e36e9b6db47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "obs_update <- function(cl, new_func_obs, list_path, measuring_intervals, lambda, n_samples = NULL, \n",
    "                       prev_num_samples, prev_num_outliers, sample_size = NULL, alpha = NULL, B = NULL, \n",
    "                       gamma = 0.05){\n",
    "    \n",
    "    main_interval <- c(min(new_func_obs$args),  max(new_func_obs$args))\n",
    "    \n",
    "    comparable <- comparable_obs_finder\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bbade-ea3b-4a38-b2f9-45ce8cfdc83c",
   "metadata": {},
   "source": [
    "## Simulated Data <a name=\"simulated\"></a>\n",
    "---\n",
    "To show possible usecases of the implementation I create three datasets:\n",
    "1. **No sampling** <br> This data set shows the outlier classification procedure of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) in action. It is characterized by:\n",
    "    * Observations with identical measuring intervals \n",
    "    * Relatively few observations meaning that the algorithm can be applied without sampling <br>\n",
    "2. **Sampling** <br>A data set where observations share the measuring interval but which is large enough that sampling is necessary to use the algorithm <br>\n",
    "3. **Full procedure**\n",
    "    * A dataset where observations share the starting point of the measuring interval but have different endpoints. \n",
    "    * This is meant to show the full procedure on a data set that is effectively already zeroed.\n",
    "\n",
    "The functional form of the non-outliers in these data sets is approximately **linear** which has two reasons: <br>\n",
    "1. Simplicity \n",
    "2. The approximate linearity of the relationship that is fundamental to the main data set explored in this project (angle and torque when tightening a bolt). \n",
    "\n",
    "All of these data sets and the results of the outlier classification procedures applied to them can be seen in the **shiny app** provided in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbf22e-8b68-4476-a17f-4dd738487334",
   "metadata": {},
   "source": [
    "### No sampling:\n",
    "This data set is based on a simple data generating process. Each of the 500 observations is generated as follows:\n",
    "\n",
    "* Determine if the observation is generated as an outlier by drawing from a Bernoulli distributed random variable with parameter 0.05.\n",
    "* Determine the number of points $k$ where the function is observed. (In the terms of the method above the number of points where measurements are taken) This is drawn from a discrete uniform with elements $10, \\dots, 100$\n",
    "\n",
    "The generation of dependent variables depends on whether the observation is generated as an outlier. The general process is as follows: <br>\n",
    "\n",
    "* Draw k-2 realizations of $p \\sim U[0, 1] \\quad \\text{s.t.} \\quad p_1, \\dots, p_k \\quad \\text{i.i.d.}\\quad$ Let $p_{(1)}, \\dots, p_{(k-2)}$ be the sorted realizations and define $(0, \\: p_{(1)},\\: \\dots, \\: p_{(k-2)},\\: 1)^T = \\vec{p}$ <br>(This is equivalent to our grid of measuring points.)<br>\n",
    "* Draw k realizations of $s \\sim U[\\underset{\\bar{}}{s}, \\bar{s}] \\quad \\text{s.t.} \\quad s_1, \\dots, s_k \\quad \\text{i.i.d.} \\quad$ and define $(s_1,\\: \\dots, \\: s_k)^T = \\vec{s}$\n",
    "* Draw k realizations of $\\epsilon \\sim \\mathcal{N}[0, \\sigma] \\quad \\text{s.t.} \\quad \\epsilon_1, \\dots, \\epsilon_k \\quad \\text{i.i.d.} \\quad$ and define $(\\epsilon_1,\\: \\dots, \\: \\epsilon_k)^T = \\vec{\\epsilon}$\n",
    "* Let $\\vec{y} = m \\vec{s} \\odot \\vec{p} + \\vec{\\epsilon}$ be the vector of realizations of the dependent variable, where $\\odot$ is the component-wise (or Hadamard) product.\n",
    "\n",
    "The parameters are different for non-outliers and outliers: <br>\n",
    "For non-outliers:\n",
    "* $\\underset{\\bar{}}{s} = 0.8, \\quad \\bar{s} = 1.2$\n",
    "* $\\sigma = 0.05$\n",
    "* $m = 1.02$\n",
    "\n",
    "and for outliers:\n",
    "* $\\underset{\\bar{}}{s} = 1, \\quad \\bar{s} = 1.4$\n",
    "* $\\sigma = 0.1$\n",
    "* $m = 1.2 * 1.02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09765d-54e6-4bcc-9bfb-6cb92533309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_1.R\n",
    "# Data set is also saved to ./data/Set_1 and if existent read from there instead\n",
    "# source(\"auxiliary/generate_set_1.R\")\n",
    "\n",
    "if(file.exists(\"./data/Set_1/functional.llo\") && \n",
    "   file.exists(\"./data/Set_1/ids.RDS\") && \n",
    "   file.exists(\"./data/Set_1/outliers.RDS\")){\n",
    "    data_set_1 <- list(data = readList(file = \"./data/Set_1/functional.llo\"),\n",
    "                       ids = readRDS(file = \"./data/Set_1/ids.RDS\"),\n",
    "                       outliers = readRDS(file = \"./data/Set_1/outliers.RDS\"))\n",
    "\n",
    "} else{\n",
    "    data_set_1 <- generate_set_1()\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_1/tibble.RDS\")){\n",
    "   tidy_set_1 <- readRDS(\"./data/Set_1/tibble.RDS\") \n",
    "} else{\n",
    "   tidy_set_1 <- tidify_1(data_set_1$data, data_set_1$ids)\n",
    "   saveRDS(object = tidy_set_1, file = \"./data/Set_1/tibble.RDS\")\n",
    "}\n",
    "                        \n",
    "vis_1(tidy_set_1)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c8411-cfe8-4051-bc0d-b09531fd2407",
   "metadata": {},
   "source": [
    "Prepare data for visualization in the shiny app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603716c4-100a-48d2-8af8-e19d0153f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(file.exists(\"./data/Set_1/detection.RDS\")){\n",
    "    set_1_detection <- readRDS(\"./data/Set_1/detection.RDS\")\n",
    "} else{\n",
    "    set_1_detection <- detection_wrap(func_dat = data_set_1$data, alpha = 0.05, B = 50, gamma = 0.05, ids = data_set_1$ids)\n",
    "    saveRDS(object = set_1_detection, file = \"./data/Set_1/detection.RDS\")\n",
    "}\n",
    "\n",
    "if(file.exists(\"./data/Set_1/summary.RDS\")){\n",
    "    set_1_summary <- readRDS(\"./data/Set_1/summary.RDS\")\n",
    "} else{\n",
    "    \n",
    "    missed_outliers = setdiff(data_set_1$outliers, set_1_detection$outlier_ind)\n",
    "    false_outliers = setdiff(set_1_detection$outlier_ind, data_set_1$outliers)\n",
    "    \n",
    "    set_1_summary <- list(flagged = set_1_detection$outlier_ind,\n",
    "                          original = data_set_1$outliers,\n",
    "                          missed = missed_outliers,\n",
    "                          false = false_outliers)\n",
    "    \n",
    "    saveRDS(object = set_1_summary, file = \"./data/Set_1/summary.RDS\")\n",
    "}\n",
    "\n",
    "if(file.exists(\"./data/Set_1/shiny_tibble.RDS\")){\n",
    "    shiny_tibble_1 <- readRDS(\"./data/Set_1/shiny_tibble.RDS\")\n",
    "} else{\n",
    "    shiny_tibble_1 <- tidy_set_1 %>%\n",
    "        mutate(outlier = ifelse(ids %in% data_set_1$outliers, TRUE, FALSE),\n",
    "               flagged = ifelse(ids %in% set_1_detection$outlier_ind, TRUE, FALSE))\n",
    "    saveRDS(object = shiny_tibble_1, file = \"./data/Set_1/shiny_tibble.RDS\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6584ad-0190-40ab-b03d-dde1a9167a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sampling:\n",
    "This second data set consists of 10000 observations where 95% are generated by the same process as the non-outlier variant in the previous data set. \n",
    "So a vector of arguments is drawn from the continuous uniform on $[0,1]$ of random length. Call this vector *args* in the following description.\n",
    "\n",
    "But the outliers take 5 different forms each appearing in about one percent of cases. I will describe their data generating processes in mathematical form in the following:\n",
    "\n",
    "1. Exactly as the outliers in data set 1.\n",
    "\n",
    "2. Sigmoid function: $\\frac{1}{\\mathbb{1} + \\exp{(-3 * \\text{args})}} + \\epsilon$ where $\\epsilon$ is a vector of appropriate length containing i.i.d. elements drawn from $\\mathcal{N}(0, 0.05^2)$\n",
    "\n",
    "3. Half sigmoid function: $\\frac{2}{\\mathbb{1} + \\exp{(-3 * \\text{args})}} - \\mathbb{1} + \\epsilon$ where $\\epsilon$ is a vector of appropriate length containing i.i.d. elements drawn from $\\mathcal{N}(0, 0.05^2)$\n",
    "\n",
    "4. Exponential function: $\\frac{\\exp(args) - \\mathbb{1}}{e - 1} + \\epsilon$ where $\\epsilon$ is a vector of appropriate length containing i.i.d. elements drawn from $\\mathcal{N}(0, 0.05^2)$\n",
    "\n",
    "5. Noise: i.i.d. draws from $U[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b225ffc-0baf-4164-862d-d86ee32d2bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_2.R\n",
    "# Data set is also saved to ./data/Set_2 and if existent read from there instead\n",
    "# source(\"auxiliary/generate_set_2.R\")\n",
    "\n",
    "if(file.exists(\"./data/Set_2/functional.llo\") && \n",
    "   file.exists(\"./data/Set_2/ids.RDS\") && \n",
    "   file.exists(\"./data/Set_2/outliers.RDS\")){\n",
    "    data_set_2 <- list(data = readList(file = \"./data/Set_2/functional.llo\"),\n",
    "                       ids = readRDS(file = \"./data/Set_2/ids.RDS\"),\n",
    "                       outliers = readRDS(file = \"./data/Set_2/outliers.RDS\"))\n",
    "\n",
    "} else{\n",
    "    data_set_2 <- generate_set_2()\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_2/tibble.RDS\")){\n",
    "   tidy_set_2 <- readRDS(\"./data/Set_2/tibble.RDS\") \n",
    "} else{\n",
    "   tidy_set_2 <- tidify_2(data_set_2$data, data_set_2$ids)\n",
    "   saveRDS(object = tidy_set_2, file = \"./data/Set_2/tibble.RDS\")\n",
    "}\n",
    "\n",
    "vis_2(tidy_set_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8a82d-c406-4f52-94ac-3835ab71f880",
   "metadata": {
    "tags": []
   },
   "source": [
    "Due to the computational cost of this classifying procedure I saved its results and only perform the classification, if those results could not be found. In the case of 10000 observations can still be done on the full set and I opted to use this lower number of observations not due to computational problems with larger data sets but due to overplotting in the visualizations, that has to be addressed, before large data sets can be appropriately visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c92f19-6228-4c42-9dda-55012e4c5182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(file.exists(\"./data/Set_2/results.RDS\")){\n",
    "    set_2_results <- readRDS(file = \"./data/Set_2/results.RDS\")\n",
    "\n",
    "} else{\n",
    "    num_cores <- detectCores()\n",
    "    cl <- makeForkCluster(nnodes = num_cores)\n",
    "                   \n",
    "    clusterExport(cl, varlist = list(\"grid_approx_set_obs\",\n",
    "                                     \"approx_C\",\n",
    "                                     \"grid_finder\",\n",
    "                                     \"outlier_iteration\",\n",
    "                                     \"outlier_detection\",\n",
    "                                     \"detection_wrap\",\n",
    "                                     \"random_access_par_helper\"),\n",
    "                                      envir = .GlobalEnv)\n",
    "                          \n",
    "    set_2_results <- sampling_wrap(cl = cl, n_obs = 10000, n_samples = 150, \n",
    "                               sample_size = 500, alpha = 0.05, B = 100, gamma = 0.05, \n",
    "                               list_path = \"./data/Set_2/functional.llo\")   \n",
    "    \n",
    "    stopCluster(cl)    \n",
    "    \n",
    "    saveRDS(object = set_2_results, file = \"./data/Set_2/results.RDS\")\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_2/shiny_tibble.RDS\")){\n",
    "    shiny_tibble_2 <- readRDS(\"./data/Set_2/shiny_tibble.RDS\")\n",
    "} else{\n",
    "    shiny_tibble_2 <- tidy_set_2 %>%\n",
    "        mutate(outlier = ifelse(ids %in% data_set_2$outliers, TRUE, FALSE))\n",
    "    \n",
    "    lengths <- unlist(map(.x = 1:10000,\n",
    "                          .f = function(i) length(data_set_2$data[[i]]$vals)))\n",
    "                          \n",
    "    shiny_tibble_2$cert <- unlist(map(.x = 1:10000,\n",
    "                                      .f = function(i) rep(set_2_results$certainties[i], times = lengths[i])))\n",
    "    \n",
    "    saveRDS(object = shiny_tibble_2, file = \"./data/Set_2/shiny_tibble.RDS\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90385-2b80-447f-8cbb-d7e6fde77e58",
   "metadata": {
    "tags": []
   },
   "source": [
    "To see the results of this classification procedure for different values of the certainty threshold, you can use the shiny app provided in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230fcd-42c5-411b-926d-b18fc2068b93",
   "metadata": {},
   "source": [
    "### Full Procedure:\n",
    "This data set is similar to the sampling data set in its data generating process. There are 30000 observations each generated by a similar process as in data set 2.\n",
    "\n",
    "The main difference is, that the measuring intervals are not identical and are drawn from the following possibilities:\n",
    "\n",
    "| Endpoint of Measuring Interval \t| 0.9  \t| 1   \t| 1.1  \t| 1.5  \t| 1.6  \t| 1.7  \t| 1.9 \t| 2    \t| 2.1  \t|\n",
    "|:------------------------------:\t|------\t|-----\t|------\t|------\t|------\t|------\t|-----\t|------\t|------\t|\n",
    "|           Probability          \t| 0.05 \t| 0.2 \t| 0.05 \t| 0.07 \t| 0.15 \t| 0.08 \t| 0.1 \t| 0.25 \t| 0.05 \t|\n",
    "\n",
    "Also the data generating processes for both the outliers and the non-outliers are scaled such that the expected realizations at the beginning and end of the data set are identical. (Especially important for the exponential and sigmoidal outliers.) An exception from this are the type 5 outliers, which stay uniformly distributed - but the upper border of the support is made to fit the expected realization of the non-outlier process at the end point of the measuring interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9bd7e-e050-435a-a5e6-7cd6a650948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_3.R\n",
    "# Data set is also saved to ./data/Set_3 and if existent read from there instead\n",
    "source(\"auxiliary/generate_set_3.R\")\n",
    "\n",
    "if(file.exists(\"./data/Set_3/functional.llo\") && \n",
    "   file.exists(\"./data/Set_3/ids.RDS\") && \n",
    "   file.exists(\"./data/Set_3/outliers.RDS\")){\n",
    "    data_set_3 <- list(data = readList(file = \"./data/Set_3/functional.llo\"),\n",
    "                       ids = readRDS(file = \"./data/Set_3/ids.RDS\"),\n",
    "                       outliers = readRDS(file = \"./data/Set_3/outliers.RDS\"))\n",
    "\n",
    "} else{\n",
    "    data_set_3 <- generate_set_3()\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_3/tibble.RDS\")){\n",
    "   tidy_set_3 <- readRDS(\"./data/Set_3/tibble.RDS\") \n",
    "} else{\n",
    "   tidy_set_3 <- tidify_3(data_set_3$data, data_set_3$ids)\n",
    "   saveRDS(object = tidy_set_3, file = \"./data/Set_3/tibble.RDS\")\n",
    "}\n",
    "\n",
    "### Due to a lengthy plotting time and overplotting, this plot is saved an inserted as a png\n",
    "# vis_3(tidy_set_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee7ba9-9249-49e0-a5e7-0cc8f656d567",
   "metadata": {},
   "source": [
    "<img src=\"material/set_3.png\" width=\"1000\" align=\"center\"> \n",
    "\n",
    "Full procedurre still does not worrk. Produces certainties greater one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04c145-4e81-4e3d-a6f7-318c2a27f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(file.exists(\"./data/Set_3/results.RDS\")){\n",
    "   set_3_results <- readRDS(file = \"./data/Set_3/results.RDS\")\n",
    "} else{\n",
    "    num_cores <- detectCores()\n",
    "    cl <- makeForkCluster(nnodes = num_cores)\n",
    "                   \n",
    "    clusterExport(cl, varlist = list(\"grid_approx_set_obs\",\n",
    "                                     \"approx_C\",\n",
    "                                     \"grid_finder\",\n",
    "                                     \"outlier_iteration\",\n",
    "                                     \"outlier_detection\",\n",
    "                                     \"detection_wrap\",\n",
    "                                     \"random_access_par_helper\"),\n",
    "                                      envir = .GlobalEnv)\n",
    "    \n",
    "    measuring_intervals <- measuring_int(func_dat = data_set_3$data)\n",
    "\n",
    "    comp_test <- comparable_obs_finder(main_interval = c(0,1), measuring_intervals = measuring_intervals,  lambda = 1.2, ids = 1:30000)$ids\n",
    "    \n",
    "    #test <- stretch_and_sample(cl = cl, n_samples = 100, sample_size = 100, alpha = 0.02, B = 100, gamma = 0.05, n_obs = 30000,\n",
    "    #                           list_path = \"./data/Set_3/functional.llo\", measuring_interval = c(0,1), \n",
    "    #                           comparable = comp_test)\n",
    "    \n",
    "    set_3_results <- dectection_zr_smpl(cl = cl, list_path = \"./data/Set_3/functional.llo\", measuring_intervals = measuring_intervals,\n",
    "                                        n_obs = 30000, lambda = 1.2, n_samples = 300, sample_size = 300, alpha = 0.02, B = 100, gamma = 0.05)\n",
    "    \n",
    "    stopCluster(cl)    \n",
    "    \n",
    "   saveRDS(object = set_3_results, file = \"./data/Set_3/results.RDS\")\n",
    "}    \n",
    "\n",
    "if(file.exists(\"./data/Set_3/shiny_tibble.RDS\")){\n",
    "    shiny_tibble_3 <- readRDS(\"./data/Set_3/shiny_tibble.RDS\")\n",
    "} else{\n",
    "    shiny_tibble_3 <- tidy_set_3 %>%\n",
    "        mutate(outlier = ifelse(ids %in% data_set_3$outliers, TRUE, FALSE))\n",
    "    \n",
    "    lengths <- unlist(map(.x = 1:30000,\n",
    "                          .f = function(i) length(data_set_3$data[[i]]$vals)))\n",
    "                          \n",
    "    shiny_tibble_3$cert <- unlist(map(.x = 1:30000,\n",
    "                                      .f = function(i) rep(set_3_results$certainties[i], times = lengths[i])))\n",
    "    \n",
    "    saveRDS(object = shiny_tibble_3, file = \"./data/Set_3/shiny_tibble.RDS\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0314d9f-f149-4d25-80c0-9f05e1813e3f",
   "metadata": {},
   "source": [
    "## Shiny App <a name=\"shiny\"></a>\n",
    "---\n",
    "Instead of presenting the visualizations for the classification procedure in static graphics, I decided to use a shiny web app instead. This has multiple advantages in the setting of this final project, but the main motivation behind it was the use case described to me:<br> **An engineer wants to get a preselection of suspicious observations they should have a look at.**\n",
    "\n",
    "In this context, having a raw R file as output without an easy way to interact with it, would not be particularly useful. Instead, being able to run this shiny app on a local server with the precalculated values stored in a database, would be more in line with the idea of the job. This also motivates the features I implemented for the app (and plan to implement in future updates) like:\n",
    "* setting the focus to single observations\n",
    "* changing the plotting window\n",
    "* changing the centrainty threshold for observations to be classified as atypical\n",
    "\n",
    "etc.\n",
    "\n",
    "To start the shiny app, I recommend cloning this repository and executing the file **app.R** locally. This will start the shiny app on a local server. Instead, one can choose to use the **binder** button on the repo site to start it. Due to a lengthy build process, this is not the recommended way to look at it.\n",
    "\n",
    "The shiny app serves as the visualization for all results of the previously explained simulation studies and shows what a possible deployment of the method in a real world scenario could look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19790688-55b5-4fc3-9c4a-1d33ab43a429",
   "metadata": {},
   "source": [
    "## Outlook <a name=\"outlook\"></a>\n",
    "---\n",
    "Since this project will be continued in the next semester as part of the lecture \"OSE - Scientific Computing for Economists\" I want to give an outlook on what is to come as part of future revisions of this project.\n",
    "\n",
    "### Finalize Implementation of Updating Procedure\n",
    "In the current state of the project, the implementation of the updating procedure is incomplete. Future revisions will fill this incompleteness and add simulations to show the process of updating and compare cases where observations were added in an updating procedure to data sets containing them from the beginning. This can serve as a device to check the validity of the updating procedure.\n",
    "\n",
    "### Improvements of Implementation\n",
    "The implementation above still has some other problems. The most striking is the tendency of the sampling procedure to mark a full sample as atypical, if it contains a lower than expected fraction of atypical observations. <br>\n",
    "I plan to address this by introducing a rescaling factor to the cutoff threshold $C$ that adjusts for the removal of observations from the sample currently under consideration. This seems reasonable, as adding a new observation to a data set will always increase calculated depths for all observations. Therefore, removing observations can lead to overall lower depths and incrementally lower all observations under the cutoff that determines if they are classified as atypical in a sample. This is not described in the paper this project is based on - so it will make some theoretical work necessary to develop an appropritae correction.\n",
    "\n",
    "### Generalizations\n",
    "As described in a previous section, the method described in this project for data sets which allow for zeroing of the observations could be generalized by introducing another parameter **acceptable shifting** in addition to **acceptable stretching**. This could then be used to define a method that finds comparable subsets in data sets that do not allow zeroing. As this introduces more variables and makes a more sophisticated splitting algorithm necessary, it was out of the scope of this project, but will be addressed in the future.\n",
    "\n",
    "### Parameter Choice\n",
    "An important part of this procedure will be the choice of its tuning parameters: \n",
    "* In its purest form the algorithm needs a choice for $\\alpha$, $\\gamma$, $B$ and a grid to use for approximation purposes\n",
    "* Adding sampling adds the choice of sample size and number of samples\n",
    "* The full procedure then additionally needs $\\lambda$\n",
    "\n",
    "Some of these like sample size and number of samples could be made dependent on the structure of comparable subsets and even change when switching from one comparable subset to the next. One goal of this could be to make each observation appear in a similar number of samples overall. But other reasonable procedures are possible. <br>\n",
    "Others like $\\alpha$ and $\\lambda$ could be chosen by a simulation method. Constructing a similar but smaller data set with intentionally added outliers to perform cross validation or a similar procedure could be an approach for this case. A more sophisticated and detailed description of this method will be part of future revisions.\n",
    "\n",
    "### Making Results reproducible\n",
    "Currently, the results of the method are mostly non-reproducible when taking about exact numbers. Qualitative results of the procedure will be similar, but due to randomization in the method the replication of exact numbers is currently not possible. This can be addressed with some work to allow for manually setting seeds in the classification algorithm without sampling and in the choice of samples. Because of parallelization this cannot be done be setting the seed once in the beginning.\n",
    "\n",
    "### Performance Measures for the Algorithm & Benchmarking\n",
    "Tightly connected to the point of parameter choice is the question of how to measure the performance of the outlier detection procedure in different settings. \n",
    "* First, I am going to look for existing data sets that are commonly used to benchmark outlier classification procedures. The performance of this algorithm in these preclassified settings can serve as grounds for determining in which cases and using which parameter choices the method performs well and compare it to existing methods that are applicable in comparable scenarios. <br>\n",
    "* Second, as can be seen by the previously generated data, an ex ante classification of realizations created by different dgps may not appropriately cover the idea of outlyingness. Some realizations in previous data sets look very typical for the non-outlier dgp and a classification as atypical due to the different dgp could lead to an underestimation of the procedures performance. Therefore, a comparison to established methods can serve as a better tool to judge the effectiveness.\n",
    "\n",
    "### Identifying what contributes to outlyingness of an observation\n",
    "Once atypical observations in a data set are identified, it is very interesting to see what contributes to their outlyingness in the eyes of the algorithm. To create some ex-post explanation for why a classification decision was made would be a useful tool to inform future real-world decisions or improve the procedure itself by incorporating that information into the mechanism. Some interesting approaches to create an ex-post explanation are the following:\n",
    "\n",
    "1. Create slightly altered realizations of an observation that has been marked as an outlier and see what effect different alterations have on its classification (or certainty in case of the sampling-based methods).\n",
    "2. Compare locally similar observations that have different outcomes in the overall procedure.\n",
    "\n",
    "There are more ways to gain information on what features contribute to outlyingness, but due to the current scope of this project, more in-depth considerations of this approach will only be part of future revisions.\n",
    "\n",
    "### Creation of an Rcpp-Package to improve usability\n",
    "Further iterations of this project will be different in the way the functions are made available. Instead of implementing the functions directly in the main notebook, there will be an additional notebook explaining the implementation and an R package that can be installed directly that contains all functionality provided above. This will make it easier to use the functions in the future.\n",
    "\n",
    "### Improvements to the Shiny App\n",
    "The visualizations of the shiny app will also undergo considerable overhauls, starting with changes to which observations are plotted when focus is set to a single observation. In this case for example plotting more observations in the close vicinity of the observation might be of greater interest, whereas observations farther away may provide little information of use and could be plotted less frequently to avoid overplotting. <br>\n",
    "There are further improvements planned but due to the interactive process of designing an interface that is meant for direct interaction with the user, it is difficult to predict the exact nature of the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453be942-e54a-45c6-a95c-96e68244202c",
   "metadata": {},
   "source": [
    "## Sources <a name=\"sources\"></a>\n",
    "---\n",
    "* Cuevas, A. & Febrero-Bande, M. & Fraiman, R. (2006). On the use of bootstrap for estimating functions with functional data. Computational Statistics & Data Analysis. 51. 1063-1074.\n",
    "* Febrero-Bande, M. & Galeano, P. & Gonzàlez-Manteiga, W. (2008). Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels. Environmetrics. 19. 331 - 345.\n",
    "* Gijbels, I. & Nagy, S. (2017). On a General Definition of Depth for Functional Data. Statistical Science. 32. 630-639."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b06320-862b-409a-9d6e-e462e2de810d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b129f37-77fc-410b-a8eb-340ab5d4a010",
   "metadata": {},
   "source": [
    "Notebook by **Jakob R. Jürgens** <br>\n",
    "Final project for the course **OSE - data science** in the summer semester 2021<br>\n",
    "Find me at [jakobjuergens.com](https://jakobjuergens.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
