{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6e68e5-c963-4563-8caf-e7b313700c93",
   "metadata": {},
   "source": [
    "# Outlier Detection in Sensor Data using Functional Depth Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc9923b-daa1-4647-b05a-02aa4775997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in system(\"timedatectl\", intern = TRUE):\n",
      "“running command 'timedatectl' had status 1”\n"
     ]
    }
   ],
   "source": [
    "suppressMessages(library(MASS))\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(shiny))\n",
    "suppressMessages(library(shinydashboard))\n",
    "suppressMessages(library(largeList))\n",
    "suppressMessages(library(parallel))\n",
    "suppressMessages(library(Rcpp))\n",
    "# suppressMessages(library(gganimate)) #needed only to generate the gifs\n",
    "\n",
    "source(\"auxiliary/observation_vis.R\")\n",
    "source(\"auxiliary/distribution_vis.R\")\n",
    "source(\"auxiliary/updating_vis.R\")\n",
    "source(\"auxiliary/generate_set_1.R\")\n",
    "source(\"auxiliary/generate_set_2.R\")\n",
    "source(\"auxiliary/generate_set_3.R\")\n",
    "\n",
    "sourceCpp('auxiliary/rcpp_functions.cpp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b19a4b-efe2-4ed2-8272-b1b4e8c44ec7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Observation Structure](#observation)\n",
    "3. [The Algorithm](#algorithm)\n",
    "4. [h-modal depth](#h-modal)\n",
    "5. [Difficulties due to the Data](#difficulties)\n",
    "6. [Sampling Approach](#sampling)\n",
    "7. [Finding Comparable Sets of Observations](#comparables)\n",
    "8. [Description of Full Procedure for existing Data sets](#procedure)\n",
    "9. [Updating](#updating)\n",
    "10. [Simulated Data](#simulated)\n",
    "11. [Implementation](#implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a2cc7-bc7f-4b0b-884c-e60b46529dc2",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "---\n",
    "This project is part of a cooperation with **Daimler AG** and deals with outlier detection in sensor data from production processes. <br>\n",
    "One example of data like this is the relation of angle and torque during the process of tightening a bolt in a screwing connection. This data set will be called \"Endanzugsproblem\" in the following notebook and contains ~350000 observations of what can be imagined as a function that maps angles to torque. The following schematic will give an idea of what the data set represents and what the problem is:\n",
    "\n",
    "<img src=\"material/SchraubdatenPrinzipskizze.png\">\n",
    "\n",
    "To clarify some things about this simplified schematic:\n",
    "* The so called Enganzug is only part of the tightening process, but the parts of the observation happening before it are not subject of this analysis.\n",
    "* The focus of this project lies on curves that are \"In Ordnung\", so observations that do not immediately disqualiy themselves in some way or other by for example not reaching the fixed window of acceptable final values.\n",
    "* The observations typically have a high frequency of measuring torque but the measuring points are not equidistant\n",
    "* The angles where torque is measured are not shared between observations, but the measuring interval of angles might overlap between observations\n",
    "* The Endanzug does not start at the same angle for every observation and also does not necessarily start at the same torque\n",
    "* Outliers can be very general, so methods based on detecting only specific types of outliers may not be able to effectively filter out other suspicious observations. So optimally we would like to have some kind of Omnibus test for outliers.\n",
    "\n",
    "Especially due to the high frequency of measurement and the non-identical points where torque is measured the idea of interpreting each observation as a function and therefore approaching the problem from a standpoint of functional data analysis comes to mind. One method that is used in functional data analysis to identify outliers is based on what is called a \"functional depth measure\". Gijbels and Nagy (2017) introduces the idea of depth as follows and then elaborate on the theoretical properties a depth function for functional data should possess.\n",
    "\n",
    "> For univariate data the sample median is well known\n",
    "to be appropriately describing the centre of a data\n",
    "cloud. An extension of this concept for multivariate\n",
    "data (say p-dimensional) is the notion of a point (in\n",
    "$\\mathbb{R}^p$) for which a statistical depth function is maximized. \n",
    "\n",
    "The idea is to define an analogous concept to centrality measures (such as the distance from some central tendency such as the median) in a scalar setting for functional data and then use those to determine which functions in a set are typical in some sense for the whole population. Due to the more applied nature of this project, I will not go into detail on the theoretical properties of the methods used, but focus on giving intuition why the chosen methods make sense in this context.\n",
    "\n",
    "The main inspiration for my approach to the problem of detecting outliers in a data set such as the one described above is the paper \"Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels\" by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I am going to first describe their algorithm, then present my implementation and finally apply it to a simulated data set mimicking the \"Endanzugsproblem\" as the original data is property of Daimler AG, which I cannot make public."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b574b3d-f896-4b6b-94bc-4d674ac8f1d5",
   "metadata": {},
   "source": [
    "## Observation Structure <a name=\"observation\"></a>\n",
    "---\n",
    "For the sake of clarity, I will show the typical structure of one observation and define a couple of objects I will refer to later.\n",
    "\n",
    "<img src=\"material/observation.png\" width=\"1000\" align=\"center\">\n",
    "\n",
    "| Observation \t| 1    \t| 2    \t| 3    \t| 4    \t| 5    \t| 6    \t| 7    \t| 8    \t| 9    \t| 10   \t| 11   \t| 12   \t| 13   \t| 14   \t| 15   \t| 16   \t| 17   \t| 18   \t| 19   \t| 20   \t|\n",
    "|:-----------:\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|------\t|\n",
    "|    Angle    \t| 2.01 \t| 2.21 \t| 2.91 \t| 3.00 \t| 3.07 \t| 3.95 \t| 4.33 \t| 4.35 \t| 4.41 \t| 4.74 \t| 4.77 \t| 5.06 \t| 6.33 \t| 6.37 \t| 6.41 \t| 6.57 \t| 7.25 \t| 7.32 \t| 7.71 \t| 7.94 \t|\n",
    "|    Torque   \t| 2.02 \t| 2.61 \t| 3.05 \t| 3.16 \t| 2.99 \t| 4.19 \t| 4.24 \t| 4.37 \t| 4.73 \t| 4.89 \t| 5.03 \t| 5.45 \t| 6.32 \t| 6.18 \t| 6.22 \t| 7.06 \t| 7.30 \t| 7.59 \t| 7.99 \t| 8.06 \t|\n",
    "\n",
    "This shows what one observation of the data set might look like.\n",
    "* The red diamonds represent measurements of torque that were taken at a recorded angle.\n",
    "* The blue lines are an example of **linear interpolation** which will become important later on.\n",
    "* The **measuring interval** marked in green is the convex hull of angles where measurements were taken for this observation.\n",
    "\n",
    "The data set contains many of these objects, that do not necessairly share these characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68e1d4ed-40ca-461f-8031-5e6fa2a02e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated using function from auxiliary/observation_vis.R\n",
    "# obs_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bdb4b-cd4a-43ec-99d6-3d072fbd5d1f",
   "metadata": {},
   "source": [
    "## The Algorithm <a name=\"algorithm\"></a>\n",
    "---\n",
    "The idea of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) is an iterative process that classifies observations as outliers if their functional depth lies below a threshold C, which is determined using a bootstrapping procedure in each iteration. The algorithm can be decomposed into two parts:\n",
    "\n",
    "1. **The iterative process**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i),\\: \\dots \\: ,D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Let $x_{i_1},\\: \\dots,\\: x_{i_k}$ be the k curves such that $D_n(x_{i_k}) \\leq C$, for a given cutoff C. Then, assume that $x_{i_1},\\: \\dots,\\: x_{i_k}$ are outliers and delete them from the sample.\n",
    "    3. Then, come back to step 1 with the new dataset after deleting the outliers found in step 2. Repeat this until no more outliers are found. \n",
    "    \n",
    "    <br>\n",
    "2. **Determining C**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i),\\: \\dots ,\\:D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Obtain B standard bootstrap samples of size n from the dataset of curves obtained after deleting the $\\alpha \\%$ less deepest curves. The bootstrap samples are denoted by $x_i^b$ for $i = 1,\\: \\dots,\\: n$ and $b = 1,\\: \\dots,\\: B$.\n",
    "    3. Obtain smoothed bootstrap samples $y_i^b = x_i^b + z_i^b$, where $z_i^b$ is such that $(z_i^b(t_1), \\dots, z_i^b(t_m))$ is normally distributed with mean 0 and covariance matrix $\\gamma\\Sigma_x$ where $\\Sigma_x$ is the covariance matrix of $x(t_1),\\: \\dots,\\: x(t_m)$ and $\\gamma$ is a smoothing parameter. Let $y_i^b$, $i = 1,\\: \\dots,\\:n$ and $b = 1,\\:\\dots,\\: B$ be these samples. *\n",
    "    4. For each bootstrap set $b = 1,\\:\\dots,\\:B$, obtain $C^b$ as the empirical 1% percentile of the distribution of the depths $D(y_i^b)$, $i = 1,\\: \\dots,\\: n$.\n",
    "    5. Take C as the median of the values of $C^b$, $b = 1,\\: \\dots,\\: B$. \n",
    "<br>\n",
    "*At this point we assume that our functional observations are observed at a set of discrete points $t_1,\\:\\dots,\\:t_m$.\n",
    "\n",
    "To clarify some things, that might get lost in these quotes:\n",
    "* n is not constant over the iterative process, which might be counterintuitive in the context of other methods\n",
    "* As a consequence: while approximating C the set of observations used is not constant over the iterative process either, as the outliers that have been removed are no longer under consideration for the bootstrap procedure\n",
    "\n",
    "The authors propose three functional depth measures and benchmark them in a simulation setting. Because of their results and the computational cost which are comparatively small, I chose to use **h-modal depth** for my implementation, which I will introduce in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63443be-611f-4d03-b664-93d445943d38",
   "metadata": {},
   "source": [
    "## h-modal depth <a name=\"h-modal\"></a>\n",
    "---\n",
    "\n",
    "Introduced by Cuevas, Febrero-Bande, and Fraiman (2006) h-modal depth is one of three depth measures covered in Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I will follow the summary in the latter paper for my overview.  <br>The idea behind this depth is that a curve is central in a set of curves if it is closely surrounded by other curves?\" <br>\n",
    "\n",
    "In mathematical terms the h-modal depth of a curve $x_i$ in relation to a set of curves $x_1, \\dots, x_n$ is defined as follows: <br>\n",
    "\\begin{equation}\n",
    "    MD_m(x_i,h) = \\sum_{k = 1}^{n} K(\\frac{||x_i - x_k||}{h})\n",
    "\\end{equation}\n",
    "\n",
    "where $K: \\mathbb{R^{+}} \\rightarrow \\mathbb{R^{+}}$ is a kernel function and $h$ is a bandwidth. <br>\n",
    "The authors recommend using the truncated Gaussian kernel, which is defined as follows:\n",
    "\\begin{equation}\n",
    "    K(t) = \\frac{2}{\\sqrt{2\\pi}} \\exp(-\\frac{t^2}{2})\n",
    "\\end{equation}\n",
    "\n",
    "and to choose $h$ as the 15th percentile of the empirical distribution of $\\{||x_i - x_k|| \\, ; \\, i,k = 1,\\:\\dots,\\:n\\}$\n",
    "\n",
    "I chose to implement the $L^2$ norm - one of the norms recommended by the authors - as it performed better than the $L^{\\infty}$ norm (which was also recommended) in my preliminary tests. In a functional setting $L^2$ is defined by:\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\int_a^b (x_i(t) - x_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "where a and b are the boundaries of the measurement interval. This can be replaced by its empirical version\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 1}^m (x_i(t_j) - x_k(t_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "in case of a discrete set of $m$ observation points shared between observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17ebc8-b676-4fef-b044-afd3a3a3e6c3",
   "metadata": {},
   "source": [
    "## Difficulties due to the Data <a name=\"difficulties\"></a>\n",
    "---\n",
    "### The Endanzug does not start at the same angle for every observation. <br>\n",
    "In this specific setting this is not much of a problem, as the specific angle where the \"Endanzug\" starts is not of importance. So all observations can be modified by subtracting the first angle of the \"Endanzug\" from all angles, effectively **zeroing** the observations.\n",
    "\n",
    "<img src=\"material/zero.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "### After zeroing, the measurement intervals might still not be identical due to differing lengths. <br>\n",
    "We can define a parameter $\\lambda \\geq 1$ that I call **acceptable stretching** and make observations comparable by stretching their measuring intervals by a factor $\\psi_i \\in [1/\\lambda, \\lambda]$ before approximating them using linear interpolation. If zeroing as described in 1. is not appropriate a combination of acceptable stretching and acceptable shifting could be implemented to increase the size of sets of pairwise comparable functions.\n",
    "\n",
    "<img src=\"material/stretch.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "As you can already see in this animation, the acceptable stretching introduces inaccuracies even in an approximately linear setting. Outlier classifications could be quite sensitive to this parameter.\n",
    "\n",
    "### The angles where torque is measured are not shared between observations. <br>\n",
    "Assuming that the measuring intervals are identical, we can use **linear interpolation** to approximate the observations and to make them compatible with the simplification described above. This is only an approximation, but choosing an appropriately fine grid to approximate the observations should limit the influence of this procedure on the calculated functional depths.\n",
    "\n",
    "<img src=\"material/grid_approx.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "Another possibility to approach this third problem would be to use a different version of the norm for discretized points I described above. <br>\n",
    "Instead of calculating \n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 1}^m (x_i(t_j) - x_k(t_j))^2}\n",
    "\\end{equation}\n",
    "\n",
    "one could instead define functions $\\tilde{x}_i$ which are just the piecewise linear functions defined by connecting the observed points of $x_i$. A norm based on this could be constructed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{\\tilde{2}} = \\sqrt{\\int_a^b (\\tilde{x}_i(t) - \\tilde{x}_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "For very fine grid approximations these criteria should result in similar depths, as in practice the second approach should be proportional to the limit of the first with an increasingly fine grid. <span style=\"color:red\"> BESSER BEGRÜNDEN</span>.\n",
    "\n",
    "### Runtime Complexity <br>\n",
    "The runtime complexity of this algorithm is at least $O(n^2)$ and I concluded that using my implementation it is infeasible use it on a very large data set such as the \"Endanzugsproblem\" (assuming that all observations are comparrable at once). Even when splitting up the observations as proposed above into comparable subsets, some of them will be too large to directly approach with this method. <br>\n",
    "To solve this problem I instead opted to use a **sampling approach**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40f8799d-49f3-40d2-9ff4-5d86db584d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The gifs have been rendered using function from auxiliary/observation_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/observation_vis.R\")\n",
    "# stretching_vis()\n",
    "# zeroing_vis()\n",
    "# lin_approx_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98161569-efdf-4525-ba07-b39c722aab8c",
   "metadata": {},
   "source": [
    "## Sampling Approach <a name=\"sampling\"></a>\n",
    "---\n",
    "Since it is infeasible to use this method on very large data sets at once, I instead made the assumption that observations that would be classified as outliers in the overall set will also be classified as outliers in subsets of the data set more frequently. So instead of performing the algorithm described above on the whole data set (or its comparable subsets), I chose to instead follow the following approach: <br>\n",
    "\n",
    "Let $\\{x_1, \\dots, x_L\\}$ be a set of observations that are comparable using the algorithm but too large to perform this procedure in reality.\n",
    "1. Define the following objects:\n",
    "    * Let *num\\_samples* $ = (a_1,\\:\\dots,\\: a_L) \\in \\mathbb{N}_0^L$ where $a_i$ is the number of samples $x_i$ was part of $\\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$. <br>(Initialize all entries as 0)\n",
    "    * Let *num\\_outliers* $ = (b_1,\\:\\dots,\\: b_L) \\in \\mathbb{N}_0^L$ where $b_i$ is the number of samples $x_i$ was classified as an outlier in $\\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$. <br>(Initialize all as entries 0)\n",
    "    * Let *frac\\_outliers* $ = (c_1,\\:\\dots,\\: c_L) \\in \\mathbb{R}_{\\geq 0}^L$ where $c_i = \\begin{cases}1 & a_i = 0 \\\\ \\frac{b_i}{a_i} & a_i > 0\\end{cases} \\quad \\forall i \\in \\{1,\\:\\dots,\\: L\\}$  <br>(Initialize all entries as 1)\n",
    "2. Draw a sample of size K from $\\{x_1,\\: \\dots,\\: x_L\\}$.    <span style=\"color:red\"> MIT REPLACEMENT ODER OHNE???</span>.\n",
    "3. Perform the outlier detection procedure on this sample and update the vectors accorrding to your results.\n",
    "4. Go back to two and iterate this process until some condition is fulfilled.\n",
    "\n",
    "Typical conditions could be:\n",
    "* A specified number of iterations was reached\n",
    "* Every observation was part of more than a specified number of samples\n",
    "* The vector of certainties did not change enough according to some criterion over a specified number of iterations\n",
    "\n",
    "In the end, the entries of *frac\\_outliers* can be used as a metric for the outlyingness of an observation. If a binary decision rule is needed, every observation with an entry over some specified threshold could be classified as an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fadb66-508b-4504-93d2-5b2ddc39365c",
   "metadata": {},
   "source": [
    "## Finding Comparable Sets of Observations <a name=\"comparables\"></a>\n",
    "---\n",
    "Assume for the sake of simplicity that **zeroing** is reasonable so that the minima of the measurement intervals of the observations are all zero. So the measurement intervals are different in their end points (or lengths which is identical in this case). As an example assume that the empirical distribution of endpoints looks as follows:\n",
    "\n",
    "<img src=\"material/dist.png\" width=\"1000\" align=\"center\">\n",
    "\n",
    "There are three options for generating comparable subsets that came to my mind:\n",
    "\n",
    "### Static Splitting\n",
    "\n",
    "Partition the whole data set into pairwise disjunct subsets of pariwise comparable observations. This partition would depend of the acceptable stretching parameter. Notice that these partitions are not necessarily unique for one acceptable stretching parameter and a choice procedure would have to be introduced if this approach were to be taken. <br>\n",
    "Some possible partitions of the set above (not necessarily consistent with the same acceptable stretching parameter) could look like this: \n",
    "\n",
    "<img src=\"material/static_splits.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "This idea has some problems:\n",
    "* The choice of subsets could introduce a new source of distortions in addition to the acceptable stretching parameter.\n",
    "* Adding new observations could change the chosen subsets, making an updating procedure difficult to realize.\n",
    "* In each individual subset, the observations that are changed due to stretching are identical over samples. This could lead to distortions since for some observations not the original but only the stretched observations are taken into account in the classification.\n",
    "\n",
    "### Dynamic Splitting\n",
    "\n",
    "Allocate the comparable subsets dynamically. For each realization of the endpoint determine the subset of comparable observations and perform the sampling approach described above on this subset but keep the parameter for acceptable strertching constant for all end points. This is the variant I chose for my implementation.\n",
    "\n",
    "<img src=\"material/dyn_splits.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "This approach has some advantages over the first one:\n",
    "* The choice of subsets becomes only a questing of the acceptable stretching parameter and not of the choice of algorithm that chooses the partition of the data set.\n",
    "* Adding new observations is unproblematic, as sampling can be done with them at the center. A procedure like this is described in the next section.\n",
    "* Each observation can enter the classification procedure undistorted in at least the comparable subset corresponding to its own endpoint. Additionally it can enter the classification in samples, where it is comparable due to acceptable stretching. The latter can realize for different degrees of stretching - increasing or decresing the length of the measuring interval. This solves the problem of observations being used for classification only in a specific distorted state.\n",
    "\n",
    "### Dynamic Splitting with varying acceptable stretching parameter\n",
    "\n",
    "As shown above, the difference in length of the measurement interval of comparable subsets changes quite substantially and does not react to the density of observations having similar measurement intervals. It would be possible to use an acceptable stretching parameter that changes locally as a function of the estimated density of end points (since zeroing was admissible in this example).\n",
    "Using a rather simple function determining the local acceptable stretching parameter to serve as an example leads to the following choice of comparable subsets.\n",
    "\n",
    "<img src=\"material/dyn_splits2.gif\" width=\"1000\" align=\"center\">\n",
    "\n",
    "In this example the effect is quite subtle, but in comparison to the previous animation, one can see that the expansion of the interval of end points of comparable observations comparable is slower in regions, where the estimated density of end points is higher. <br>\n",
    "There are two reasons why I decided against making acceptable stretching a varying parameter in my implementation:\n",
    "1. It introduces another complication as the function to determine the local acceptable stretching has to be chosen.\n",
    "2. It makes the updating procedure descrribed later more difficult, as adding more observations will change the estimated density of the end points and thereby change the comparable subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "592f2ce3-b4be-4c0b-b107-9f7001831d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The graphics and gifs have been rendered using function from auxiliary/observation_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/distribution_vis.R\")\n",
    "# dist_vis()\n",
    "# static_splits_vis()\n",
    "# dynamic_splits_vis()\n",
    "# dynamic_splits2_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2980c-964d-4c2f-bbf0-d849e57d438b",
   "metadata": {},
   "source": [
    "## Description of Full Procedure for existing Data sets <a name=\"procedure\"></a>\n",
    "---\n",
    "Having explained its parts:\n",
    "* The algorithm by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008)\n",
    "* The sampling approach \n",
    "* The procedure for selecting comparable subsets with dynamic splitting of the data set \n",
    "\n",
    "I am going to explain the full procedure applied to an existing data set assuming that zeroing is admissible. <br>\n",
    "\n",
    "### Definition of Objects\n",
    "* Let $x_1,\\: \\dots,\\: x_n$ be the full data set of observations as described in the beginning\n",
    "* Let $I_1 = [s_1, e_1], \\dots, I_n = [s_n, e_n]$ be the measuring intervals of $x_1, \\dots, x_n$\n",
    "\n",
    "Assuming that zeroing is admissible:\n",
    "* Let $\\bar{x}_1,\\: \\dots, \\:\\bar{x}_n$ be the corresponding zeroed observations\n",
    "* Let $\\bar{I}_1 = [0, \\: \\bar{e}_1], \\: \\dots, \\bar{I}_n = [0, \\bar{e}_n]$ be the corresponding zeroed measuring intervals\n",
    "* Let $\\bar{E} = \\{\\bar{e} \\: | \\: \\exists k \\in \\{1, \\dots, n\\} \\: \\text{s.t.} \\: \\bar{e} = \\max(\\bar{I}_k)\\}$ be the set of measuring interval end points occuring in $\\bar{I}_1,\\: \\dots,\\: \\bar{I}_n$\n",
    "* Let $\\bar{\\Lambda}(\\bar{x}, \\bar{e})$ be the resulting object when zeroed observation $\\bar{x}$ is stretched to fit zeroed measuring interval $\\bar{I} = [0, \\bar{e}]$\n",
    "\n",
    "Again assuming that zeroing is admissible, we can use the following objects. \n",
    "* Let $\\bar{Z}(\\bar{e}, \\lambda) = \\{\\bar{x}_k \\: | \\: \\frac{\\bar{e}_k}{\\bar{e}} \\in [\\frac{1}{\\lambda},\\lambda]\\}$ be the set of zeroed observations that can be compared to a zeroed observation with measuring interval $\\bar{I} = [0, \\bar{e}]$ with an acceptable stretching factor of $\\lambda$.\n",
    "* Let $\\bar{S}(\\bar{e}, \\lambda) = \\{\\bar{\\Lambda}(\\bar{e}, \\bar{x}) \\: | \\: \\bar{x} \\in \\bar{Z}(\\bar{e}, \\lambda)\\}$ be the set of zeroed observations that have been stretched with an admissible stretching factor to be comparable to a zeroed observation with zeroed measuring interval $\\bar{I} = [0, \\bar{e}]$.\n",
    "\n",
    "If zeroing is not a valid approach, we will have to define correspoding objects dependent on acceptable shifting and acceptable stretching.\n",
    "\n",
    "### Procedure\n",
    "1. Choose parameters:\n",
    "    * $\\lambda$ acceptable stretching\n",
    "    * $L$ sample size in sampling procedure (may also be varying depending on chosen approach for sampling)*\n",
    "    * $K$ number of equidistant points in grid for approximation by linear interpolation*\n",
    "    * $B$ sample size in estimation of cutoff value $C$ in outlier classification algorithm*\n",
    "    * $\\gamma$ tuning parameter in estimation of cutoff value $C$ in outlier classification algorithm*\n",
    "2. Initialize the following vectors:\n",
    "    * *num\\_samples* $ = (a_1,\\:\\dots,\\: a_n)\\in \\mathbb{N}_0^n \\quad$ with all entries being 0\n",
    "    * *num\\_outliers* $ = (b_1,\\:\\dots,\\: b_n)\\in \\mathbb{N}_0^n \\quad$ with all entries being 0\n",
    "    * *frac\\_outliers* $ = (c_1,\\:\\dots,\\: c_n) \\in \\mathbb{R}_{\\geq 0}^n \\quad$ with all entries being 1\n",
    "3. Iterate through the elements of $\\bar{E}$ in ascending order doing the following:\n",
    "    * Let $\\hat{e}$ be the element of $\\bar{E}$ currently looked at\n",
    "    * Determine $\\bar{S}(\\hat{e}, \\lambda)$ and perform the sampling based outlier identification procedure on this set for some predetermined stopping condition\n",
    "    * Update *num\\_samples*, *num\\_outliers*, and *frac\\_outliers* for both the stretched and non stretched observations in $\\bar{S}(\\hat{e}, \\lambda)$ as described in the section about sampling\n",
    "    * Go to the next element of $\\bar{E}$ and repeat until all elements have been reached.\n",
    "4. Report *frac\\_outliers* as a measure of outlyingness for the observations.\n",
    "\n",
    "*not explicitly mentioned in the procedure below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d441b-7b69-41d2-ba6b-c05f1ae73b78",
   "metadata": {},
   "source": [
    "## Updating <a name=\"updating\"></a>\n",
    "---\n",
    "The procedure described above is constructed to work for a full data set. In a day to day setting the data set will not be static. Instead new observations will be added and it would be a problem, if all calculations would have to be done all over again only to incorporate a comparatively small number of new observations.\n",
    "\n",
    "Instead one could devise a mechanism to assign comparable values of *frac\\_outliers* to the newly added observations and possibly also update the pre-existing observations due to the presence of the newly added ones. In the following assume that new observations are added sequentially. Two ways came to my mind to approach this updating procedure, one more true to the original process (1) and the other one less computationally expensive (2).\n",
    "\n",
    "### Version 1:\n",
    "This procedure insolves additional samples from all sets the new observation could have been part of. So both in a stretched form or in its original form <br>\n",
    "* Let $x'$ be the new observation and $\\bar{x}'$ its zeroed counterpart. Define $I'$, $\\bar{I'}$ and $\\bar{e}'$ accordingly.\n",
    "* Determine all elements $\\bar{e} \\in \\bar{E}$ such that $\\bar{x}' \\in \\bar{Z}(\\bar{e}, \\lambda)$. Define $\\bar{U}(\\bar{x}', \\lambda) = \\{\\bar{e} \\in \\bar{E} \\: | \\: \\bar{x}' \\in \\bar{Z}(\\bar{e}, \\lambda)\\}$ as the subset of $\\bar{E}$ called the Updating Window. <br> In this setting where zeroing is admissible, this can be simplified to $\\bar{U}(\\bar{x}', \\lambda) = \\{\\bar{e} \\in \\bar{E} \\: | \\: \\bar{e} \\in [\\frac{1}{\\lambda} \\bar{e}', \\lambda \\bar{e}']\\} = \\bar{E} \\cap [\\frac{1}{\\lambda} \\bar{e}', \\lambda \\bar{e}']$\n",
    "    \n",
    "<img src=\"material/update_1.gif\" width=\"1000\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400468a-f7cf-426d-b4dd-7cb79ea9baa9",
   "metadata": {},
   "source": [
    "For all $\\tilde{e} \\in \\bar{U}(\\bar{x}', \\lambda)$ perform a sampling procedure as follows: <br>\n",
    "* $\\bar{\\Lambda}(\\bar{x}', \\tilde{e})$ is part of each sample \n",
    "* The size of each sample is identical to the one used in the original procedure\n",
    "* The number of samples for each $\\tilde{e}$ is the expected value of the number of samples the new observation would have been part of, if it had been in the original data set\n",
    "* The updating procedure works as before. Not only the entry of the new observation in each vector is added and updated, also the entries for the original observations are updated.\n",
    "\n",
    "So the set of zeroed observations that is potentially updated during this procedure is the following: <br> \n",
    "$$\\bigcup_{\\tilde{e}\\in \\bar{U}(\\bar{x}', \\lambda)} \\bar{Z}(\\tilde{e}, \\lambda) = \\Big\\{\\bar{x} \\: | \\: \\bar{e} \\in \\big[\\frac{1}{\\lambda}\\min\\{\\bar{U}(\\bar{x}', \\lambda)\\}, \\lambda \\max\\{\\bar{U}(\\bar{x}', \\lambda)\\}\\big]\\Big\\}$$\n",
    "<br>\n",
    "Which is visualized in the following animation. (The endpoints of the potentially updated zeroed observations are marked by the red rectangle.)\n",
    "\n",
    "<img src=\"material/update_2.gif\" width=\"1000\" align=\"center\"> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dce8b0-35e1-4ad9-ac68-88e51625c847",
   "metadata": {},
   "source": [
    "        \n",
    "### Version 2:\n",
    "In comparison the other procedure involves only additional sampling from the set where the new observation is unchanged.\n",
    "* Let $x'$ be the new observation and $\\bar{x}'$ its zeroed counterpart. Define $I'$, $\\bar{I'}$ and $\\bar{e}'$ accordingly.\n",
    "* Determine $\\bar{S}(\\bar{e}', \\lambda)$ and perform additional sampling as follows:\n",
    "    * $\\bar{x}'$ is part of each sample\n",
    "    * The number of samples drawn is the expected value of samples the new observation would have been part of, if it had been in the original data set\n",
    "    * The updating procedure works as before. Not only the entry of the new observation in each vector is added and updated, also the entries for the original observations are updated.\n",
    "\n",
    "The following graphic shows the equivalent objects of version 1:\n",
    "\n",
    "<img src=\"material/update_3.png\" width=\"1000\" align=\"center\"> \n",
    "\n",
    "This procedure is less true to the values calculated in the original data set, as the new observation is only taken into consideration in its non-stretched form. Additionally pre-existing observations could be taken into consideration in their stretched form more frequently depending on the structure of new data being added which could lead to additional distortions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f18c20b-ce5c-42fa-82bd-7d3a9719fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The graphics and gifs have been rendered using functions from auxiliary/updating_vis.R (needs more packages, than are available in this environment)\n",
    "# source(\"auxiliary/updating_vis.R\")\n",
    "# upd_1_vis()\n",
    "# pot_upd_obs_vis()\n",
    "# upd_2_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bbade-ea3b-4a38-b2f9-45ce8cfdc83c",
   "metadata": {},
   "source": [
    "## Simulated Data <a name=\"simulated\"></a>\n",
    "---\n",
    "To show different properties of the procedure described above I am going to devise three separate simulated data sets:\n",
    "1. **No sampling**<br> This data set shows the outlier classification procedure of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) in action. It is characterized by:\n",
    "    * Observations with identical measuring intervals \n",
    "    * Relavtively few observations meaning that the algorithm can be applied without sampling\n",
    "2. **Sampling** <br> One data set where observations share the measuring interval but which is large enough that sampling is necessary to use the algorithm\n",
    "3. **Full procedure** <br> One where observations share the starting point of the measuring interval but have different endpoints to employ the full procedure on a data set that is effectively already zeroed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbf22e-8b68-4476-a17f-4dd738487334",
   "metadata": {},
   "source": [
    "### No sampling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94e85fff-47c6-4aa3-80cb-b52bd1360cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated, transformed and visualized with functions from auxiliary/generate_set_1.R\n",
    "# data_set_1 <- generate_set_1()\n",
    "# tidy_set_1 <- tidify_1(data_set_1$data, data_set_1$ids)            \n",
    "# vis_1(tidy_set_1)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe848ab-7d08-4ba3-834b-3bce18eb96d4",
   "metadata": {},
   "source": [
    "<img src=\"material/set_1_raw.png\" width=\"1000\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03850b93-1565-40df-b91f-4b2805c45544",
   "metadata": {},
   "source": [
    "## Implementation <a name=\"implementation\"></a>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ab8ba-1483-4edc-9339-daef05ee7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_approx_single_obs <- function(func_obs, grid) {    \n",
    "  return(grid_approx_obs(func_obs$args, func_obs$vals, grid))\n",
    "}\n",
    "\n",
    "grid_approx_set_obs <- function(func_dat, grid) {\n",
    "  res_mat <- matrix(data = unlist(\n",
    "      map(.x = func_dat,\n",
    "          .f = function(obs) grid_approx_single_obs(obs, grid))\n",
    "    ), nrow = length(func_dat), byrow = TRUE)\n",
    "                    \n",
    "  return(res_mat)\n",
    "}\n",
    "\n",
    "# func_dat needs to be observed at the same discretized points for every observation\n",
    "approx_C <- function(matr_data, fdepths, alpha, B, gamma, grid) {\n",
    "    \n",
    "  n <- length(fdepths)\n",
    "  depth_thr <- quantile(x = fdepths, probs = alpha)\n",
    "  matr_data_red <- matr_data[fdepths >= depth_thr, ]\n",
    "  n_red <- dim(matr_data_red)[1]\n",
    "    \n",
    "  Sigma_x <- cov(matr_data_red)\n",
    "  my_vcov <- gamma*Sigma_x  \n",
    "  grid_length <- length(grid)  \n",
    "\n",
    "  sample_inds <- map(.x = 1:B,\n",
    "                     .f = function(x) sample(x = 1:n_red, size = n, replace = TRUE))\n",
    "  \n",
    "  fsamples <- map(.x = sample_inds,\n",
    "                  .f = function(inds) matr_data_red[inds, ])\n",
    "  \n",
    "  smoothing_components <- map(.x = 1:B,\n",
    "                              .f = function(x) mvrnorm(n = n, mu = rep(0, times = grid_length), Sigma = my_vcov))\n",
    "  \n",
    "  smoothed_BS_samples <- map(.x = 1:B,\n",
    "                             .f = function(b) fsamples[[b]] + smoothing_components[[b]])\n",
    "\n",
    "  bootstrap_depths <- map(.x = 1:B,\n",
    "                          .f = function(b) hM_depth(smoothed_BS_samples[[b]]))\n",
    "\n",
    "  one_perc_quantiles <- unlist(map(.x = bootstrap_depths,\n",
    "                                   .f = function(sample) quantile(sample, probs = 0.01)))\n",
    "  \n",
    "  return(median(one_perc_quantiles))\n",
    "}\n",
    "\n",
    "### Like this for testing purposes (plan a method later)\n",
    "grid_finder <- function(func_dat){\n",
    "  return(seq(0, 1, length.out = 100))\n",
    "}\n",
    "\n",
    "### function for the iterative process\n",
    "outlier_iteration <- function(matr_data, alpha, B, gamma, ids, grid){\n",
    "    \n",
    "  fdepths <- hM_depth(matr_data)\n",
    "    \n",
    "  C <- approx_C(matr_data = matr_data, fdepths = fdepths, alpha = alpha,\n",
    "                B = B, gamma = gamma, grid = grid)\n",
    "    \n",
    "  outliers <- which(fdepths < C)\n",
    "  return(list(matr_data = matr_data[-outliers, ],\n",
    "              outlier_ids = ids[outliers],\n",
    "              outliers = outliers))\n",
    "}\n",
    "                                   \n",
    "### function for the whole algorithm\n",
    "outlier_detection <- function(func_dat, alpha, B, gamma, ids){\n",
    "    \n",
    "    condition <- TRUE\n",
    "    outliers <- c()\n",
    "    tmp_ids = ids\n",
    "    i <- 1\n",
    "    \n",
    "    grid <- grid_finder(func_dat)\n",
    "    data <- grid_approx_set_obs(func_dat, grid)\n",
    "    \n",
    "    while(condition){\n",
    "        tmp <- outlier_iteration(matr_data = data, alpha = alpha, B = B, gamma = gamma, ids = tmp_ids, grid = grid)\n",
    "        new_outliers <- tmp$outlier_ids\n",
    "        if(length(new_outliers) == 0){condition <- FALSE}\n",
    "        else{\n",
    "          outliers <- c(outliers, new_outliers)\n",
    "          data <- tmp$matr_data\n",
    "          tmp_ids <- tmp$ids \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return(outliers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e849a5-63e1-4ae5-9212-9aa9bcdef2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Sys.getenv('CI') == \"true\"){\n",
    "    num_outliers <- readRDS(file = \"data/num_outliers.RDS\")\n",
    "    num_samples <- readRDS(file = \"data/num_samples.RDS\")\n",
    "    certainties <- readRDS(file = \"data/certainties.RDS\")\n",
    "} else{\n",
    "total_samples <- 500\n",
    "k <- 100\n",
    "num_samples <- rep(x = 0, times = n)\n",
    "num_outliers <- rep(x = 0, times = n)\n",
    "certainties <- rep(x = 1, times = n)\n",
    "\n",
    "sample_inds <- map(.x = 1:total_samples, \n",
    "                   .f = function(i) sample(x = 1:n, size = k, replace = FALSE))\n",
    "                   \n",
    "freq_samples <- tabulate(unlist(sample_inds))\n",
    "num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples    \n",
    "                      \n",
    "### Parallelization:\n",
    "\n",
    "par_helper_fun <- function(list_path, index, alpha, B, gamma){\n",
    "    dat <- readList(file = list_path, index = index)\n",
    "    sample_flagged <- outlier_detection(func_dat = dat, alpha = 0.05, B = 50, gamma = 2, ids = index)\n",
    "}\n",
    "                      \n",
    "num_cores <- detectCores()\n",
    "cl <- makeForkCluster(num_cores)\n",
    "                   \n",
    "invisible(clusterCall(cl, fun = function() library('largeList')))\n",
    "invisible(clusterCall(cl, fun = function() library('Rcpp')))\n",
    "                   \n",
    "clusterExport(cl, varlist = list(\"grid_approx_single_obs\",\n",
    "                                 \"grid_approx_set_obs\",\n",
    "                                 \"approx_C\",\n",
    "                                 \"grid_finder\",\n",
    "                                 \"outlier_iteration\",\n",
    "                                 \"outlier_detection\",\n",
    "                                 \"par_helper_fun\"),\n",
    "             envir = .GlobalEnv)\n",
    "                      \n",
    "sample_flagged_par <- clusterApplyLB(cl = cl,\n",
    "                                     x = sample_inds,\n",
    "                                     fun = function(smp) par_helper_fun(list_path = \"data/functions.llo\", \n",
    "                                                                      index = smp, alpha = 0.05, \n",
    "                                                                      B = 50, gamma = 2))\n",
    "\n",
    "                      \n",
    "stopCluster(cl)\n",
    "\n",
    "### Only works if ids are integers (else slighty change function outlier detection to give back indices and not ids)                      \n",
    "#ind_outliers <- map(.x = sample_flagged,\n",
    "#                    .f = as.integer)                \n",
    "\n",
    "#freq_outliers <- tabulate(unlist(ind_outliers))\n",
    "                   \n",
    "freq_outliers <- tabulate(unlist(sample_flagged_par))\n",
    "num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "                \n",
    "certainties <- unlist(map(.x = 1:n,\n",
    "                          .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i]/num_samples[i], 1)))    \n",
    "\n",
    "saveRDS(object = num_outliers, file = \"data/num_outliers.RDS\")                          \n",
    "saveRDS(object = num_samples, file = \"data/num_samples.RDS\")\n",
    "saveRDS(object = certainties, file = \"data/certainties.RDS\")\n",
    "saveRDS(object = num_outliers, file = \"visual/data/num_outliers.RDS\")                          \n",
    "saveRDS(object = num_samples, file = \"visual/data/num_samples.RDS\")\n",
    "saveRDS(object = certainties, file = \"visual/data/certainties.RDS\")\n",
    "}                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4d297-711b-46c9-8fde-2153b3d8fe7f",
   "metadata": {},
   "source": [
    "Applying these to the original simulated data set yields the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbd8b6-4095-4534-b44b-15f28bb05646",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Sys.getenv('CI') == \"true\"){\n",
    "    num_outliers <- readRDS(file = \"data/num_outliers.RDS\")\n",
    "    num_samples <- readRDS(file = \"data/num_samples.RDS\")\n",
    "    certainties <- readRDS(file = \"data/certainties.RDS\")\n",
    "}\n",
    "\n",
    "print(which(num_samples == 0))                          \n",
    "flagged <- which(certainties > 0.5)\n",
    "original_outliers <- which(outliers == 1)\n",
    "missed_outliers <- setdiff(original_outliers, flagged)\n",
    "false_outliers <- setdiff(flagged, original_outliers)\n",
    "\n",
    "saveRDS(object = list(flagged = flagged,\n",
    "                      original = original_outliers,\n",
    "                      missed = missed_outliers,\n",
    "                      false = false_outliers),\n",
    "        file = \"data/outliers_info.RDS\")\n",
    "saveRDS(object = list(flagged = flagged,\n",
    "                      original = original_outliers,\n",
    "                      missed = missed_outliers,\n",
    "                      false = false_outliers),\n",
    "        file = \"visual/data/outliers_info.RDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fd93b-1f95-4c4b-84ba-54545fe688fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tibble for visualization\n",
    "\n",
    "my_tibble <- bind_rows(map(\n",
    "  .x = 1:n,\n",
    "  .f = function(i) {\n",
    "    tibble(\n",
    "      args = functions[[i]]$args,\n",
    "      vals = functions[[i]]$vals,\n",
    "      id = rep(i, times = length(args)),\n",
    "      cert = rep(certainties[i], times = length(args))\n",
    "    )\n",
    "  }\n",
    "))\n",
    "\n",
    "saveRDS(object = my_tibble, file = \"data/shiny_tibble.RDS\")\n",
    "saveRDS(object = my_tibble, file = \"visual/data/shiny_tibble.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d06ea8-97b9-47f0-9d54-c669acc5a38b",
   "metadata": {},
   "source": [
    "So at least in this simple setting the algorithm identifies nearly all generated outliers and does not classify any regular point as an outlier. This is not always the case and careful choice of the tuning parameters is necessary to ensure a good performance. In the case of other data generating processes, this method can also perform worse, but is applicable in very general settings due to its functional nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02253f-4b88-4eb0-98d2-f915d9d7156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Sys.getenv('CI') != \"true\"){suppressWarnings(runApp('visual'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768452e9-118c-4738-a37f-9e99ce7825df",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "---\n",
    "* Cuevas, A. & Febrero-Bande, M. & Fraiman, R. (2006). On the use of bootstrap for estimating functions with functional data. Computational Statistics & Data Analysis. 51. 1063-1074.\n",
    "* Febrero-Bande, M. & Galeano, P. & Gonzàlez-Manteiga, W. (2008). Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels. Environmetrics. 19. 331 - 345.\n",
    "* Gijbels, I. & Nagy, S. (2017). On a General Definition of Depth for Functional Data. Statistical Science. 32. 630-639."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
