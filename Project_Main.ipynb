{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b027d5-f9dd-4bb6-8144-8e1af13d88c1",
   "metadata": {},
   "source": [
    "# Outlier Detection in Sensor Data using Functional Depth Measures\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "This project is part of a cooperation with Daimler AG and deals with outlier detection in sensor data from production processes. <br>\n",
    "One example of data like this is the relation of angle and torque during the process of tightening a bolt in a screwing connection. This data set will be called \"Endanzugsproblem\" in the following notebook and contains ~350000 observations of what can be imagined as a function that maps angles to torque. The following schematic will give an idea of what the data set represents and what the problem is:\n",
    "\n",
    "<img src=\"material/SchraubdatenPrinzipskizze.png\">\n",
    "\n",
    "To clarify some things about this simplified schematic:\n",
    "* The so called Enganzug is only part of the tightening process, but the parts of the observation happening before it are not subject of this analysis.\n",
    "* The focus of this project lies on curves that are \"In Ordnung\", so observations that do not immediately disqualiy themselves in some way or other by for example not reaching the fixed window of acceptable final values.\n",
    "* The observations typically have a high frequency of measuring torque but the measuring points are not equidistant\n",
    "* The angles where torque is measured are not shared between observations, but the interval of angles might overlap between observations\n",
    "* The Endanzug does not start at the same angle for every observation and also does not necessarily start at the same torque\n",
    "* Outliers can be very general, so methods based on detecting only specific types of outliers may not be able to effectively filter out other suspicious observations. So optimally we would like to have some kind of Omnibus test for outliers.\n",
    "\n",
    "Especially due to the high frequency of measurement and the non-identical points where torque is measured the idea of interpreting each observation as a function and therefore approaching the problem from a standpoint of functional data analysis comes to mind. One method that is used in functional data analysis to identify outliers is based on what is called a \"functional depth measure\". Gijbels and Nagy (2017) introduces the idea of depth as follows and then elaborate on the theoretical properties a depth function for functional data should possess.\n",
    "\n",
    "> For univariate data the sample median is well known\n",
    "to be appropriately describing the centre of a data\n",
    "cloud. An extension of this concept for multivariate\n",
    "data (say p-dimensional) is the notion of a point (in\n",
    "$\\mathbb{R}^p$) for which a statistical depth function is maximized. \n",
    "\n",
    "The idea is to define an analogous concept to centrality measures (such as the distance from some central tendency such as the median) in a scalar setting for functional data and then use those to determine which functions in a set are typical in some sense for the whole population. Due to the more applied nature of this project, I will not go into detail on the theoretical properties of the methods used, but focus on giving intuition why the chosen methods make sense in this context.\n",
    "\n",
    "The main inspiration for my approach to the problem of detecting outliers in a data set such as the one described above is the paper \"Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels\" by Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I am going to first describe their algorithm, then present my implementation and finally apply it to a simulated data set mimicking the \"Endanzugsproblem\" as the original data is property of Daimler AG, which I cannot make public."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bdb4b-cd4a-43ec-99d6-3d072fbd5d1f",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "---\n",
    "The idea of Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008) is an iterative process that classifies observations as outliers if their functional depth lies below a threshold C, which is determined using a bootstrapping procedure in each iteration. The algorithm can be decomposed into two parts:\n",
    "\n",
    "1. **The iterative process**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i), \\dots ,D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Let $x_{i_1}, \\dots, x_{i_k}$ be the k curves such that $D_n(x_{i_k}) \\leq C$, for a given cutoff C. Then, assume that $x_{i_1}, \\dots, x_{i_k}$ are outliers and delete them from the sample.\n",
    "    3. Then, come back to step 1 with the new dataset after deleting the outliers found in step 2. Repeat this until no more outliers are found.\n",
    "<br>\n",
    "2. **Determining C**: (quoted from Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008))\n",
    "    1. Obtain the functional depths $D_n(x_i), \\dots ,D_n(x_n)$ for one of the functional depths [...]\n",
    "    2. Obtain B standard bootstrap samples of size n from the dataset of curves obtained after deleting the $\\alpha \\%$ less deepest curves. The bootstrap samples are denoted by $x_i^b$ for $i = 1, \\dots, n$ and $b = 1, \\dots, B$.\n",
    "    3. Obtain smoothed bootstrap samples $y_i^b = x_i^b + z_i^b$, where $z_i^b$ is such that $(z_i^b(t_1), \\dots, z_i^b(t_m))$ is normally distributed with mean 0 and covariance matrix $\\gamma\\Sigma_x$ where $\\Sigma_x$ is the covariance matrix of $x(t_1), \\dots, x(t_m)$ and $\\gamma$ is a smoothing parameter. Let $y_i^b$, $i = 1, \\dots,n$ and $b = 1,\\dots, B$ be these samples. *\n",
    "    4. For each bootstrap set $b = 1,\\dots,B$, obtain $C^b$ as the empirical 1% percentile of the distribution of the depths $D(y_i^b)$, $i = 1, \\dots, n$.\n",
    "    5. Take C as the median of the values of $C^b$, $b = 1, \\dots, B$. \n",
    "<br>\n",
    "*At this point we assume that our functional observations are observed at a set of discrete points $t_1,\\dots,t_m$.\n",
    "\n",
    "To clarify some things, that might get lost in these quotes:\n",
    "* n is not constant overr the iterative process, which might be counterintuitive in the context of other methods\n",
    "* As a consequence: while approximating C the set of observations used is not constant over the iterative process either, as the outliers that have been removed are no longer under consideration for the bootstrap procedure\n",
    "\n",
    "The authors propose three functional depth measures and benchmark them in a simulation setting. Because of their results and the computational cost which are comparatively small, I chose to use **h-modal depth** for my implementation, which I will introduce in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63443be-611f-4d03-b664-93d445943d38",
   "metadata": {},
   "source": [
    "## h-modal depth\n",
    "---\n",
    "\n",
    "Introduced by Cuevas, Febrero-Bande, and Fraiman (2006) h-modal depth is one of three depth measures covered in Febrero-Bande, Galeano, and Gonzàlez-Manteiga (2008). I will follow the summary in the latter paper for my overview.  <br>The idea behind this depth is that a curve is central in a set of curves if it is closely surrounded by other curves?\" <br>\n",
    "\n",
    "In mathematical terms the h-modal depth of a curve $x_i$ in relation to a set of curves $x_1, \\dots, x_n$ is defined as follows: <br>\n",
    "\\begin{equation}\n",
    "    MD_m(x_i,h) = \\sum_{k = 1}^{n} K(\\frac{||x_i - x_k||}{h})\n",
    "\\end{equation}\n",
    "\n",
    "where $K: \\mathbb{R^{+}} \\rightarrow \\mathbb{R^{+}}$ is a kernel function and $h$ is a bandwidth. <br>\n",
    "The authors recommend using the truncated Gaussian kernel, which is defined as follows:\n",
    "\\begin{equation}\n",
    "    K(t) = \\frac{2}{\\sqrt{2\\pi}} \\exp(-\\frac{t^2}{2})\n",
    "\\end{equation}\n",
    "\n",
    "and to choose $h$ as the 15th percentile of the empirical distribution of $\\{||x_i - x_k|| \\, ; \\, i,k = 1,\\dots,n\\}$\n",
    "\n",
    "I chose to implement the $L^2$ norm - one of the norms recommended by the authors - as it performed better than the $L^{\\infty}$ norm (which was also recommended) in my preliminary tests. In a functional setting $L^2$ is defined by:\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\int_a^b (x_i(t) - x_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "where a and b are the boundaries of the interval where measurements are taken. This can be replaced by its empirical version\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 1}^m (x_i(t_j) - x_k(t_j))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "in case of a discrete set of $m$ observation points shared between observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197111c-25fe-4347-83cb-7e42cd3b2c4e",
   "metadata": {},
   "source": [
    "## Difficulties due to the Data\n",
    "---\n",
    "1. **The Endanzug does not start at the same angle for every observation.** <br>\n",
    "In this specific setting this is not much of a problem, as the specific angle where the \"Endanzug\" starts is not of importance. So all observations can be modified by subtracting the first angle of the \"Endanzug\" from all angles, effectively **zeroing** the observations.\n",
    "\n",
    "2. **After zeroing, the observation intervals on which measurements are taken might still not be identical due to differing lengths.** <br>\n",
    "We can define a parameter $\\gamma \\geq 1$ that I call **acceptable stretching** and make observations comparable by stretching their lengths by a factor $\\lambda_i \\in [1/\\gamma, \\gamma]$ before approximating them using linear interpolation. If zeroing as described in 1. is not appropriate a combination of acceptable stretching and acceptable shifting could be implemented to increase the size of sets of pairwise comparable functions.\n",
    "\n",
    "3. **The angles where torque is measured are not shared between observations.** <br>\n",
    "Assuming that the intervals on which torque is measured are identical, we can use **linear interpolation** to approximate the observations and to make them compatible with the simplification described above. This is only an approximation, but choosing an appropriately fine grid to approximate the observations should limit the influence of this procedure on the calculated functional depths.\n",
    "\n",
    "Another possibility to approach this third problem would be to use a different version of the norm for discretized points I described above. <br>\n",
    "Instead of calculating \n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{2} = \\sqrt{\\sum_{j = 1}^m (x_i(t_j) - x_k(t_j))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "one could instead define functions $\\tilde{x}_i$ which are just the piecewise linear functions defined by connecting the observed points of $x_i$. A norm based on this could be constructed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    ||x_i - x_k||_{\\tilde{2}} = \\sqrt{\\int_a^b (\\tilde{x}_i(t) - \\tilde{x}_k(t))^2 dt}\n",
    "\\end{equation}\n",
    "\n",
    "4. **Runtime Complexity** <br>\n",
    "The runtime complexity of this algorithm is at least $O(n^2)$ and I concluded that using my implementation it is infeasible use it on a very large data set such as the \"Endanzugsproblem\" (assuming that all observations are comparrable at once). Even when splitting up the observations as proposed above into comparable subsets, some of them will be too large to directly approach with this method. <br>\n",
    "To solve this problem I instead opted to use a **sampling approach**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebc202-2b78-4955-a45b-c01ee877dcbb",
   "metadata": {},
   "source": [
    "## Sampling Approach\n",
    "---\n",
    "Since it is infeasible to use this method on very large data sets at once, I instead made the assumption that observations that would be classified as outliers in the overall set will also be classified as outliers in subsets of the data set more frequently. So instead of performing the algorithm described above on the whole data set (or its comparable subsets), I chose to instead follow the following approach: <br>\n",
    "\n",
    "Let $\\{x_1, \\dots, x_L\\}$ be a set of observations that are comparable using the algorithm but too large to perform this procedure in reality.\n",
    "1. Define the following objects:\n",
    "    * Let _num\\_samples_ be a vector of length M that keeps track of how many subsamples each observation was part of. (Initialize all as 0)\n",
    "    * Let _num\\_outliers_ be a vector of length M that keeps track of how many subsamples each observation was classified as an outlier in. (Initialize all as 0)\n",
    "    * Let _frac\\_outliers_ be a vector of length M that keeps track of track of the fraction of subsamples an observation was classified as an outlier in. If an observation has not been part of any samples yet, set the corresponding element to 1. (Initialize all as 1)\n",
    "2. Draw a sample of size K from $\\{x_1, \\dots, x_L\\}$.    <span style=\"color:red\"> MIT REPLACEMENT ODER OHNE???</span>.\n",
    "3. Perform the outlier detection procedure on this sample and update the vectors accorrding to your results.\n",
    "4. Go back to two and iterate this process until some condition (for example a specified number of iterations was reached) is fulfilled.\n",
    "\n",
    "In the end, the entries of _frac\\_outliers_ can be used as a metric for the outlyingness of an observation. If a binary decision rule is needed, every observation with an entry over some specified threshold could be classified as an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03850b93-1565-40df-b91f-4b2805c45544",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936d24b8-d34d-4fb1-a1ef-abe697fd0241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in system(\"timedatectl\", intern = TRUE):\n",
      "“running command 'timedatectl' had status 1”\n"
     ]
    }
   ],
   "source": [
    "suppressMessages(library(MASS))\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(shiny))\n",
    "suppressMessages(library(shinydashboard))\n",
    "suppressMessages(library(largeList))\n",
    "suppressMessages(library(parallel))\n",
    "suppressMessages(library(Rcpp))\n",
    "\n",
    "set.seed(17203476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4538296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the simulation:\n",
    "n <- 2000\n",
    "ids <- as.character(1:n)\n",
    "outliers <- rbinom(n = n, size = 1, prob = 0.05)\n",
    "\n",
    "lengths <- sample(x = 10:100, size = n, replace = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91c9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process for drawing one realisation of functional data\n",
    "random_dat <- function(grid, slope, out){\n",
    "    args <- grid\n",
    "    if(out == 0){\n",
    "        vals <- runif(n = length(grid), min = 0, max = slope) * slope * args + rnorm(n = length(grid), mean = 0, sd = 0.05)\n",
    "    }\n",
    "    else{\n",
    "        vals <- runif(n = length(grid), min = 0, max = 1.25 * slope) * 1.25 *slope * args + rnorm(n = length(grid), mean = 0, sd = 0.05)\n",
    "    }\n",
    "    \n",
    "    return(list(args = grid,\n",
    "                vals = vals))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f53645-117d-4404-992a-1974f4f3de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions <- map(.x = 1:n,\n",
    "                 .f = function(x) random_dat(grid = seq(from = 0, to = 1, length.out = lengths[x]), slope = 1.02, out = outliers[x]))\n",
    "\n",
    "### Allows for random access in parallelization (saves memory)\n",
    "saveList(functions, \"data/functions.llo\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691f97c-011e-48e7-b7df-bf43f4b22e1e",
   "metadata": {},
   "source": [
    "In the setting of the \"Endanzugsproblem\" this has some caveats:\n",
    "1. The runtime complexity of this algorithm is O(n²) and therefore infeasible for the whole data set.\n",
    "2. The observations often do not even share the same interval of arguments.\n",
    "3. The functional observations are not observed at a shared set of discrete arguments.\n",
    " \n",
    "My ideas to address these problems:\n",
    "1. Instead of performing this outlier detection procedure on the whole data set at once, I chose to instead follow a sampling approach:\n",
    "* The overall data set consists of n functional observations. \n",
    "* Draw $P$ many samples of size $q<<n$ from the data set and perform outlier detection on each sample.\n",
    "* For each observation i let $a_i$ be the number of samples it appeared in and $b_i \\leq a_i$ the number of samples it was marked as an outlier in.\n",
    "* For each observation report $c_i = \\frac{b_i}{a_i}$ as a measure of the certainty that an observation is an outlier.\n",
    "* Flag every observation i for which $c_i > W$ as a potential outlier for some prespecified $W \\in ]0,1[$\n",
    "\n",
    "This approach has the advantage of being easily parallelizable, so there is not only potential for speed up due to the runtime complexity of O(n²) but also due to parallelization.\n",
    "\n",
    "2. Due to the nature of the data set it is possible to perform some slight alterations to the data, which make this problem easier to deal with:\n",
    "* It is possible to set the beginning of each process to zero - meaning a subtraction of the first angle which was measured at the beginning of the \"Endanzugsprozess\" from all angles of an observation. So now the problem of differing intervals of arguments is only really a problem of differing interval lengths.\n",
    "* There are many subsets of the data set that share the same interval of arguments. So after the zeroing process described before these are comparable with the described procedure.\n",
    "* There are also a lot of observations that have very similar endpoints after the zeroing process. I suspect that an approximation could be made to make these comparable without losing an important amount of identifying accuracy.\n",
    "\n",
    "### Hier weiterarbeiten\n",
    "\n",
    "3.\n",
    "* It might be possible to modify this algorithm so that once new observations are added, the old scores only have to be updated and not calculated completely anew.\n",
    "* To make these observations with similar observation intervals tangible in this algorithm I used linear interpolation between the points at which the function was originally observed adjacent to the grid points. \n",
    "* When the observation intervals are too dissimilar my idea was to split up the groups and instead look for outliers in group, if the observations are not outliers in another way (for example extreme observation intervals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962a30c-fcbb-4f16-a261-2b6097c25a51",
   "metadata": {},
   "source": [
    "I implemented these functions as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23b9e4a-c803-4a2f-bd7c-4416bef4ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceCpp('auxiliary/rcpp_functions.cpp')\n",
    "# gives access to grid_approx_obs & hM_depth written in C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2ab8ba-1483-4edc-9339-daef05ee7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_approx_single_obs <- function(func_obs, grid) {    \n",
    "  return(grid_approx_obs(func_obs$args, func_obs$vals, grid))\n",
    "}\n",
    "\n",
    "grid_approx_set_obs <- function(func_dat, grid) {\n",
    "  res_mat <- matrix(data = unlist(\n",
    "      map(.x = func_dat,\n",
    "          .f = function(obs) grid_approx_single_obs(obs, grid))\n",
    "    ), nrow = length(func_dat), byrow = TRUE)\n",
    "                    \n",
    "  return(res_mat)\n",
    "}\n",
    "\n",
    "# func_dat needs to be observed at the same discretized points for every observation\n",
    "approx_C <- function(matr_data, fdepths, alpha, B, gamma, grid) {\n",
    "    \n",
    "  n <- length(fdepths)\n",
    "  depth_thr <- quantile(x = fdepths, probs = alpha)\n",
    "  matr_data_red <- matr_data[fdepths >= depth_thr, ]\n",
    "  n_red <- dim(matr_data_red)[1]\n",
    "    \n",
    "  Sigma_x <- cov(matr_data_red)\n",
    "  my_vcov <- gamma*Sigma_x  \n",
    "  grid_length <- length(grid)  \n",
    "\n",
    "  sample_inds <- map(.x = 1:B,\n",
    "                     .f = function(x) sample(x = 1:n_red, size = n, replace = TRUE))\n",
    "  \n",
    "  fsamples <- map(.x = sample_inds,\n",
    "                  .f = function(inds) matr_data_red[inds, ])\n",
    "  \n",
    "  smoothing_components <- map(.x = 1:B,\n",
    "                              .f = function(x) mvrnorm(n = n, mu = rep(0, times = grid_length), Sigma = my_vcov))\n",
    "  \n",
    "  smoothed_BS_samples <- map(.x = 1:B,\n",
    "                             .f = function(b) fsamples[[b]] + smoothing_components[[b]])\n",
    "\n",
    "  bootstrap_depths <- map(.x = 1:B,\n",
    "                          .f = function(b) hM_depth(smoothed_BS_samples[[b]]))\n",
    "\n",
    "  one_perc_quantiles <- unlist(map(.x = bootstrap_depths,\n",
    "                                   .f = function(sample) quantile(sample, probs = 0.01)))\n",
    "  \n",
    "  return(median(one_perc_quantiles))\n",
    "}\n",
    "\n",
    "### Like this for testing purposes (plan a method later)\n",
    "grid_finder <- function(func_dat){\n",
    "  return(seq(0, 1, length.out = 100))\n",
    "}\n",
    "\n",
    "### function for the iterative process\n",
    "outlier_iteration <- function(matr_data, alpha, B, gamma, ids, grid){\n",
    "    \n",
    "  fdepths <- hM_depth(matr_data)\n",
    "    \n",
    "  C <- approx_C(matr_data = matr_data, fdepths = fdepths, alpha = alpha,\n",
    "                B = B, gamma = gamma, grid = grid)\n",
    "    \n",
    "  outliers <- which(fdepths < C)\n",
    "  return(list(matr_data = matr_data[-outliers, ],\n",
    "              outlier_ids = ids[outliers],\n",
    "              outliers = outliers))\n",
    "}\n",
    "                                   \n",
    "### function for the whole algorithm\n",
    "outlier_detection <- function(func_dat, alpha, B, gamma, ids){\n",
    "    \n",
    "    condition <- TRUE\n",
    "    outliers <- c()\n",
    "    tmp_ids = ids\n",
    "    i <- 1\n",
    "    \n",
    "    grid <- grid_finder(func_dat)\n",
    "    data <- grid_approx_set_obs(func_dat, grid)\n",
    "    \n",
    "    while(condition){\n",
    "        tmp <- outlier_iteration(matr_data = data, alpha = alpha, B = B, gamma = gamma, ids = tmp_ids, grid = grid)\n",
    "        new_outliers <- tmp$outlier_ids\n",
    "        if(length(new_outliers) == 0){condition <- FALSE}\n",
    "        else{\n",
    "          outliers <- c(outliers, new_outliers)\n",
    "          data <- tmp$matr_data\n",
    "          tmp_ids <- tmp$ids \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return(outliers)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd673af2-f460-4e0e-bf89-420d77e8d68c",
   "metadata": {},
   "source": [
    "It is infeasible to use this algorithm on the full data set due to its runtime complexity of O(n²). So I am going to use a sampling approach, that I will describe in the following:\n",
    "1. Choose a sample of size k from the overall data set (with or without replacement?) that is comparable using functional depth measures. (In the data set I generated here, each subsample is comparable due to their shared observation intervals, but this does not have to be the case.)\n",
    "2. Perform the outlier detection described above on the sample and mark the suspected outliers in the sample.\n",
    "3. Assume that an observation was part of $w$ samples in the overall process and was marked as an outlier in $v \\leq w$ of them. For each observation report $\\frac{v}{w}$ as a certainty measure that an observation is an outlier in the full data set.\n",
    "4. Fix a threshold $Z \\in [0,1]$ and report all observations for which $\\frac{v}{w} \\geq Z$ as suspected outliers.\n",
    "\n",
    "One big advantage of this procedure is that it can be easily parallelized, making it possible to examine multiple samples at once. For the sake of simplicity I am not going to implement this parallelization in this notebook but at a later stage in a more streamlined format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e849a5-63e1-4ae5-9212-9aa9bcdef2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Sys.getenv('CI') == \"true\"){\n",
    "    num_outliers <- readRDS(file = \"data/num_outliers.RDS\")\n",
    "    num_samples <- readRDS(file = \"data/num_samples.RDS\")\n",
    "    certainties <- readRDS(file = \"data/certainties.RDS\")\n",
    "} else{\n",
    "total_samples <- 500\n",
    "k <- 100\n",
    "num_samples <- rep(x = 0, times = n)\n",
    "num_outliers <- rep(x = 0, times = n)\n",
    "certainties <- rep(x = 1, times = n)\n",
    "\n",
    "sample_inds <- map(.x = 1:total_samples, \n",
    "                   .f = function(i) sample(x = 1:n, size = k, replace = FALSE))\n",
    "                   \n",
    "freq_samples <- tabulate(unlist(sample_inds))\n",
    "num_samples[1:length(freq_samples)] <- num_samples[1:length(freq_samples)] + freq_samples    \n",
    "                      \n",
    "### Parallelization:\n",
    "\n",
    "par_helper_fun <- function(list_path, index, alpha, B, gamma){\n",
    "    dat <- readList(file = list_path, index = index)\n",
    "    sample_flagged <- outlier_detection(func_dat = dat, alpha = 0.05, B = 50, gamma = 2, ids = index)\n",
    "}\n",
    "                      \n",
    "num_cores <- detectCores()\n",
    "cl <- makeForkCluster(num_cores)\n",
    "                   \n",
    "invisible(clusterCall(cl, fun = function() library('largeList')))\n",
    "invisible(clusterCall(cl, fun = function() library('Rcpp')))\n",
    "                   \n",
    "clusterExport(cl, varlist = list(\"grid_approx_single_obs\",\n",
    "                                 \"grid_approx_set_obs\",\n",
    "                                 \"approx_C\",\n",
    "                                 \"grid_finder\",\n",
    "                                 \"outlier_iteration\",\n",
    "                                 \"outlier_detection\",\n",
    "                                 \"par_helper_fun\"),\n",
    "             envir = .GlobalEnv)\n",
    "                      \n",
    "sample_flagged_par <- clusterApplyLB(cl = cl,\n",
    "                                     x = sample_inds,\n",
    "                                     fun = function(smp) par_helper_fun(list_path = \"data/functions.llo\", \n",
    "                                                                      index = smp, alpha = 0.05, \n",
    "                                                                      B = 50, gamma = 2))\n",
    "\n",
    "                      \n",
    "stopCluster(cl)\n",
    "\n",
    "### Only works if ids are integers (else slighty change function outlier detection to give back indices and not ids)                      \n",
    "#ind_outliers <- map(.x = sample_flagged,\n",
    "#                    .f = as.integer)                \n",
    "\n",
    "#freq_outliers <- tabulate(unlist(ind_outliers))\n",
    "                   \n",
    "freq_outliers <- tabulate(unlist(sample_flagged_par))\n",
    "num_outliers[1:length(freq_outliers)] <- num_outliers[1:length(freq_outliers)] + freq_outliers\n",
    "                \n",
    "certainties <- unlist(map(.x = 1:n,\n",
    "                          .f = function(i) ifelse(num_samples[i] != 0, num_outliers[i]/num_samples[i], 1)))    \n",
    "\n",
    "saveRDS(object = num_outliers, file = \"data/num_outliers.RDS\")                          \n",
    "saveRDS(object = num_samples, file = \"data/num_samples.RDS\")\n",
    "saveRDS(object = certainties, file = \"data/certainties.RDS\")\n",
    "saveRDS(object = num_outliers, file = \"visual/data/num_outliers.RDS\")                          \n",
    "saveRDS(object = num_samples, file = \"visual/data/num_samples.RDS\")\n",
    "saveRDS(object = certainties, file = \"visual/data/certainties.RDS\")\n",
    "}                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4d297-711b-46c9-8fde-2153b3d8fe7f",
   "metadata": {},
   "source": [
    "Applying these to the original simulated data set yields the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33fbd8b6-4095-4534-b44b-15f28bb05646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer(0)\n"
     ]
    }
   ],
   "source": [
    "if(Sys.getenv('CI') == \"true\"){\n",
    "    num_outliers <- readRDS(file = \"data/num_outliers.RDS\")\n",
    "    num_samples <- readRDS(file = \"data/num_samples.RDS\")\n",
    "    certainties <- readRDS(file = \"data/certainties.RDS\")\n",
    "}\n",
    "\n",
    "print(which(num_samples == 0))                          \n",
    "flagged <- which(certainties > 0.5)\n",
    "original_outliers <- which(outliers == 1)\n",
    "missed_outliers <- setdiff(original_outliers, flagged)\n",
    "false_outliers <- setdiff(flagged, original_outliers)\n",
    "\n",
    "saveRDS(object = list(flagged = flagged,\n",
    "                      original = original_outliers,\n",
    "                      missed = missed_outliers,\n",
    "                      false = false_outliers),\n",
    "        file = \"data/outliers_info.RDS\")\n",
    "saveRDS(object = list(flagged = flagged,\n",
    "                      original = original_outliers,\n",
    "                      missed = missed_outliers,\n",
    "                      false = false_outliers),\n",
    "        file = \"visual/data/outliers_info.RDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "492fd93b-1f95-4c4b-84ba-54545fe688fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tibble for visualization\n",
    "\n",
    "my_tibble <- bind_rows(map(\n",
    "  .x = 1:n,\n",
    "  .f = function(i) {\n",
    "    tibble(\n",
    "      args = functions[[i]]$args,\n",
    "      vals = functions[[i]]$vals,\n",
    "      id = rep(i, times = length(args)),\n",
    "      cert = rep(certainties[i], times = length(args))\n",
    "    )\n",
    "  }\n",
    "))\n",
    "\n",
    "saveRDS(object = my_tibble, file = \"data/shiny_tibble.RDS\")\n",
    "saveRDS(object = my_tibble, file = \"visual/data/shiny_tibble.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d06ea8-97b9-47f0-9d54-c669acc5a38b",
   "metadata": {},
   "source": [
    "So at least in this simple setting the algorithm identifies nearly all generated outliers and does not classify any regular point as an outlier. This is not always the case and careful choice of the tuning parameters is necessary to ensure a good performance. In the case of other data generating processes, this method can also perform worse, but is applicable in very general settings due to its functional nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c02253f-4b88-4eb0-98d2-f915d9d7156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Listening on http://127.0.0.1:7525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if(Sys.getenv('CI') != \"true\"){suppressWarnings(runApp('visual'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768452e9-118c-4738-a37f-9e99ce7825df",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "---\n",
    "* Gijbels, I. & Nagy, S. (2017). On a General Definition of Depth for Functional Data. Statistical Science. 32. 630-639.\n",
    "* Febrero-Bande, M. & Galeano, P. & Gonzàlez-Manteiga, W. (2008). Outlier detection in functional data by depth measures, with application to identify abnormal NOx levels. Environmetrics. 19. 331 - 345.\n",
    "* Cuevas, A. & Febrero-Bande, M. & Fraiman, R. (2006). On the use of bootstrap for estimating functions with functional data. Computational Statistics & Data Analysis. 51. 1063-1074."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86371d-fcba-4106-9c7c-63baaa3f90c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
